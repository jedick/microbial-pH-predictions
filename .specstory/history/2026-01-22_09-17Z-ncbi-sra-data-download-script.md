<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 7504b652-9191-43a2-9a9c-046ed8322f03 (2026-01-22 09:17Z) -->

# NCBI SRA data download script (2026-01-22 09:17Z)

_**User**_

Write a script to download data from the NCBI Sequence Read Archive (SRA). Look in @sample_data.csv  for sample_id with the pattern SRR* or ERR*. The download URL looks like this: https://trace.ncbi.nlm.nih.gov/Traces/sra-reads-be/fasta?acc=SRR7775171 . The file format is gzip (i.e., *.gz even though the URL doesn't have that suffix). I would like to download *only* the first 1MB of this file. Note that this will make the gz file incomplete, but it can still be extracted using zcat (I have tried it on the command line as in zcat <incomplete_downloaded_file> > SRR7775171.fasta). Remove the last DNA sequence from the file (as it may be truncated) and then gzip the fasta file and save it as data/fasta/<study_name>/<sample_id>.fasta.gz. Before downloading anything, first check if the exists in the file system and skip the download if it does. Make the script loop over all the rows in sample_data.csv, showing progress with the current study_name and sample_id. Some sample_ids are repeated in the file, so just skip them if they have been downloaded already. Also skip downloading sample_ids that do not match the SRR* or ERR* pattern - they are from other sources that need manual downloading. Do not leave temporary files in the project directory if the script is interrupted (e.g. with Ctrl+C).

---

_**Agent (model default, mode Agent)**_


Examining the CSV structure to understand the data format.


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/data/sample_data.csv</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="unknown" data-tool-name="semantic_search_full">
<details>
<summary>Tool use: **semantic_search_full**</summary>


Parameters:

```json
{
  "query": "How are SRA files or fasta files downloaded or processed?",
  "topK": 10,
  "explanation": ""
}
```
Result:

```json
{
  "codeResults": [
    {
      "codeBlock": {
        "relativeWorkspacePath": "./create_hf_dataset.py",
        "range": {
          "startPosition": {
            "line": 110,
            "column": 22
          },
          "endPosition": {
            "line": 166
          }
        },
        "contents": "def validate_fasta_files(\n    fasta_dir: Path, sample_data: Dict[Tuple[str, str], Dict]\n) -&gt; List[Tuple[Path, str, str]]:\n    \"\"\"\n    Validate that all fasta files have matching entries in sample_data.csv.\n    Returns list of (fasta_path, study_name, sample_id) tuples.\n    \"\"\"\n    fasta_files = []\n    missing_entries = []\n\n    # Walk through fasta directory structure: data/fasta/{study_name}/{sample_id}.fasta.gz\n    for study_dir in fasta_dir.iterdir():\n        if not study_dir.is_dir():\n            continue\n\n        study_name = study_dir.name\n        for fasta_file in study_dir.glob(\"*.fasta.gz\"):\n            sample_id = fasta_file.stem.replace(\n                \".fasta\", \"\"\n            )  # Remove .fasta.gz -&gt; get SRR7775171\n\n            key = (study_name, sample_id)\n            if key not in sample_data:\n                missing_entries.append((fasta_file, study_name, sample_id))\n            else:\n                fasta_files.append((fasta_file, study_name, sample_id))\n\n    if missing_entries:\n        print(\n            \"ERROR: The following fasta files do not have matching entries in sample_data.csv:\"\n        )\n        for fasta_file, study_name, sample_id in missing_entries:\n            print(f\"  - {fasta_file}: study_name={study_name}, sample_id={sample_id}\")\n        raise ValueError(\"Some fasta files are missing from sample_data.csv\")\n\n    print(f\"✓ Validated {len(fasta_files)} fasta files against sample_data.csv\")\n    return fasta_files\n\n\ndef get_existing_records(dataset_repo: str) -&gt; set:\n    \"\"\"",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def validate_fasta_files(",
            "lineNumber": 114,
            "isSignature": true
          },
          {
            "text": "    fasta_dir: Path, sample_data: Dict[Tuple[str, str], Dict]",
            "lineNumber": 115,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[Tuple[Path, str, str]]:",
            "lineNumber": 116,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 117
          },
          {
            "text": "    Validate that all fasta files have matching entries in sample_data.csv.",
            "lineNumber": 118
          },
          {
            "text": "    Returns list of (fasta_path, study_name, sample_id) tuples.",
            "lineNumber": 119
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 120
          },
          {
            "text": "    fasta_files = []",
            "lineNumber": 121
          },
          {
            "text": "    missing_entries = []",
            "lineNumber": 122
          },
          {
            "lineNumber": 123
          },
          {
            "text": "    # Walk through fasta directory structure: data/fasta/{study_name}/{sample_id}.fasta.gz",
            "lineNumber": 124
          },
          {
            "text": "    for study_dir in fasta_dir.iterdir():",
            "lineNumber": 125
          },
          {
            "text": "        if not study_dir.is_dir():",
            "lineNumber": 126
          },
          {
            "text": "            continue",
            "lineNumber": 127
          },
          {
            "lineNumber": 128
          },
          {
            "text": "        study_name = study_dir.name",
            "lineNumber": 129
          },
          {
            "text": "        for fasta_file in study_dir.glob(\"*.fasta.gz\"):",
            "lineNumber": 130
          },
          {
            "text": "            sample_id = fasta_file.stem.replace(",
            "lineNumber": 131
          },
          {
            "text": "                \".fasta\", \"\"",
            "lineNumber": 132
          },
          {
            "text": "            )  # Remove .fasta.gz -&gt; get SRR7775171",
            "lineNumber": 133
          },
          {
            "lineNumber": 134
          },
          {
            "text": "            key = (study_name, sample_id)",
            "lineNumber": 135
          },
          {
            "text": "            if key not in sample_data:",
            "lineNumber": 136
          },
          {
            "text": "                missing_entries.append((fasta_file, study_name, sample_id))",
            "lineNumber": 137
          },
          {
            "text": "            else:",
            "lineNumber": 138
          },
          {
            "text": "                fasta_files.append((fasta_file, study_name, sample_id))",
            "lineNumber": 139
          },
          {
            "lineNumber": 140
          },
          {
            "text": "    if missing_entries:",
            "lineNumber": 141
          },
          {
            "text": "        print(",
            "lineNumber": 142
          },
          {
            "text": "            \"ERROR: The following fasta files do not have matching entries in sample_data.csv:\"",
            "lineNumber": 143
          },
          {
            "text": "        )",
            "lineNumber": 144
          },
          {
            "text": "        for fasta_file, study_name, sample_id in missing_entries:",
            "lineNumber": 145
          },
          {
            "text": "            print(f\"  - {fasta_file}: study_name={study_name}, sample_id={sample_id}\")",
            "lineNumber": 146
          },
          {
            "text": "        raise ValueError(\"Some fasta files are missing from sample_data.csv\")",
            "lineNumber": 147
          },
          {
            "lineNumber": 148
          },
          {
            "text": "    print(f\"✓ Validated {len(fasta_files)} fasta files against sample_data.csv\")",
            "lineNumber": 149
          },
          {
            "text": "    return fasta_files",
            "lineNumber": 150,
            "isSignature": true
          },
          {
            "lineNumber": 151
          },
          {
            "lineNumber": 152
          },
          {
            "text": "def get_existing_records(dataset_repo: str) -&gt; set:",
            "lineNumber": 153,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 154
          }
        ]
      },
      "score": 0.3841421604156494
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./create_hf_dataset.py",
        "range": {
          "startPosition": {
            "line": 33,
            "column": 15
          },
          "endPosition": {
            "line": 110
          }
        },
        "contents": "def read_sequences_from_fasta(\n    fasta_path: Path, max_length: int = MAX_DNA_LENGTH\n) -&gt; List[str]:\n    \"\"\"\n    Read sequences from a gzipped FASTA file until reaching max_length total DNA.\n    Returns a list of sequence strings.\n    \"\"\"\n    sequences = []\n    current_sequence = []\n    current_header = None\n    total_length = 0\n\n    with gzip.open(fasta_path, \"rt\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n\n            if line.startswith(\"&gt;\"):\n                # Save previous sequence if exists\n                if current_sequence:\n                    seq = \"\".join(current_sequence)\n                    seq_len = len(seq)\n                    if total_length + seq_len &lt;= max_length:\n                        sequences.append(seq)\n                        total_length += seq_len\n                    else:\n                        # Take partial sequence if we haven't reached max yet\n                        remaining = max_length - total_length\n                        if remaining &gt; 0:\n                            sequences.append(seq[:remaining])\n                        break\n\n                # Start new sequence\n                current_header = line\n                current_sequence = []\n            else:\n                # Add to current sequence\n                current_sequence.append(line)\n\n        # Handle last sequence\n        if current_sequence and total_length &lt; max_length:\n            seq = \"\".join(current_sequence)\n            seq_len = len(seq)\n            if total_length + seq_len &lt;= max_length:\n                sequences.append(seq)\n                total_length += seq_len\n            else:\n                remaining = max_length - total_length\n                if remaining &gt; 0:\n                    sequences.append(seq[:remaining])\n\n    return sequences\n\n\ndef load_sample_data(csv_path: Path) -&gt; Dict[Tuple[str, str], Dict]:\n    \"\"\"",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def read_sequences_from_fasta(",
            "lineNumber": 37,
            "isSignature": true
          },
          {
            "text": "    fasta_path: Path, max_length: int = MAX_DNA_LENGTH",
            "lineNumber": 38,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[str]:",
            "lineNumber": 39,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 40
          },
          {
            "text": "    Read sequences from a gzipped FASTA file until reaching max_length total DNA.",
            "lineNumber": 41
          },
          {
            "text": "    Returns a list of sequence strings.",
            "lineNumber": 42
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 43
          },
          {
            "text": "    sequences = []",
            "lineNumber": 44
          },
          {
            "text": "    current_sequence = []",
            "lineNumber": 45
          },
          {
            "text": "    current_header = None",
            "lineNumber": 46
          },
          {
            "text": "    total_length = 0",
            "lineNumber": 47
          },
          {
            "lineNumber": 48
          },
          {
            "text": "    with gzip.open(fasta_path, \"rt\") as f:",
            "lineNumber": 49
          },
          {
            "text": "        for line in f:",
            "lineNumber": 50
          },
          {
            "text": "            line = line.strip()",
            "lineNumber": 51
          },
          {
            "text": "            if not line:",
            "lineNumber": 52
          },
          {
            "text": "                continue",
            "lineNumber": 53
          },
          {
            "lineNumber": 54
          },
          {
            "text": "            if line.startswith(\"&gt;\"):",
            "lineNumber": 55
          },
          {
            "text": "                # Save previous sequence if exists",
            "lineNumber": 56
          },
          {
            "text": "                if current_sequence:",
            "lineNumber": 57
          },
          {
            "text": "                    seq = \"\".join(current_sequence)",
            "lineNumber": 58
          },
          {
            "text": "                    seq_len = len(seq)",
            "lineNumber": 59
          },
          {
            "text": "                    if total_length + seq_len &lt;= max_length:",
            "lineNumber": 60
          },
          {
            "text": "                        sequences.append(seq)",
            "lineNumber": 61
          },
          {
            "text": "                        total_length += seq_len",
            "lineNumber": 62
          },
          {
            "text": "                    else:",
            "lineNumber": 63
          },
          {
            "text": "                        # Take partial sequence if we haven't reached max yet",
            "lineNumber": 64
          },
          {
            "text": "                        remaining = max_length - total_length",
            "lineNumber": 65
          },
          {
            "text": "                        if remaining &gt; 0:",
            "lineNumber": 66
          },
          {
            "text": "                            sequences.append(seq[:remaining])",
            "lineNumber": 67
          },
          {
            "text": "                        break",
            "lineNumber": 68
          },
          {
            "lineNumber": 69
          },
          {
            "text": "                # Start new sequence",
            "lineNumber": 70
          },
          {
            "text": "                current_header = line",
            "lineNumber": 71
          },
          {
            "text": "                current_sequence = []",
            "lineNumber": 72
          },
          {
            "text": "            else:",
            "lineNumber": 73
          },
          {
            "text": "                # Add to current sequence",
            "lineNumber": 74
          },
          {
            "text": "                current_sequence.append(line)",
            "lineNumber": 75
          },
          {
            "lineNumber": 76
          },
          {
            "text": "        # Handle last sequence",
            "lineNumber": 77
          },
          {
            "text": "        if current_sequence and total_length &lt; max_length:",
            "lineNumber": 78
          },
          {
            "text": "            seq = \"\".join(current_sequence)",
            "lineNumber": 79
          },
          {
            "text": "            seq_len = len(seq)",
            "lineNumber": 80
          },
          {
            "text": "            if total_length + seq_len &lt;= max_length:",
            "lineNumber": 81
          },
          {
            "text": "                sequences.append(seq)",
            "lineNumber": 82
          },
          {
            "text": "                total_length += seq_len",
            "lineNumber": 83
          },
          {
            "text": "            else:",
            "lineNumber": 84
          },
          {
            "text": "                remaining = max_length - total_length",
            "lineNumber": 85
          },
          {
            "text": "                if remaining &gt; 0:",
            "lineNumber": 86
          },
          {
            "text": "                    sequences.append(seq[:remaining])",
            "lineNumber": 87
          },
          {
            "lineNumber": 88
          },
          {
            "text": "    return sequences",
            "lineNumber": 89,
            "isSignature": true
          },
          {
            "lineNumber": 90
          },
          {
            "lineNumber": 91
          },
          {
            "text": "def load_sample_data(csv_path: Path) -&gt; Dict[Tuple[str, str], Dict]:",
            "lineNumber": 92,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 93
          }
        ]
      },
      "score": 0.35125815868377686
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./process_rdp_data.py",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 96
          }
        },
        "contents": "\"\"\"\nProcess RDP classifier tab files to extract and aggregate taxonomic counts.\n\nThis module provides functionality to parse RDP classifier output files,\nextract taxonomic information from lineage strings, and aggregate counts\nby taxonomic level (phylum, class, order, family, genus).\n\"\"\"\n\nimport pandas as pd\nimport re\nfrom pathlib import Path\nfrom typing import Optional\n\n\ndef process_rdp_data(filename: str, domain: str = \"Bacteria\") -&gt; dict:\n    \"\"\"\n    Process RDP classifier tab file and return aggregated taxonomic counts.\n\n    Parameters\n    ----------\n    filename : str\n        Path to the RDP classifier tab file (e.g., \"APV+20.tab\")\n    domain : str, optional\n        Domain to filter by: \"Bacteria\" or \"Archaea\" (default: \"Bacteria\")\n\n    Returns\n    -------\n    dict\n        Dictionary with structure {sample_id: {taxon_name: count}}.\n        Taxonomic names use double underscores (e.g., \"phylum__Omnitrophota\",\n        \"class__Koll11\").\n\n    Examples\n    --------\n    &gt;&gt;&gt; result = process_rdp_data(\"data/RDP_classifier/APV+20.tab\", domain=\"Bacteria\")\n    &gt;&gt;&gt; print(result['SRR7775171']['phylum__Omnitrophota'])\n    \"\"\"\n    # Validate domain\n    if domain not in [\"Bacteria\", \"Archaea\"]:\n        raise ValueError(f\"domain must be 'Bacteria' or 'Archaea', got '{domain}'\")\n\n    # Read the tab file\n    df = pd.read_csv(filename, sep=\"\\t\")\n\n    # Filter by domain\n    domain_pattern = f\"{domain};domain\"\n    df = df[df[\"lineage\"].str.contains(domain_pattern, na=False, regex=False)]\n\n    # Filter lineages: must end with \"genus;\" or have \"unclassified_\" in last position\n    def is_valid_lineage(lineage: str) -&gt; bool:\n        if pd.isna(lineage):\n            return False\n        # Check if ends with \"genus;\"\n        if lineage.endswith(\"genus;\"):\n            return True\n        # Check if has \"unclassified_\" in the last position before final semicolons\n        # Pattern: ...;unclassified_XXX;;\n        if re.search(r\";unclassified_[^;]+;;$\", lineage):\n            return True\n        return False\n\n    df = df[df[\"lineage\"].apply(is_valid_lineage)]",
        "signatures": {},
        "detailedLines": [
          {
            "text": "\"\"\"",
            "lineNumber": 1
          },
          {
            "text": "Process RDP classifier tab files to extract and aggregate taxonomic counts.",
            "lineNumber": 2
          },
          {
            "lineNumber": 3
          },
          {
            "text": "This module provides functionality to parse RDP classifier output files,",
            "lineNumber": 4
          },
          {
            "text": "extract taxonomic information from lineage strings, and aggregate counts",
            "lineNumber": 5
          },
          {
            "text": "by taxonomic level (phylum, class, order, family, genus).",
            "lineNumber": 6
          },
          {
            "text": "\"\"\"",
            "lineNumber": 7
          },
          {
            "lineNumber": 8
          },
          {
            "text": "import pandas as pd",
            "lineNumber": 9
          },
          {
            "text": "import re",
            "lineNumber": 10
          },
          {
            "text": "from pathlib import Path",
            "lineNumber": 11
          },
          {
            "text": "from typing import Optional",
            "lineNumber": 12
          },
          {
            "lineNumber": 13
          },
          {
            "lineNumber": 14
          },
          {
            "text": "def process_rdp_data(filename: str, domain: str = \"Bacteria\") -&gt; dict:",
            "lineNumber": 15,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 16
          },
          {
            "text": "    Process RDP classifier tab file and return aggregated taxonomic counts.",
            "lineNumber": 17
          },
          {
            "lineNumber": 18
          },
          {
            "text": "    Parameters",
            "lineNumber": 19
          },
          {
            "text": "    ----------",
            "lineNumber": 20
          },
          {
            "text": "    filename : str",
            "lineNumber": 21
          },
          {
            "text": "        Path to the RDP classifier tab file (e.g., \"APV+20.tab\")",
            "lineNumber": 22
          },
          {
            "text": "    domain : str, optional",
            "lineNumber": 23
          },
          {
            "text": "        Domain to filter by: \"Bacteria\" or \"Archaea\" (default: \"Bacteria\")",
            "lineNumber": 24
          },
          {
            "lineNumber": 25
          },
          {
            "text": "    Returns",
            "lineNumber": 26
          },
          {
            "text": "    -------",
            "lineNumber": 27
          },
          {
            "text": "    dict",
            "lineNumber": 28
          },
          {
            "text": "        Dictionary with structure {sample_id: {taxon_name: count}}.",
            "lineNumber": 29
          },
          {
            "text": "        Taxonomic names use double underscores (e.g., \"phylum__Omnitrophota\",",
            "lineNumber": 30
          },
          {
            "text": "        \"class__Koll11\").",
            "lineNumber": 31
          },
          {
            "lineNumber": 32
          },
          {
            "text": "    Examples",
            "lineNumber": 33
          },
          {
            "text": "    --------",
            "lineNumber": 34
          },
          {
            "text": "    &gt;&gt;&gt; result = process_rdp_data(\"data/RDP_classifier/APV+20.tab\", domain=\"Bacteria\")",
            "lineNumber": 35
          },
          {
            "text": "    &gt;&gt;&gt; print(result['SRR7775171']['phylum__Omnitrophota'])",
            "lineNumber": 36
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 37
          },
          {
            "text": "    # Validate domain",
            "lineNumber": 38
          },
          {
            "text": "    if domain not in [\"Bacteria\", \"Archaea\"]:",
            "lineNumber": 39
          },
          {
            "text": "        raise ValueError(f\"domain must be 'Bacteria' or 'Archaea', got '{domain}'\")",
            "lineNumber": 40
          },
          {
            "lineNumber": 41
          },
          {
            "text": "    # Read the tab file",
            "lineNumber": 42
          },
          {
            "text": "    df = pd.read_csv(filename, sep=\"\\t\")",
            "lineNumber": 43
          },
          {
            "lineNumber": 44
          },
          {
            "text": "    # Filter by domain",
            "lineNumber": 45
          },
          {
            "text": "    domain_pattern = f\"{domain};domain\"",
            "lineNumber": 46
          },
          {
            "text": "    df = df[df[\"lineage\"].str.contains(domain_pattern, na=False, regex=False)]",
            "lineNumber": 47
          },
          {
            "lineNumber": 48
          },
          {
            "text": "    # Filter lineages: must end with \"genus;\" or have \"unclassified_\" in last position",
            "lineNumber": 49
          },
          {
            "text": "    def is_valid_lineage(lineage: str) -&gt; bool:",
            "lineNumber": 50
          },
          {
            "text": "        if pd.isna(lineage):",
            "lineNumber": 51
          },
          {
            "text": "            return False",
            "lineNumber": 52
          },
          {
            "text": "        # Check if ends with \"genus;\"",
            "lineNumber": 53
          },
          {
            "text": "        if lineage.endswith(\"genus;\"):",
            "lineNumber": 54
          },
          {
            "text": "            return True",
            "lineNumber": 55
          },
          {
            "text": "        # Check if has \"unclassified_\" in the last position before final semicolons",
            "lineNumber": 56
          },
          {
            "text": "        # Pattern: ...;unclassified_XXX;;",
            "lineNumber": 57
          },
          {
            "text": "        if re.search(r\";unclassified_[^;]+;;$\", lineage):",
            "lineNumber": 58
          },
          {
            "text": "            return True",
            "lineNumber": 59
          },
          {
            "text": "        return False",
            "lineNumber": 60
          },
          {
            "lineNumber": 61
          },
          {
            "text": "    df = df[df[\"lineage\"].apply(is_valid_lineage)]",
            "lineNumber": 62
          }
        ]
      },
      "score": 0.3506391942501068
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./README.md",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 34
          }
        },
        "contents": "# Microbial pH predictions\n\n## Project overview\n\nThis project aims to develop machine learning models for predicting pH from microbial abundances and 16S rRNA gene sequences. The approach incorporates both traditional machine learning methods (e.g., Random Forests and KNN regression) and deep learning techniques, with particular emphasis on language models for predictions from gene sequences.\n\n## Methods\n\n- **Traditional ML**: Random Forests, KNN regression, and other classical approaches\n- **Deep Learning**: Language models and other neural network architectures, especially for gene sequence-based predictions\n\n## Data processing\n\nThe &#96;process_rdp_data.py&#96; script is used to aggregate taxonomic counts from RDP classifier output files. The script processes &#96;.tab&#96; files in the &#96;data/RDP_classifier&#96; directory and generates aggregated count tables saved as CSV files in the &#96;data&#96; directory. The output files contain taxonomic counts at various levels (phylum, class, order, family, genus) with samples as rows and taxonomic groups as columns.\n\n**Note**: The source RDP classifier files are not stored in this repository but can be downloaded from the [JMDplots repository](https://github.com/jedick/JMDplots/tree/main/inst/extdata/orp16S/RDP-GTDB).\n\n## Hugging Face dataset creation\n\nThe &#96;create_hf_dataset.py&#96; script creates a Hugging Face dataset from DNA sequences stored in gzipped FASTA files and sample metadata. This dataset is designed for training language models on 16S rRNA gene sequences to predict pH values.\n\n&#96;&#96;&#96;bash\n# Requires HF_TOKEN in .env file\npython create_hf_dataset.py\n&#96;&#96;&#96;\n\n&lt;details&gt;\n&lt;summary&gt;Details&lt;/summary&gt;\n\nThe script processes DNA sequences from FASTA files organized by study (&#96;data/fasta/{study_name}/{sample_id}.fasta.gz&#96;) and combines them with metadata from &#96;sample_data.csv&#96; to create a structured dataset suitable for machine learning workflows.\n\n- **Automatic validation**: Ensures all FASTA files have corresponding entries in &#96;sample_data.csv&#96;\n- **Incremental updates**: Skips samples that already exist in the dataset, allowing for incremental additions\n- **Size management**: Limits total DNA length per sample to 1 MB to manage dataset size",
        "signatures": {},
        "detailedLines": [
          {
            "text": "# Microbial pH predictions",
            "lineNumber": 1
          },
          {
            "lineNumber": 2
          },
          {
            "text": "## Project overview",
            "lineNumber": 3
          },
          {
            "lineNumber": 4
          },
          {
            "text": "This project aims to develop machine learning models for predicting pH from microbial abundances and 16S rRNA gene sequences. The approach incorporates both traditional machine learning methods (e.g., Random Forests and KNN regression) and deep learning techniques, with particular emphasis on language models for predictions from gene sequences.",
            "lineNumber": 5
          },
          {
            "lineNumber": 6
          },
          {
            "text": "## Methods",
            "lineNumber": 7
          },
          {
            "lineNumber": 8
          },
          {
            "text": "- **Traditional ML**: Random Forests, KNN regression, and other classical approaches",
            "lineNumber": 9
          },
          {
            "text": "- **Deep Learning**: Language models and other neural network architectures, especially for gene sequence-based predictions",
            "lineNumber": 10
          },
          {
            "lineNumber": 11
          },
          {
            "text": "## Data processing",
            "lineNumber": 12
          },
          {
            "lineNumber": 13
          },
          {
            "text": "The &#96;process_rdp_data.py&#96; script is used to aggregate taxonomic counts from RDP classifier output files. The script processes &#96;.tab&#96; files in the &#96;data/RDP_classifier&#96; directory and generates aggregated count tables saved as CSV files in the &#96;data&#96; directory. The output files contain taxonomic counts at various levels (phylum, class, order, family, genus) with samples as rows and taxonomic groups as columns.",
            "lineNumber": 14
          },
          {
            "lineNumber": 15
          },
          {
            "text": "**Note**: The source RDP classifier files are not stored in this repository but can be downloaded from the [JMDplots repository](https://github.com/jedick/JMDplots/tree/main/inst/extdata/orp16S/RDP-GTDB).",
            "lineNumber": 16
          },
          {
            "lineNumber": 17
          },
          {
            "text": "## Hugging Face dataset creation",
            "lineNumber": 18
          },
          {
            "lineNumber": 19
          },
          {
            "text": "The &#96;create_hf_dataset.py&#96; script creates a Hugging Face dataset from DNA sequences stored in gzipped FASTA files and sample metadata. This dataset is designed for training language models on 16S rRNA gene sequences to predict pH values.",
            "lineNumber": 20
          },
          {
            "lineNumber": 21
          },
          {
            "text": "&#96;&#96;&#96;bash",
            "lineNumber": 22
          },
          {
            "text": "# Requires HF_TOKEN in .env file",
            "lineNumber": 23
          },
          {
            "text": "python create_hf_dataset.py",
            "lineNumber": 24
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 25
          },
          {
            "lineNumber": 26
          },
          {
            "text": "&lt;details&gt;",
            "lineNumber": 27
          },
          {
            "text": "&lt;summary&gt;Details&lt;/summary&gt;",
            "lineNumber": 28
          },
          {
            "lineNumber": 29
          },
          {
            "text": "The script processes DNA sequences from FASTA files organized by study (&#96;data/fasta/{study_name}/{sample_id}.fasta.gz&#96;) and combines them with metadata from &#96;sample_data.csv&#96; to create a structured dataset suitable for machine learning workflows.",
            "lineNumber": 30
          },
          {
            "lineNumber": 31
          },
          {
            "text": "- **Automatic validation**: Ensures all FASTA files have corresponding entries in &#96;sample_data.csv&#96;",
            "lineNumber": 32
          },
          {
            "text": "- **Incremental updates**: Skips samples that already exist in the dataset, allowing for incremental additions",
            "lineNumber": 33
          },
          {
            "text": "- **Size management**: Limits total DNA length per sample to 1 MB to manage dataset size",
            "lineNumber": 34
          }
        ]
      },
      "score": 0.34830060601234436
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./create_hf_dataset.py",
        "range": {
          "startPosition": {
            "line": 166,
            "column": 20
          },
          "endPosition": {
            "line": 241
          }
        },
        "contents": "def create_dataset_records(\n    fasta_files: List[Tuple[Path, str, str]],\n    sample_data: Dict[Tuple[str, str], Dict],\n    existing_records: set,\n) -&gt; List[Dict]:\n    \"\"\"\n    Create dataset records from fasta files.\n    Returns a list of dictionaries with study_name, sample_id, pH, and sequences.\n    \"\"\"\n    records = []\n\n    for fasta_path, study_name, sample_id in fasta_files:\n        key = (study_name, sample_id)\n\n        # Skip if already exists\n        if key in existing_records:\n            print(f\"  Skipping {study_name}/{sample_id} (already exists)\")\n            continue\n\n        print(f\"  Processing {study_name}/{sample_id}...\")\n\n        # Read sequences\n        sequences = read_sequences_from_fasta(fasta_path)\n        total_length = sum(len(seq) for seq in sequences)\n\n        # Get sample data\n        data = sample_data[key]\n\n        # Create record\n        record = {\n            \"study_name\": study_name,\n            \"sample_id\": sample_id,\n            \"pH\": data[\"pH\"],\n            \"sequences\": sequences,  # List of strings for easy slicing\n            \"num_sequences\": len(sequences),\n            \"total_dna_length\": total_length,\n            \"sample\": data[\"sample\"],\n            \"environment\": data[\"environment\"],\n            \"domain\": data[\"domain\"],\n        }\n\n        records.append(record)\n        print(f\"    Read {len(sequences)} sequences ({total_length:,} nucleotides)\")\n\n    return records\n\n\ndef upload_dataset(records: List[Dict], dataset_repo: str):\n    \"\"\"\n    Create or update the Hugging Face dataset with new records.\n    \"\"\"",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def create_dataset_records(",
            "lineNumber": 170,
            "isSignature": true
          },
          {
            "text": "    fasta_files: List[Tuple[Path, str, str]],",
            "lineNumber": 171,
            "isSignature": true
          },
          {
            "text": "    sample_data: Dict[Tuple[str, str], Dict],",
            "lineNumber": 172,
            "isSignature": true
          },
          {
            "text": "    existing_records: set,",
            "lineNumber": 173,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[Dict]:",
            "lineNumber": 174,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 175
          },
          {
            "text": "    Create dataset records from fasta files.",
            "lineNumber": 176
          },
          {
            "text": "    Returns a list of dictionaries with study_name, sample_id, pH, and sequences.",
            "lineNumber": 177
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 178
          },
          {
            "text": "    records = []",
            "lineNumber": 179
          },
          {
            "lineNumber": 180
          },
          {
            "text": "    for fasta_path, study_name, sample_id in fasta_files:",
            "lineNumber": 181
          },
          {
            "text": "        key = (study_name, sample_id)",
            "lineNumber": 182
          },
          {
            "lineNumber": 183
          },
          {
            "text": "        # Skip if already exists",
            "lineNumber": 184
          },
          {
            "text": "        if key in existing_records:",
            "lineNumber": 185
          },
          {
            "text": "            print(f\"  Skipping {study_name}/{sample_id} (already exists)\")",
            "lineNumber": 186
          },
          {
            "text": "            continue",
            "lineNumber": 187
          },
          {
            "lineNumber": 188
          },
          {
            "text": "        print(f\"  Processing {study_name}/{sample_id}...\")",
            "lineNumber": 189
          },
          {
            "lineNumber": 190
          },
          {
            "text": "        # Read sequences",
            "lineNumber": 191
          },
          {
            "text": "        sequences = read_sequences_from_fasta(fasta_path)",
            "lineNumber": 192
          },
          {
            "text": "        total_length = sum(len(seq) for seq in sequences)",
            "lineNumber": 193
          },
          {
            "lineNumber": 194
          },
          {
            "text": "        # Get sample data",
            "lineNumber": 195
          },
          {
            "text": "        data = sample_data[key]",
            "lineNumber": 196
          },
          {
            "lineNumber": 197
          },
          {
            "text": "        # Create record",
            "lineNumber": 198
          },
          {
            "text": "        record = {",
            "lineNumber": 199
          },
          {
            "text": "            \"study_name\": study_name,",
            "lineNumber": 200
          },
          {
            "text": "            \"sample_id\": sample_id,",
            "lineNumber": 201
          },
          {
            "text": "            \"pH\": data[\"pH\"],",
            "lineNumber": 202
          },
          {
            "text": "            \"sequences\": sequences,  # List of strings for easy slicing",
            "lineNumber": 203
          },
          {
            "text": "            \"num_sequences\": len(sequences),",
            "lineNumber": 204
          },
          {
            "text": "            \"total_dna_length\": total_length,",
            "lineNumber": 205
          },
          {
            "text": "            \"sample\": data[\"sample\"],",
            "lineNumber": 206
          },
          {
            "text": "            \"environment\": data[\"environment\"],",
            "lineNumber": 207
          },
          {
            "text": "            \"domain\": data[\"domain\"],",
            "lineNumber": 208
          },
          {
            "text": "        }",
            "lineNumber": 209
          },
          {
            "lineNumber": 210
          },
          {
            "text": "        records.append(record)",
            "lineNumber": 211
          },
          {
            "text": "        print(f\"    Read {len(sequences)} sequences ({total_length:,} nucleotides)\")",
            "lineNumber": 212
          },
          {
            "lineNumber": 213
          },
          {
            "text": "    return records",
            "lineNumber": 214,
            "isSignature": true
          },
          {
            "lineNumber": 215
          },
          {
            "lineNumber": 216
          },
          {
            "text": "def upload_dataset(records: List[Dict], dataset_repo: str):",
            "lineNumber": 217,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 218
          },
          {
            "text": "    Create or update the Hugging Face dataset with new records.",
            "lineNumber": 219
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 220
          }
        ]
      },
      "score": 0.3442504107952118
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./process_rdp_data.py",
        "range": {
          "startPosition": {
            "line": 144
          },
          "endPosition": {
            "line": 331
          }
        },
        "contents": "def process_rdp_directory(\n    directory: str,\n    domain: str = \"Bacteria\",\n    output_file: Optional[str] = None,\n    taxonomic_level: Optional[str] = None,\n) -&gt; pd.DataFrame:\n\n    if not dir_path.exists():\n        raise FileNotFoundError(f\"Directory not found: {directory}\")\n\n    # Find all .tab files\n    tab_files = sorted(dir_path.glob(\"*.tab\"))\n\n    if not tab_files:\n        print(f\"No .tab files found in: {directory}\")\n        return pd.DataFrame()\n\n    print(f\"Found {len(tab_files)} .tab files to process\")\n    print(f\"Domain: {domain}\")\n    if taxonomic_level:\n        print(f\"Taxonomic level: {taxonomic_level}\")\n    else:\n        print(f\"Taxonomic level: all\")\n    print(\"-\" * 80)\n\n    # List to store dictionaries from each file (will convert to DataFrame later)\n    all_results = []\n\n    # Process each file\n    for i, tab_file in enumerate(tab_files, 1):\n        print(f\"\\n[{i}/{len(tab_files)}] Processing: {tab_file.name}\")\n\n        try:\n            # Extract study name from filename\n            study_name = tab_file.stem\n\n            # Process the file (returns dict: {sample_id: {taxon_name: count}})\n            sample_counts = process_rdp_data(str(tab_file), domain=domain)\n\n            if not sample_counts:\n                print(f\"  ⚠️  No data found for {domain} domain\")\n                continue\n\n            # Convert dictionary to list of records for DataFrame creation\n            # This is more memory-efficient than creating intermediate DataFrames\n            records = []\n            all_taxa = set()\n\n            for sample_id, taxa_dict in sample_counts.items():\n                if not taxa_dict:  # Skip empty samples\n                    continue\n                record = {\"study_name\": study_name, \"sample_id\": sample_id}\n                record.update(taxa_dict)\n                records.append(record)\n                all_taxa.update(taxa_dict.keys())\n\n            if not records:\n                print(f\"  ⚠️  No data found for {domain} domain\")\n                continue\n\n            # Create DataFrame from records\n            df = pd.DataFrame(records)\n\n            # Set index\n            df = df.set_index([\"study_name\", \"sample_id\"])\n\n            # Convert all count columns to integers (they're already ints in the dict, but",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 145,
                "column": 1
              },
              "endPosition": {
                "line": 151,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def process_rdp_directory(",
            "lineNumber": 145,
            "isSignature": true
          },
          {
            "text": "    directory: str,",
            "lineNumber": 146,
            "isSignature": true
          },
          {
            "text": "    domain: str = \"Bacteria\",",
            "lineNumber": 147,
            "isSignature": true
          },
          {
            "text": "    output_file: Optional[str] = None,",
            "lineNumber": 148,
            "isSignature": true
          },
          {
            "text": "    taxonomic_level: Optional[str] = None,",
            "lineNumber": 149,
            "isSignature": true
          },
          {
            "text": ") -&gt; pd.DataFrame:",
            "lineNumber": 150,
            "isSignature": true
          },
          {
            "lineNumber": 194
          },
          {
            "text": "    if not dir_path.exists():",
            "lineNumber": 195
          },
          {
            "text": "        raise FileNotFoundError(f\"Directory not found: {directory}\")",
            "lineNumber": 196
          },
          {
            "lineNumber": 197
          },
          {
            "text": "    # Find all .tab files",
            "lineNumber": 198
          },
          {
            "text": "    tab_files = sorted(dir_path.glob(\"*.tab\"))",
            "lineNumber": 199
          },
          {
            "lineNumber": 200
          },
          {
            "text": "    if not tab_files:",
            "lineNumber": 201
          },
          {
            "text": "        print(f\"No .tab files found in: {directory}\")",
            "lineNumber": 202
          },
          {
            "text": "        return pd.DataFrame()",
            "lineNumber": 203
          },
          {
            "lineNumber": 204
          },
          {
            "text": "    print(f\"Found {len(tab_files)} .tab files to process\")",
            "lineNumber": 205
          },
          {
            "text": "    print(f\"Domain: {domain}\")",
            "lineNumber": 206
          },
          {
            "text": "    if taxonomic_level:",
            "lineNumber": 207
          },
          {
            "text": "        print(f\"Taxonomic level: {taxonomic_level}\")",
            "lineNumber": 208
          },
          {
            "text": "    else:",
            "lineNumber": 209
          },
          {
            "text": "        print(f\"Taxonomic level: all\")",
            "lineNumber": 210
          },
          {
            "text": "    print(\"-\" * 80)",
            "lineNumber": 211
          },
          {
            "lineNumber": 212
          },
          {
            "text": "    # List to store dictionaries from each file (will convert to DataFrame later)",
            "lineNumber": 213
          },
          {
            "text": "    all_results = []",
            "lineNumber": 214
          },
          {
            "lineNumber": 215
          },
          {
            "text": "    # Process each file",
            "lineNumber": 216
          },
          {
            "text": "    for i, tab_file in enumerate(tab_files, 1):",
            "lineNumber": 217
          },
          {
            "text": "        print(f\"\\n[{i}/{len(tab_files)}] Processing: {tab_file.name}\")",
            "lineNumber": 218
          },
          {
            "lineNumber": 219
          },
          {
            "text": "        try:",
            "lineNumber": 220
          },
          {
            "text": "            # Extract study name from filename",
            "lineNumber": 221
          },
          {
            "text": "            study_name = tab_file.stem",
            "lineNumber": 222
          },
          {
            "lineNumber": 223
          },
          {
            "text": "            # Process the file (returns dict: {sample_id: {taxon_name: count}})",
            "lineNumber": 224
          },
          {
            "text": "            sample_counts = process_rdp_data(str(tab_file), domain=domain)",
            "lineNumber": 225
          },
          {
            "lineNumber": 226
          },
          {
            "text": "            if not sample_counts:",
            "lineNumber": 227
          },
          {
            "text": "                print(f\"  ⚠️  No data found for {domain} domain\")",
            "lineNumber": 228
          },
          {
            "text": "                continue",
            "lineNumber": 229
          },
          {
            "lineNumber": 230
          },
          {
            "text": "            # Convert dictionary to list of records for DataFrame creation",
            "lineNumber": 231
          },
          {
            "text": "            # This is more memory-efficient than creating intermediate DataFrames",
            "lineNumber": 232
          },
          {
            "text": "            records = []",
            "lineNumber": 233
          },
          {
            "text": "            all_taxa = set()",
            "lineNumber": 234
          },
          {
            "lineNumber": 235
          },
          {
            "text": "            for sample_id, taxa_dict in sample_counts.items():",
            "lineNumber": 236
          },
          {
            "text": "                if not taxa_dict:  # Skip empty samples",
            "lineNumber": 237
          },
          {
            "text": "                    continue",
            "lineNumber": 238
          },
          {
            "text": "                record = {\"study_name\": study_name, \"sample_id\": sample_id}",
            "lineNumber": 239
          },
          {
            "text": "                record.update(taxa_dict)",
            "lineNumber": 240
          },
          {
            "text": "                records.append(record)",
            "lineNumber": 241
          },
          {
            "text": "                all_taxa.update(taxa_dict.keys())",
            "lineNumber": 242
          },
          {
            "lineNumber": 243
          },
          {
            "text": "            if not records:",
            "lineNumber": 244
          },
          {
            "text": "                print(f\"  ⚠️  No data found for {domain} domain\")",
            "lineNumber": 245
          },
          {
            "text": "                continue",
            "lineNumber": 246
          },
          {
            "lineNumber": 247
          },
          {
            "text": "            # Create DataFrame from records",
            "lineNumber": 248
          },
          {
            "text": "            df = pd.DataFrame(records)",
            "lineNumber": 249
          },
          {
            "lineNumber": 250
          },
          {
            "text": "            # Set index",
            "lineNumber": 251
          },
          {
            "text": "            df = df.set_index([\"study_name\", \"sample_id\"])",
            "lineNumber": 252
          },
          {
            "lineNumber": 253
          },
          {
            "text": "            # Convert all count columns to integers (they're already ints in the dict, but",
            "lineNumber": 254
          }
        ]
      },
      "score": 0.34168124198913574
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./create_hf_dataset.py",
        "range": {
          "startPosition": {
            "line": 241,
            "column": 84
          },
          "endPosition": {
            "line": 287,
            "column": 10
          }
        },
        "contents": "def main():\n    \"\"\"Main function to orchestrate the dataset creation.\"\"\"\n    # Paths\n    project_root = Path(__file__).parent\n    fasta_dir = project_root / \"data\" / \"fasta\"\n    sample_data_path = project_root / \"data\" / \"sample_data.csv\"\n\n    # Check HF_TOKEN\n    if not HF_TOKEN:\n        raise ValueError(\"HF_TOKEN not found in .env file\")\n\n    # Login to Hugging Face\n    print(\"Logging in to Hugging Face...\")\n    login(token=HF_TOKEN)\n\n    # Load sample data\n    print(\"\\nLoading sample data...\")\n    sample_data = load_sample_data(sample_data_path)\n    print(f\"✓ Loaded {len(sample_data)} sample records\")\n\n    # Validate fasta files\n    print(\"\\nValidating fasta files...\")\n    fasta_files = validate_fasta_files(fasta_dir, sample_data)\n\n    # Check existing records\n    print(\"\\nChecking existing dataset records...\")\n    existing_records = get_existing_records(HF_DATASET_REPO)\n\n    # Create dataset records\n    print(\"\\nProcessing fasta files...\")\n    records = create_dataset_records(fasta_files, sample_data, existing_records)\n\n    # Upload dataset\n    if records:\n        print(\"\\nUploading dataset...\")\n        upload_dataset(records, HF_DATASET_REPO)\n    else:\n        print(\"\\nAll records already exist in the dataset. Nothing to upload.\")\n\n    print(\"\\n✓ Done!\")\n\n\nif __name__ == \"__main__\":\n    main()",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def main():",
            "lineNumber": 245,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Main function to orchestrate the dataset creation.\"\"\"",
            "lineNumber": 246
          },
          {
            "text": "    # Paths",
            "lineNumber": 247
          },
          {
            "text": "    project_root = Path(__file__).parent",
            "lineNumber": 248
          },
          {
            "text": "    fasta_dir = project_root / \"data\" / \"fasta\"",
            "lineNumber": 249
          },
          {
            "text": "    sample_data_path = project_root / \"data\" / \"sample_data.csv\"",
            "lineNumber": 250
          },
          {
            "lineNumber": 251
          },
          {
            "text": "    # Check HF_TOKEN",
            "lineNumber": 252
          },
          {
            "text": "    if not HF_TOKEN:",
            "lineNumber": 253
          },
          {
            "text": "        raise ValueError(\"HF_TOKEN not found in .env file\")",
            "lineNumber": 254
          },
          {
            "lineNumber": 255
          },
          {
            "text": "    # Login to Hugging Face",
            "lineNumber": 256
          },
          {
            "text": "    print(\"Logging in to Hugging Face...\")",
            "lineNumber": 257
          },
          {
            "text": "    login(token=HF_TOKEN)",
            "lineNumber": 258
          },
          {
            "lineNumber": 259
          },
          {
            "text": "    # Load sample data",
            "lineNumber": 260
          },
          {
            "text": "    print(\"\\nLoading sample data...\")",
            "lineNumber": 261
          },
          {
            "text": "    sample_data = load_sample_data(sample_data_path)",
            "lineNumber": 262
          },
          {
            "text": "    print(f\"✓ Loaded {len(sample_data)} sample records\")",
            "lineNumber": 263
          },
          {
            "lineNumber": 264
          },
          {
            "text": "    # Validate fasta files",
            "lineNumber": 265
          },
          {
            "text": "    print(\"\\nValidating fasta files...\")",
            "lineNumber": 266
          },
          {
            "text": "    fasta_files = validate_fasta_files(fasta_dir, sample_data)",
            "lineNumber": 267
          },
          {
            "lineNumber": 268
          },
          {
            "text": "    # Check existing records",
            "lineNumber": 269
          },
          {
            "text": "    print(\"\\nChecking existing dataset records...\")",
            "lineNumber": 270
          },
          {
            "text": "    existing_records = get_existing_records(HF_DATASET_REPO)",
            "lineNumber": 271
          },
          {
            "lineNumber": 272
          },
          {
            "text": "    # Create dataset records",
            "lineNumber": 273
          },
          {
            "text": "    print(\"\\nProcessing fasta files...\")",
            "lineNumber": 274
          },
          {
            "text": "    records = create_dataset_records(fasta_files, sample_data, existing_records)",
            "lineNumber": 275
          },
          {
            "lineNumber": 276
          },
          {
            "text": "    # Upload dataset",
            "lineNumber": 277
          },
          {
            "text": "    if records:",
            "lineNumber": 278
          },
          {
            "text": "        print(\"\\nUploading dataset...\")",
            "lineNumber": 279
          },
          {
            "text": "        upload_dataset(records, HF_DATASET_REPO)",
            "lineNumber": 280
          },
          {
            "text": "    else:",
            "lineNumber": 281
          },
          {
            "text": "        print(\"\\nAll records already exist in the dataset. Nothing to upload.\")",
            "lineNumber": 282
          },
          {
            "lineNumber": 283
          },
          {
            "text": "    print(\"\\n✓ Done!\")",
            "lineNumber": 284,
            "isSignature": true
          },
          {
            "lineNumber": 285
          },
          {
            "lineNumber": 286
          },
          {
            "text": "if __name__ == \"__main__\":",
            "lineNumber": 287
          },
          {
            "text": "    main()",
            "lineNumber": 288
          }
        ]
      },
      "score": 0.33861520886421204
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./process_rdp_data.py",
        "range": {
          "startPosition": {
            "line": 369,
            "column": 30
          },
          "endPosition": {
            "line": 418,
            "column": 19
          }
        },
        "contents": "    path_obj = Path(path)\n\n    if path_obj.is_file():\n        # Process single file\n        study_name = path_obj.stem\n        sample_counts = process_rdp_data(path, domain)\n\n        if not sample_counts:\n            print(\"No data found\")\n            sys.exit(0)\n\n        # Convert to DataFrame for display\n        records = []\n        for sample_id, taxa_dict in sample_counts.items():\n            if not taxa_dict:\n                continue\n            record = {\"sample_id\": sample_id}\n            record.update(taxa_dict)\n            records.append(record)\n\n        if not records:\n            print(\"No data found\")\n            sys.exit(0)\n\n        df = pd.DataFrame(records).set_index(\"sample_id\")\n        df = df.reindex(sorted(df.columns), axis=1)\n\n        # Convert all count columns to integers\n        for col in df.columns:\n            df[col] = df[col].fillna(0).astype(\"int64\")\n\n        print(f\"\\nProcessed {len(df)} samples\")\n        print(f\"Found {len(df.columns)} taxonomic groups\")\n        print(\"\\nFirst few rows:\")\n        print(df.head())\n        print(\"\\nDataFrame shape:\", df.shape)\n        print(\"\\nColumn names (first 10):\", list(df.columns[:10]))\n    elif path_obj.is_dir():\n        # Process directory\n        df = process_rdp_directory(\n            path,\n            domain=domain,\n            output_file=output_file,\n            taxonomic_level=taxonomic_level,\n        )\n    else:\n        print(f\"Error: {path} is not a valid file or directory\")\n        sys.exit(1)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "    path_obj = Path(path)",
            "lineNumber": 372
          },
          {
            "lineNumber": 373
          },
          {
            "text": "    if path_obj.is_file():",
            "lineNumber": 374
          },
          {
            "text": "        # Process single file",
            "lineNumber": 375
          },
          {
            "text": "        study_name = path_obj.stem",
            "lineNumber": 376
          },
          {
            "text": "        sample_counts = process_rdp_data(path, domain)",
            "lineNumber": 377
          },
          {
            "lineNumber": 378
          },
          {
            "text": "        if not sample_counts:",
            "lineNumber": 379
          },
          {
            "text": "            print(\"No data found\")",
            "lineNumber": 380
          },
          {
            "text": "            sys.exit(0)",
            "lineNumber": 381
          },
          {
            "lineNumber": 382
          },
          {
            "text": "        # Convert to DataFrame for display",
            "lineNumber": 383
          },
          {
            "text": "        records = []",
            "lineNumber": 384
          },
          {
            "text": "        for sample_id, taxa_dict in sample_counts.items():",
            "lineNumber": 385
          },
          {
            "text": "            if not taxa_dict:",
            "lineNumber": 386
          },
          {
            "text": "                continue",
            "lineNumber": 387
          },
          {
            "text": "            record = {\"sample_id\": sample_id}",
            "lineNumber": 388
          },
          {
            "text": "            record.update(taxa_dict)",
            "lineNumber": 389
          },
          {
            "text": "            records.append(record)",
            "lineNumber": 390
          },
          {
            "lineNumber": 391
          },
          {
            "text": "        if not records:",
            "lineNumber": 392
          },
          {
            "text": "            print(\"No data found\")",
            "lineNumber": 393
          },
          {
            "text": "            sys.exit(0)",
            "lineNumber": 394
          },
          {
            "lineNumber": 395
          },
          {
            "text": "        df = pd.DataFrame(records).set_index(\"sample_id\")",
            "lineNumber": 396
          },
          {
            "text": "        df = df.reindex(sorted(df.columns), axis=1)",
            "lineNumber": 397
          },
          {
            "lineNumber": 398
          },
          {
            "text": "        # Convert all count columns to integers",
            "lineNumber": 399
          },
          {
            "text": "        for col in df.columns:",
            "lineNumber": 400
          },
          {
            "text": "            df[col] = df[col].fillna(0).astype(\"int64\")",
            "lineNumber": 401
          },
          {
            "lineNumber": 402
          },
          {
            "text": "        print(f\"\\nProcessed {len(df)} samples\")",
            "lineNumber": 403
          },
          {
            "text": "        print(f\"Found {len(df.columns)} taxonomic groups\")",
            "lineNumber": 404
          },
          {
            "text": "        print(\"\\nFirst few rows:\")",
            "lineNumber": 405
          },
          {
            "text": "        print(df.head())",
            "lineNumber": 406
          },
          {
            "text": "        print(\"\\nDataFrame shape:\", df.shape)",
            "lineNumber": 407
          },
          {
            "text": "        print(\"\\nColumn names (first 10):\", list(df.columns[:10]))",
            "lineNumber": 408
          },
          {
            "text": "    elif path_obj.is_dir():",
            "lineNumber": 409
          },
          {
            "text": "        # Process directory",
            "lineNumber": 410
          },
          {
            "text": "        df = process_rdp_directory(",
            "lineNumber": 411
          },
          {
            "text": "            path,",
            "lineNumber": 412
          },
          {
            "text": "            domain=domain,",
            "lineNumber": 413
          },
          {
            "text": "            output_file=output_file,",
            "lineNumber": 414
          },
          {
            "text": "            taxonomic_level=taxonomic_level,",
            "lineNumber": 415
          },
          {
            "text": "        )",
            "lineNumber": 416
          },
          {
            "text": "    else:",
            "lineNumber": 417
          },
          {
            "text": "        print(f\"Error: {path} is not a valid file or directory\")",
            "lineNumber": 418
          },
          {
            "text": "        sys.exit(1)",
            "lineNumber": 419
          }
        ]
      },
      "score": 0.3345648944377899
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./create_hf_dataset.py",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 88
          }
        },
        "contents": "#!/usr/bin/env python3\n\"\"\"\nCreate a Hugging Face dataset from DNA sequences in fasta files and sample data.\n\"\"\"\n\nimport os\nimport gzip\nimport csv\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional\nfrom dotenv import load_dotenv\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom huggingface_hub import HfApi, login\n\n# Load environment variables\nload_dotenv()\n\nHF_TOKEN = os.getenv(\"HF_TOKEN\")\nHF_USERNAME = \"jedick\"\nHF_DATASET_NAME = \"microbial-DNA-pH\"\nHF_DATASET_REPO = f\"{HF_USERNAME}/{HF_DATASET_NAME}\"\n\n# Constants\nMAX_DNA_LENGTH = 1 * 1024 * 1024  # 1 MB in bytes (1,048,576 bytes)\n\n\ndef parse_fasta_header(header: str) -&gt; Optional[str]:\n    \"\"\"Extract sequence identifier from FASTA header.\"\"\"\n    # Headers look like: &gt;SRR7775171.1 1 length=251\n    match = re.match(r\"&gt;(\\S+)\", header)\n    if match:\n        return match.group(1)\n    return None\n\n\ndef read_sequences_from_fasta(\n    fasta_path: Path, max_length: int = MAX_DNA_LENGTH\n) -&gt; List[str]:\n    ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "#!/usr/bin/env python3",
            "lineNumber": 1
          },
          {
            "text": "\"\"\"",
            "lineNumber": 2
          },
          {
            "text": "Create a Hugging Face dataset from DNA sequences in fasta files and sample data.",
            "lineNumber": 3
          },
          {
            "text": "\"\"\"",
            "lineNumber": 4
          },
          {
            "lineNumber": 5
          },
          {
            "text": "import os",
            "lineNumber": 6
          },
          {
            "text": "import gzip",
            "lineNumber": 7
          },
          {
            "text": "import csv",
            "lineNumber": 8
          },
          {
            "text": "import re",
            "lineNumber": 9
          },
          {
            "text": "from pathlib import Path",
            "lineNumber": 10
          },
          {
            "text": "from typing import Dict, List, Tuple, Optional",
            "lineNumber": 11
          },
          {
            "text": "from dotenv import load_dotenv",
            "lineNumber": 12
          },
          {
            "text": "from datasets import Dataset, DatasetDict, load_dataset",
            "lineNumber": 13
          },
          {
            "text": "from huggingface_hub import HfApi, login",
            "lineNumber": 14
          },
          {
            "lineNumber": 15
          },
          {
            "text": "# Load environment variables",
            "lineNumber": 16
          },
          {
            "text": "load_dotenv()",
            "lineNumber": 17
          },
          {
            "lineNumber": 18
          },
          {
            "text": "HF_TOKEN = os.getenv(\"HF_TOKEN\")",
            "lineNumber": 19
          },
          {
            "text": "HF_USERNAME = \"jedick\"",
            "lineNumber": 20
          },
          {
            "text": "HF_DATASET_NAME = \"microbial-DNA-pH\"",
            "lineNumber": 21
          },
          {
            "text": "HF_DATASET_REPO = f\"{HF_USERNAME}/{HF_DATASET_NAME}\"",
            "lineNumber": 22
          },
          {
            "lineNumber": 23
          },
          {
            "text": "# Constants",
            "lineNumber": 24
          },
          {
            "text": "MAX_DNA_LENGTH = 1 * 1024 * 1024  # 1 MB in bytes (1,048,576 bytes)",
            "lineNumber": 25
          },
          {
            "lineNumber": 26
          },
          {
            "lineNumber": 27
          },
          {
            "text": "def parse_fasta_header(header: str) -&gt; Optional[str]:",
            "lineNumber": 28,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Extract sequence identifier from FASTA header.\"\"\"",
            "lineNumber": 29
          },
          {
            "text": "    # Headers look like: &gt;SRR7775171.1 1 length=251",
            "lineNumber": 30
          },
          {
            "text": "    match = re.match(r\"&gt;(\\S+)\", header)",
            "lineNumber": 31
          },
          {
            "text": "    if match:",
            "lineNumber": 32
          },
          {
            "text": "        return match.group(1)",
            "lineNumber": 33
          },
          {
            "text": "    return None",
            "lineNumber": 34,
            "isSignature": true
          },
          {
            "lineNumber": 35
          },
          {
            "lineNumber": 36
          },
          {
            "text": "def read_sequences_from_fasta(",
            "lineNumber": 37,
            "isSignature": true
          },
          {
            "text": "    fasta_path: Path, max_length: int = MAX_DNA_LENGTH",
            "lineNumber": 38,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[str]:",
            "lineNumber": 39,
            "isSignature": true
          },
          {
            "text": "    ",
            "lineNumber": 40
          }
        ]
      },
      "score": 0.32412654161453247
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./process_rdp_data.py",
        "range": {
          "startPosition": {
            "line": 141,
            "column": 15
          },
          "endPosition": {
            "line": 331
          }
        },
        "contents": "def process_rdp_directory(\n    directory: str,\n    domain: str = \"Bacteria\",\n    output_file: Optional[str] = None,\n    taxonomic_level: Optional[str] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Process all RDP classifier tab files in a directory and combine results.\n\n    Parameters\n    ----------\n    directory : str\n        Path to directory containing RDP classifier tab files (e.g., \"data/RDP_classifier\")\n    domain : str, optional\n        Domain to filter by: \"Bacteria\" or \"Archaea\" (default: \"Bacteria\")\n    output_file : str, optional\n        Path to save the combined results as CSV (e.g., \"data/bacteria_counts.csv\").\n        If None, defaults to \"data/{domain.lower()}_counts.csv\" or\n        \"data/{domain.lower()}_{level}_counts.csv\" if taxonomic_level is specified\n    taxonomic_level : str, optional\n        Taxonomic level to filter results: \"phylum\", \"class\", \"order\", \"family\", or \"genus\".\n        If None, includes all levels (default: None)\n\n    Returns\n    -------\n    pd.DataFrame\n        Combined DataFrame with two-level index (study_name, sample_id) and taxonomic\n        names as columns. All samples from all files are included. If taxonomic_level\n        is specified, only columns for that level are included.\n\n    Examples\n    --------\n    &gt;&gt;&gt; df = process_rdp_directory(\"data/RDP_classifier\", domain=\"Bacteria\")\n    &gt;&gt;&gt; df_phylum = process_rdp_directory(\"data/RDP_classifier\", domain=\"Bacteria\", taxonomic_level=\"phylum\")\n    &gt;&gt;&gt; print(df.head())\n    \"\"\"\n    # Validate domain\n    if domain not in [\"Bacteria\", \"Archaea\"]:\n        raise ValueError(f\"domain must be 'Bacteria' or 'Archaea', got '{domain}'\")\n\n    # Validate taxonomic_level if provided\n    valid_levels = [\"phylum\", \"class\", \"order\", \"family\", \"genus\"]\n    if taxonomic_level is not None:\n        if taxonomic_level not in valid_levels:\n            raise ValueError(\n                f\"taxonomic_level must be one of {valid_levels}, got '{taxonomic_level}'\"\n            )\n\n    # Get directory path\n    dir_path = Path(directory)\n    if not dir_path.exists():\n        raise",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def process_rdp_directory(",
            "lineNumber": 145,
            "isSignature": true
          },
          {
            "text": "    directory: str,",
            "lineNumber": 146,
            "isSignature": true
          },
          {
            "text": "    domain: str = \"Bacteria\",",
            "lineNumber": 147,
            "isSignature": true
          },
          {
            "text": "    output_file: Optional[str] = None,",
            "lineNumber": 148,
            "isSignature": true
          },
          {
            "text": "    taxonomic_level: Optional[str] = None,",
            "lineNumber": 149,
            "isSignature": true
          },
          {
            "text": ") -&gt; pd.DataFrame:",
            "lineNumber": 150,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 151
          },
          {
            "text": "    Process all RDP classifier tab files in a directory and combine results.",
            "lineNumber": 152
          },
          {
            "lineNumber": 153
          },
          {
            "text": "    Parameters",
            "lineNumber": 154
          },
          {
            "text": "    ----------",
            "lineNumber": 155
          },
          {
            "text": "    directory : str",
            "lineNumber": 156
          },
          {
            "text": "        Path to directory containing RDP classifier tab files (e.g., \"data/RDP_classifier\")",
            "lineNumber": 157
          },
          {
            "text": "    domain : str, optional",
            "lineNumber": 158
          },
          {
            "text": "        Domain to filter by: \"Bacteria\" or \"Archaea\" (default: \"Bacteria\")",
            "lineNumber": 159
          },
          {
            "text": "    output_file : str, optional",
            "lineNumber": 160
          },
          {
            "text": "        Path to save the combined results as CSV (e.g., \"data/bacteria_counts.csv\").",
            "lineNumber": 161
          },
          {
            "text": "        If None, defaults to \"data/{domain.lower()}_counts.csv\" or",
            "lineNumber": 162
          },
          {
            "text": "        \"data/{domain.lower()}_{level}_counts.csv\" if taxonomic_level is specified",
            "lineNumber": 163
          },
          {
            "text": "    taxonomic_level : str, optional",
            "lineNumber": 164
          },
          {
            "text": "        Taxonomic level to filter results: \"phylum\", \"class\", \"order\", \"family\", or \"genus\".",
            "lineNumber": 165
          },
          {
            "text": "        If None, includes all levels (default: None)",
            "lineNumber": 166
          },
          {
            "lineNumber": 167
          },
          {
            "text": "    Returns",
            "lineNumber": 168
          },
          {
            "text": "    -------",
            "lineNumber": 169
          },
          {
            "text": "    pd.DataFrame",
            "lineNumber": 170
          },
          {
            "text": "        Combined DataFrame with two-level index (study_name, sample_id) and taxonomic",
            "lineNumber": 171
          },
          {
            "text": "        names as columns. All samples from all files are included. If taxonomic_level",
            "lineNumber": 172
          },
          {
            "text": "        is specified, only columns for that level are included.",
            "lineNumber": 173
          },
          {
            "lineNumber": 174
          },
          {
            "text": "    Examples",
            "lineNumber": 175
          },
          {
            "text": "    --------",
            "lineNumber": 176
          },
          {
            "text": "    &gt;&gt;&gt; df = process_rdp_directory(\"data/RDP_classifier\", domain=\"Bacteria\")",
            "lineNumber": 177
          },
          {
            "text": "    &gt;&gt;&gt; df_phylum = process_rdp_directory(\"data/RDP_classifier\", domain=\"Bacteria\", taxonomic_level=\"phylum\")",
            "lineNumber": 178
          },
          {
            "text": "    &gt;&gt;&gt; print(df.head())",
            "lineNumber": 179
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 180
          },
          {
            "text": "    # Validate domain",
            "lineNumber": 181
          },
          {
            "text": "    if domain not in [\"Bacteria\", \"Archaea\"]:",
            "lineNumber": 182
          },
          {
            "text": "        raise ValueError(f\"domain must be 'Bacteria' or 'Archaea', got '{domain}'\")",
            "lineNumber": 183
          },
          {
            "lineNumber": 184
          },
          {
            "text": "    # Validate taxonomic_level if provided",
            "lineNumber": 185
          },
          {
            "text": "    valid_levels = [\"phylum\", \"class\", \"order\", \"family\", \"genus\"]",
            "lineNumber": 186
          },
          {
            "text": "    if taxonomic_level is not None:",
            "lineNumber": 187
          },
          {
            "text": "        if taxonomic_level not in valid_levels:",
            "lineNumber": 188
          },
          {
            "text": "            raise ValueError(",
            "lineNumber": 189
          },
          {
            "text": "                f\"taxonomic_level must be one of {valid_levels}, got '{taxonomic_level}'\"",
            "lineNumber": 190
          },
          {
            "text": "            )",
            "lineNumber": 191
          },
          {
            "lineNumber": 192
          },
          {
            "text": "    # Get directory path",
            "lineNumber": 193
          },
          {
            "text": "    dir_path = Path(directory)",
            "lineNumber": 194
          },
          {
            "text": "    if not dir_path.exists():",
            "lineNumber": 195
          },
          {
            "text": "        raise",
            "lineNumber": 196
          }
        ]
      },
      "score": 0.3012206554412842
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/huggingface_wrapper.py",
        "range": {
          "startPosition": {
            "line": 66,
            "column": 23
          },
          "endPosition": {
            "line": 143
          }
        },
        "contents": "class HyenaDNAPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    base_model_prefix = \"hyenadna\"\n\n    def __init__(self, config):\n        pass\n\n    def forward(self, input_ids, **kwargs):\n        return self.model(input_ids, **kwargs)\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        path,\n        model_name,\n        download=False,\n        config=None,\n        device=\"cpu\",\n        use_head=False,\n        n_classes=2,\n    ):\n        # first check if it is a local path\n        pretrained_model_name_or_path = os.path.join(path, model_name)\n        if os.path.isdir(pretrained_model_name_or_path) and download == False:\n            if config is None:\n                config = json.load(\n                    open(os.path.join(pretrained_model_name_or_path, \"config.json\"))\n                )\n        else:\n            hf_url = f\"https://huggingface.co/LongSafari/{model_name}\"\n\n            subprocess.run(f\"rm -rf {pretrained_model_name_or_path}\", shell=True)\n            command = (\n                f\"mkdir -p {path} &amp;&amp; cd {path} &amp;&amp; git lfs install &amp;&amp; git clone {hf_url}\"\n            )\n            subprocess.run(command, shell=True)\n\n            if config is None:\n                config = json.load(\n                    open(os.path.join(pretrained_model_name_or_path, \"config.json\"))\n                )\n\n        scratch_model = HyenaDNAModel(\n            **config, use_head=use_head, n_classes=n_classes\n        )  # the new model format\n        loaded_ckpt = torch.load(\n            os.path.join(pretrained_model_name_or_path, \"weights.ckpt\"),\n            map_location=torch.device(device),\n            # Needed for PyTorch &gt;= 2.6\n            weights_only=False,\n        )\n\n        # need to load weights slightly different if using gradient checkpointing",
        "signatures": {},
        "detailedLines": [
          {
            "text": "class HyenaDNAPreTrainedModel(PreTrainedModel):",
            "lineNumber": 70,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 71
          },
          {
            "text": "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained",
            "lineNumber": 72
          },
          {
            "text": "    models.",
            "lineNumber": 73
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 74
          },
          {
            "lineNumber": 75
          },
          {
            "text": "    base_model_prefix = \"hyenadna\"",
            "lineNumber": 76
          },
          {
            "lineNumber": 77
          },
          {
            "text": "    def __init__(self, config):",
            "lineNumber": 78
          },
          {
            "text": "        pass",
            "lineNumber": 79
          },
          {
            "lineNumber": 80
          },
          {
            "text": "    def forward(self, input_ids, **kwargs):",
            "lineNumber": 81
          },
          {
            "text": "        return self.model(input_ids, **kwargs)",
            "lineNumber": 82
          },
          {
            "lineNumber": 83
          },
          {
            "text": "    @classmethod",
            "lineNumber": 84
          },
          {
            "text": "    def from_pretrained(",
            "lineNumber": 85
          },
          {
            "text": "        cls,",
            "lineNumber": 86
          },
          {
            "text": "        path,",
            "lineNumber": 87
          },
          {
            "text": "        model_name,",
            "lineNumber": 88
          },
          {
            "text": "        download=False,",
            "lineNumber": 89
          },
          {
            "text": "        config=None,",
            "lineNumber": 90
          },
          {
            "text": "        device=\"cpu\",",
            "lineNumber": 91
          },
          {
            "text": "        use_head=False,",
            "lineNumber": 92
          },
          {
            "text": "        n_classes=2,",
            "lineNumber": 93
          },
          {
            "text": "    ):",
            "lineNumber": 94
          },
          {
            "text": "        # first check if it is a local path",
            "lineNumber": 95
          },
          {
            "text": "        pretrained_model_name_or_path = os.path.join(path, model_name)",
            "lineNumber": 96
          },
          {
            "text": "        if os.path.isdir(pretrained_model_name_or_path) and download == False:",
            "lineNumber": 97
          },
          {
            "text": "            if config is None:",
            "lineNumber": 98
          },
          {
            "text": "                config = json.load(",
            "lineNumber": 99
          },
          {
            "text": "                    open(os.path.join(pretrained_model_name_or_path, \"config.json\"))",
            "lineNumber": 100
          },
          {
            "text": "                )",
            "lineNumber": 101
          },
          {
            "text": "        else:",
            "lineNumber": 102
          },
          {
            "text": "            hf_url = f\"https://huggingface.co/LongSafari/{model_name}\"",
            "lineNumber": 103
          },
          {
            "lineNumber": 104
          },
          {
            "text": "            subprocess.run(f\"rm -rf {pretrained_model_name_or_path}\", shell=True)",
            "lineNumber": 105
          },
          {
            "text": "            command = (",
            "lineNumber": 106
          },
          {
            "text": "                f\"mkdir -p {path} &amp;&amp; cd {path} &amp;&amp; git lfs install &amp;&amp; git clone {hf_url}\"",
            "lineNumber": 107
          },
          {
            "text": "            )",
            "lineNumber": 108
          },
          {
            "text": "            subprocess.run(command, shell=True)",
            "lineNumber": 109
          },
          {
            "lineNumber": 110
          },
          {
            "text": "            if config is None:",
            "lineNumber": 111
          },
          {
            "text": "                config = json.load(",
            "lineNumber": 112
          },
          {
            "text": "                    open(os.path.join(pretrained_model_name_or_path, \"config.json\"))",
            "lineNumber": 113
          },
          {
            "text": "                )",
            "lineNumber": 114
          },
          {
            "lineNumber": 115
          },
          {
            "text": "        scratch_model = HyenaDNAModel(",
            "lineNumber": 116
          },
          {
            "text": "            **config, use_head=use_head, n_classes=n_classes",
            "lineNumber": 117
          },
          {
            "text": "        )  # the new model format",
            "lineNumber": 118
          },
          {
            "text": "        loaded_ckpt = torch.load(",
            "lineNumber": 119
          },
          {
            "text": "            os.path.join(pretrained_model_name_or_path, \"weights.ckpt\"),",
            "lineNumber": 120
          },
          {
            "text": "            map_location=torch.device(device),",
            "lineNumber": 121
          },
          {
            "text": "            # Needed for PyTorch &gt;= 2.6",
            "lineNumber": 122
          },
          {
            "text": "            weights_only=False,",
            "lineNumber": 123
          },
          {
            "text": "        )",
            "lineNumber": 124
          },
          {
            "lineNumber": 125
          },
          {
            "text": "        # need to load weights slightly different if using gradient checkpointing",
            "lineNumber": 126
          }
        ]
      },
      "score": 0.28375744819641113
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./process_rdp_data.py",
        "range": {
          "startPosition": {
            "line": 144
          },
          "endPosition": {
            "line": 331
          }
        },
        "contents": "def process_rdp_directory(\n    directory: str,\n    domain: str = \"Bacteria\",\n    output_file: Optional[str] = None,\n    taxonomic_level: Optional[str] = None,\n) -&gt; pd.DataFrame:\n\n\n            # Convert all count columns to integers (they're already ints in the dict, but\n            # pandas might infer float dtype, so we explicitly convert)\n            # Fill any NaN values with 0 (can occur when combining DataFrames)\n            for col in df.columns:\n                df[col] = df[col].fillna(0).astype(\"int64\")\n\n            # Print summary statistics\n            num_samples = len(df)\n            num_taxa = len(df.columns)\n            total_reads = df.sum().sum()\n\n            print(f\"  ✓ Samples: {num_samples}\")\n            print(f\"  ✓ Taxonomic groups: {num_taxa}\")\n            print(f\"  ✓ Total reads: {total_reads:,.0f}\")\n\n            all_results.append(df)\n\n        except Exception as e:\n            print(f\"  ✗ Error processing {tab_file.name}: {e}\")\n            import traceback\n\n            traceback.print_exc()\n            continue\n\n    if not all_results:\n        print(\"\\n⚠️  No data was successfully processed.\")\n        return pd.DataFrame()\n\n    # Combine all DataFrames\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Combining results from all files...\")\n    combined_df = pd.concat(all_results, axis=0)\n\n    # Fill NaN values with 0 (can occur when different files have different taxa)\n    # and ensure all count columns are integers\n    for col in combined_df.columns:\n        combined_df[col] = combined_df[col].fillna(0).astype(\"int64\")\n\n    # Filter by taxonomic level if specified\n    if taxonomic_level:\n        level_prefix = f\"{taxonomic_level}__\"\n        matching_cols = [\n            col for col in combined_df.columns if col.startswith(level_prefix)\n        ]\n        if not matching_cols:\n            print(\n                f\"\\n⚠️  Warning: No columns found for taxonomic level '{taxonomic_level}'\"\n            )\n            return pd.DataFrame()\n        combined_df = combined_df[matching_cols]\n        print(\n            f\"Filtered to {taxonomic_level} level: {len(matching_cols)} taxonomic groups\"\n        )\n\n    # Reorder columns alphabetically\n    combined_df =",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 145,
                "column": 1
              },
              "endPosition": {
                "line": 151,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def process_rdp_directory(",
            "lineNumber": 145,
            "isSignature": true
          },
          {
            "text": "    directory: str,",
            "lineNumber": 146,
            "isSignature": true
          },
          {
            "text": "    domain: str = \"Bacteria\",",
            "lineNumber": 147,
            "isSignature": true
          },
          {
            "text": "    output_file: Optional[str] = None,",
            "lineNumber": 148,
            "isSignature": true
          },
          {
            "text": "    taxonomic_level: Optional[str] = None,",
            "lineNumber": 149,
            "isSignature": true
          },
          {
            "text": ") -&gt; pd.DataFrame:",
            "lineNumber": 150,
            "isSignature": true
          },
          {
            "lineNumber": 252
          },
          {
            "lineNumber": 253
          },
          {
            "text": "            # Convert all count columns to integers (they're already ints in the dict, but",
            "lineNumber": 254
          },
          {
            "text": "            # pandas might infer float dtype, so we explicitly convert)",
            "lineNumber": 255
          },
          {
            "text": "            # Fill any NaN values with 0 (can occur when combining DataFrames)",
            "lineNumber": 256
          },
          {
            "text": "            for col in df.columns:",
            "lineNumber": 257
          },
          {
            "text": "                df[col] = df[col].fillna(0).astype(\"int64\")",
            "lineNumber": 258
          },
          {
            "lineNumber": 259
          },
          {
            "text": "            # Print summary statistics",
            "lineNumber": 260
          },
          {
            "text": "            num_samples = len(df)",
            "lineNumber": 261
          },
          {
            "text": "            num_taxa = len(df.columns)",
            "lineNumber": 262
          },
          {
            "text": "            total_reads = df.sum().sum()",
            "lineNumber": 263
          },
          {
            "lineNumber": 264
          },
          {
            "text": "            print(f\"  ✓ Samples: {num_samples}\")",
            "lineNumber": 265
          },
          {
            "text": "            print(f\"  ✓ Taxonomic groups: {num_taxa}\")",
            "lineNumber": 266
          },
          {
            "text": "            print(f\"  ✓ Total reads: {total_reads:,.0f}\")",
            "lineNumber": 267
          },
          {
            "lineNumber": 268
          },
          {
            "text": "            all_results.append(df)",
            "lineNumber": 269
          },
          {
            "lineNumber": 270
          },
          {
            "text": "        except Exception as e:",
            "lineNumber": 271
          },
          {
            "text": "            print(f\"  ✗ Error processing {tab_file.name}: {e}\")",
            "lineNumber": 272
          },
          {
            "text": "            import traceback",
            "lineNumber": 273
          },
          {
            "lineNumber": 274
          },
          {
            "text": "            traceback.print_exc()",
            "lineNumber": 275
          },
          {
            "text": "            continue",
            "lineNumber": 276
          },
          {
            "lineNumber": 277
          },
          {
            "text": "    if not all_results:",
            "lineNumber": 278
          },
          {
            "text": "        print(\"\\n⚠️  No data was successfully processed.\")",
            "lineNumber": 279
          },
          {
            "text": "        return pd.DataFrame()",
            "lineNumber": 280
          },
          {
            "lineNumber": 281
          },
          {
            "text": "    # Combine all DataFrames",
            "lineNumber": 282
          },
          {
            "text": "    print(\"\\n\" + \"=\" * 80)",
            "lineNumber": 283
          },
          {
            "text": "    print(\"Combining results from all files...\")",
            "lineNumber": 284
          },
          {
            "text": "    combined_df = pd.concat(all_results, axis=0)",
            "lineNumber": 285
          },
          {
            "lineNumber": 286
          },
          {
            "text": "    # Fill NaN values with 0 (can occur when different files have different taxa)",
            "lineNumber": 287
          },
          {
            "text": "    # and ensure all count columns are integers",
            "lineNumber": 288
          },
          {
            "text": "    for col in combined_df.columns:",
            "lineNumber": 289
          },
          {
            "text": "        combined_df[col] = combined_df[col].fillna(0).astype(\"int64\")",
            "lineNumber": 290
          },
          {
            "lineNumber": 291
          },
          {
            "text": "    # Filter by taxonomic level if specified",
            "lineNumber": 292
          },
          {
            "text": "    if taxonomic_level:",
            "lineNumber": 293
          },
          {
            "text": "        level_prefix = f\"{taxonomic_level}__\"",
            "lineNumber": 294
          },
          {
            "text": "        matching_cols = [",
            "lineNumber": 295
          },
          {
            "text": "            col for col in combined_df.columns if col.startswith(level_prefix)",
            "lineNumber": 296
          },
          {
            "text": "        ]",
            "lineNumber": 297
          },
          {
            "text": "        if not matching_cols:",
            "lineNumber": 298
          },
          {
            "text": "            print(",
            "lineNumber": 299
          },
          {
            "text": "                f\"\\n⚠️  Warning: No columns found for taxonomic level '{taxonomic_level}'\"",
            "lineNumber": 300
          },
          {
            "text": "            )",
            "lineNumber": 301
          },
          {
            "text": "            return pd.DataFrame()",
            "lineNumber": 302
          },
          {
            "text": "        combined_df = combined_df[matching_cols]",
            "lineNumber": 303
          },
          {
            "text": "        print(",
            "lineNumber": 304
          },
          {
            "text": "            f\"Filtered to {taxonomic_level} level: {len(matching_cols)} taxonomic groups\"",
            "lineNumber": 305
          },
          {
            "text": "        )",
            "lineNumber": 306
          },
          {
            "lineNumber": 307
          },
          {
            "text": "    # Reorder columns alphabetically",
            "lineNumber": 308
          },
          {
            "text": "    combined_df =",
            "lineNumber": 309
          }
        ]
      },
      "score": 0.2723759412765503
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./process_rdp_data.py",
        "range": {
          "startPosition": {
            "line": 14
          },
          "endPosition": {
            "line": 141
          }
        },
        "contents": "def process_rdp_data(filename: str, domain: str = \"Bacteria\") -&gt; dict:\ndef is_valid_lineage(lineage: str) -&gt; bool:\n\n        return False\n\n    df = df[df[\"lineage\"].apply(is_valid_lineage)]\n\n    # Get sample columns (all columns except first 4: taxid, lineage, name, rank)\n    sample_cols = df.columns[4:].tolist()\n\n    # Dictionary to accumulate counts: {sample_id: {taxon_name: count}}\n    # This structure is more memory-efficient than creating intermediate lists\n    sample_counts = {}\n\n    # Initialize all sample dictionaries\n    for sample_col in sample_cols:\n        sample_counts[sample_col] = {}\n\n    # Process each row\n    for idx, row in df.iterrows():\n        lineage = row[\"lineage\"]\n\n        # Parse taxonomic names from lineage\n        taxa = parse_lineage(lineage)\n\n        # Get counts for this row across all samples\n        for sample_col in sample_cols:\n            count = row[sample_col]\n            if pd.isna(count) or count == 0:\n                continue\n\n            # Convert to int early (source data has integer counts)\n            count = int(count)\n\n            # Add counts for each taxonomic level present\n            for taxon_name in taxa:\n                if taxon_name not in sample_counts[sample_col]:\n                    sample_counts[sample_col][taxon_name] = 0\n                sample_counts[sample_col][taxon_name] += count\n\n    return sample_counts\n\n\ndef parse_lineage(lineage: str) -&gt; list:\n    \"\"\"",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 15,
                "column": 1
              },
              "endPosition": {
                "line": 16,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def process_rdp_data(filename: str, domain: str = \"Bacteria\") -&gt; dict:",
            "lineNumber": 15,
            "isSignature": true
          },
          {
            "text": "def is_valid_lineage(lineage: str) -&gt; bool:",
            "lineNumber": 50
          },
          {
            "lineNumber": 59
          },
          {
            "text": "        return False",
            "lineNumber": 60
          },
          {
            "lineNumber": 61
          },
          {
            "text": "    df = df[df[\"lineage\"].apply(is_valid_lineage)]",
            "lineNumber": 62
          },
          {
            "lineNumber": 63
          },
          {
            "text": "    # Get sample columns (all columns except first 4: taxid, lineage, name, rank)",
            "lineNumber": 64
          },
          {
            "text": "    sample_cols = df.columns[4:].tolist()",
            "lineNumber": 65
          },
          {
            "lineNumber": 66
          },
          {
            "text": "    # Dictionary to accumulate counts: {sample_id: {taxon_name: count}}",
            "lineNumber": 67
          },
          {
            "text": "    # This structure is more memory-efficient than creating intermediate lists",
            "lineNumber": 68
          },
          {
            "text": "    sample_counts = {}",
            "lineNumber": 69
          },
          {
            "lineNumber": 70
          },
          {
            "text": "    # Initialize all sample dictionaries",
            "lineNumber": 71
          },
          {
            "text": "    for sample_col in sample_cols:",
            "lineNumber": 72
          },
          {
            "text": "        sample_counts[sample_col] = {}",
            "lineNumber": 73
          },
          {
            "lineNumber": 74
          },
          {
            "text": "    # Process each row",
            "lineNumber": 75
          },
          {
            "text": "    for idx, row in df.iterrows():",
            "lineNumber": 76
          },
          {
            "text": "        lineage = row[\"lineage\"]",
            "lineNumber": 77
          },
          {
            "lineNumber": 78
          },
          {
            "text": "        # Parse taxonomic names from lineage",
            "lineNumber": 79
          },
          {
            "text": "        taxa = parse_lineage(lineage)",
            "lineNumber": 80
          },
          {
            "lineNumber": 81
          },
          {
            "text": "        # Get counts for this row across all samples",
            "lineNumber": 82
          },
          {
            "text": "        for sample_col in sample_cols:",
            "lineNumber": 83
          },
          {
            "text": "            count = row[sample_col]",
            "lineNumber": 84
          },
          {
            "text": "            if pd.isna(count) or count == 0:",
            "lineNumber": 85
          },
          {
            "text": "                continue",
            "lineNumber": 86
          },
          {
            "lineNumber": 87
          },
          {
            "text": "            # Convert to int early (source data has integer counts)",
            "lineNumber": 88
          },
          {
            "text": "            count = int(count)",
            "lineNumber": 89
          },
          {
            "lineNumber": 90
          },
          {
            "text": "            # Add counts for each taxonomic level present",
            "lineNumber": 91
          },
          {
            "text": "            for taxon_name in taxa:",
            "lineNumber": 92
          },
          {
            "text": "                if taxon_name not in sample_counts[sample_col]:",
            "lineNumber": 93
          },
          {
            "text": "                    sample_counts[sample_col][taxon_name] = 0",
            "lineNumber": 94
          },
          {
            "text": "                sample_counts[sample_col][taxon_name] += count",
            "lineNumber": 95
          },
          {
            "lineNumber": 96
          },
          {
            "text": "    return sample_counts",
            "lineNumber": 97,
            "isSignature": true
          },
          {
            "lineNumber": 98
          },
          {
            "lineNumber": 99
          },
          {
            "text": "def parse_lineage(lineage: str) -&gt; list:",
            "lineNumber": 100,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 101
          }
        ]
      },
      "score": 0.267822265625
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./process_rdp_data.py",
        "range": {
          "startPosition": {
            "line": 331,
            "column": 22
          },
          "endPosition": {
            "line": 373,
            "column": 26
          }
        },
        "contents": "if __name__ == \"__main__\":\n    # Example usage\n    import sys\n\n    if len(sys.argv) &lt; 2:\n        print(\"Usage:\")\n        print(\"  Single file: python process_rdp_data.py &lt;filename&gt; [domain]\")\n        print(\n            \"  Directory:   python process_rdp_data.py &lt;directory&gt; [domain] [taxonomic_level] [output_file]\"\n        )\n        print(\"\\nExamples:\")\n        print(\"  python process_rdp_data.py data/RDP_classifier/APV+20.tab Bacteria\")\n        print(\"  python process_rdp_data.py data/RDP_classifier Bacteria\")\n        print(\"  python process_rdp_data.py data/RDP_classifier Bacteria phylum\")\n        print(\n            \"  python process_rdp_data.py data/RDP_classifier Bacteria phylum data/bacteria_phylum_counts.csv\"\n        )\n        print(\"\\nValid taxonomic levels: phylum, class, order, family, genus\")\n        sys.exit(1)\n\n    path = sys.argv[1]\n    domain = sys.argv[2] if len(sys.argv) &gt; 2 else \"Bacteria\"\n    taxonomic_level = None\n    output_file = None\n\n    # Parse remaining arguments\n    # Check if 3rd arg is a taxonomic level or output file\n    if len(sys.argv) &gt; 3:\n        arg3 = sys.argv[3]\n        valid_levels = [\"phylum\", \"class\", \"order\", \"family\", \"genus\"]\n        if arg3 in valid_levels:\n            taxonomic_level = arg3\n            output_file = sys.argv[4] if len(sys.argv) &gt; 4 else None\n        else:\n            # Assume it's an output file\n            output_file = arg3\n\n    path_obj = Path(path)\n\n    if path_obj.is_file():",
        "signatures": {},
        "detailedLines": [
          {
            "text": "if __name__ == \"__main__\":",
            "lineNumber": 335
          },
          {
            "text": "    # Example usage",
            "lineNumber": 336
          },
          {
            "text": "    import sys",
            "lineNumber": 337
          },
          {
            "lineNumber": 338
          },
          {
            "text": "    if len(sys.argv) &lt; 2:",
            "lineNumber": 339
          },
          {
            "text": "        print(\"Usage:\")",
            "lineNumber": 340
          },
          {
            "text": "        print(\"  Single file: python process_rdp_data.py &lt;filename&gt; [domain]\")",
            "lineNumber": 341
          },
          {
            "text": "        print(",
            "lineNumber": 342
          },
          {
            "text": "            \"  Directory:   python process_rdp_data.py &lt;directory&gt; [domain] [taxonomic_level] [output_file]\"",
            "lineNumber": 343
          },
          {
            "text": "        )",
            "lineNumber": 344
          },
          {
            "text": "        print(\"\\nExamples:\")",
            "lineNumber": 345
          },
          {
            "text": "        print(\"  python process_rdp_data.py data/RDP_classifier/APV+20.tab Bacteria\")",
            "lineNumber": 346
          },
          {
            "text": "        print(\"  python process_rdp_data.py data/RDP_classifier Bacteria\")",
            "lineNumber": 347
          },
          {
            "text": "        print(\"  python process_rdp_data.py data/RDP_classifier Bacteria phylum\")",
            "lineNumber": 348
          },
          {
            "text": "        print(",
            "lineNumber": 349
          },
          {
            "text": "            \"  python process_rdp_data.py data/RDP_classifier Bacteria phylum data/bacteria_phylum_counts.csv\"",
            "lineNumber": 350
          },
          {
            "text": "        )",
            "lineNumber": 351
          },
          {
            "text": "        print(\"\\nValid taxonomic levels: phylum, class, order, family, genus\")",
            "lineNumber": 352
          },
          {
            "text": "        sys.exit(1)",
            "lineNumber": 353
          },
          {
            "lineNumber": 354
          },
          {
            "text": "    path = sys.argv[1]",
            "lineNumber": 355
          },
          {
            "text": "    domain = sys.argv[2] if len(sys.argv) &gt; 2 else \"Bacteria\"",
            "lineNumber": 356
          },
          {
            "text": "    taxonomic_level = None",
            "lineNumber": 357
          },
          {
            "text": "    output_file = None",
            "lineNumber": 358
          },
          {
            "lineNumber": 359
          },
          {
            "text": "    # Parse remaining arguments",
            "lineNumber": 360
          },
          {
            "text": "    # Check if 3rd arg is a taxonomic level or output file",
            "lineNumber": 361
          },
          {
            "text": "    if len(sys.argv) &gt; 3:",
            "lineNumber": 362
          },
          {
            "text": "        arg3 = sys.argv[3]",
            "lineNumber": 363
          },
          {
            "text": "        valid_levels = [\"phylum\", \"class\", \"order\", \"family\", \"genus\"]",
            "lineNumber": 364
          },
          {
            "text": "        if arg3 in valid_levels:",
            "lineNumber": 365
          },
          {
            "text": "            taxonomic_level = arg3",
            "lineNumber": 366
          },
          {
            "text": "            output_file = sys.argv[4] if len(sys.argv) &gt; 4 else None",
            "lineNumber": 367
          },
          {
            "text": "        else:",
            "lineNumber": 368
          },
          {
            "text": "            # Assume it's an output file",
            "lineNumber": 369
          },
          {
            "text": "            output_file = arg3",
            "lineNumber": 370
          },
          {
            "lineNumber": 371
          },
          {
            "text": "    path_obj = Path(path)",
            "lineNumber": 372
          },
          {
            "lineNumber": 373
          },
          {
            "text": "    if path_obj.is_file():",
            "lineNumber": 374
          }
        ]
      },
      "score": 0.26416081190109253
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./predict_ph.py",
        "range": {
          "startPosition": {
            "line": 32,
            "column": 33
          },
          "endPosition": {
            "line": 120
          }
        },
        "contents": "def load_and_preprocess_data(sample_data_path, phylum_counts_path, random_seed=42):\n    \"\"\"\n    Load and preprocess data for pH prediction.\n\n    Parameters\n    ----------\n    sample_data_path : str\n        Path to sample_data.csv\n    phylum_counts_path : str\n        Path to bacteria_phylum_counts.csv.xz (compressed)\n    random_seed : int\n        Random seed for reproducibility\n\n    Returns\n    -------\n    tuple\n        (X_train, X_test, y_train, y_test, feature_names, test_metadata)\n        where test_metadata is a DataFrame with study_name and sample_id for test set\n    \"\"\"\n    # Load sample data\n    print(\"Loading sample data...\")\n    sample_data = pd.read_csv(sample_data_path)\n\n    # Filter to Bacteria domain and remove missing pH\n    print(\"Filtering data...\")\n    sample_data = sample_data[sample_data[\"domain\"] == \"Bacteria\"].copy()\n    sample_data = sample_data.dropna(subset=[\"pH\"])\n\n    print(f\"Sample data shape after filtering: {sample_data.shape}\")\n\n    # Load phylum counts (pandas can read .xz compressed files directly)\n    print(\"Loading phylum counts from compressed file...\")\n    phylum_counts = pd.read_csv(phylum_counts_path, compression=\"xz\")\n\n    print(f\"Phylum counts shape: {phylum_counts.shape}\")\n\n    # Join on study_name and sample_id\n    print(\"Joining datasets...\")\n    merged_data = sample_data.merge(\n        phylum_counts,\n        left_on=[\"study_name\", \"sample_id\"],\n        right_on=[\"study_name\", \"sample_id\"],\n        how=\"inner\",\n    )\n\n    print(f\"Merged data shape: {merged_data.shape}\")\n\n    # Identify phylum count columns (all columns starting with 'phylum__')\n    phylum_cols = [col for col in merged_data.columns if col.startswith(\"phylum__\")]\n    print(f\"Number of phylum features: {len(phylum_cols)}\")\n\n    # Extract features (phylum counts) and target (pH)\n    X = merged_data[phylum_cols].values\n    y = merged_data[\"pH\"].values\n    environment = merged_data[\"environment\"].values\n\n    # Store metadata for test set (study_name and sample_id)\n    metadata = merged_data[[\"study_name\", \"sample_id\"]].copy()\n\n    # Create stratified train-test split (80-20)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def load_and_preprocess_data(sample_data_path, phylum_counts_path, random_seed=42):",
            "lineNumber": 36,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 37
          },
          {
            "text": "    Load and preprocess data for pH prediction.",
            "lineNumber": 38
          },
          {
            "lineNumber": 39
          },
          {
            "text": "    Parameters",
            "lineNumber": 40
          },
          {
            "text": "    ----------",
            "lineNumber": 41
          },
          {
            "text": "    sample_data_path : str",
            "lineNumber": 42
          },
          {
            "text": "        Path to sample_data.csv",
            "lineNumber": 43
          },
          {
            "text": "    phylum_counts_path : str",
            "lineNumber": 44
          },
          {
            "text": "        Path to bacteria_phylum_counts.csv.xz (compressed)",
            "lineNumber": 45
          },
          {
            "text": "    random_seed : int",
            "lineNumber": 46
          },
          {
            "text": "        Random seed for reproducibility",
            "lineNumber": 47
          },
          {
            "lineNumber": 48
          },
          {
            "text": "    Returns",
            "lineNumber": 49
          },
          {
            "text": "    -------",
            "lineNumber": 50
          },
          {
            "text": "    tuple",
            "lineNumber": 51
          },
          {
            "text": "        (X_train, X_test, y_train, y_test, feature_names, test_metadata)",
            "lineNumber": 52
          },
          {
            "text": "        where test_metadata is a DataFrame with study_name and sample_id for test set",
            "lineNumber": 53
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 54
          },
          {
            "text": "    # Load sample data",
            "lineNumber": 55
          },
          {
            "text": "    print(\"Loading sample data...\")",
            "lineNumber": 56
          },
          {
            "text": "    sample_data = pd.read_csv(sample_data_path)",
            "lineNumber": 57
          },
          {
            "lineNumber": 58
          },
          {
            "text": "    # Filter to Bacteria domain and remove missing pH",
            "lineNumber": 59
          },
          {
            "text": "    print(\"Filtering data...\")",
            "lineNumber": 60
          },
          {
            "text": "    sample_data = sample_data[sample_data[\"domain\"] == \"Bacteria\"].copy()",
            "lineNumber": 61
          },
          {
            "text": "    sample_data = sample_data.dropna(subset=[\"pH\"])",
            "lineNumber": 62
          },
          {
            "lineNumber": 63
          },
          {
            "text": "    print(f\"Sample data shape after filtering: {sample_data.shape}\")",
            "lineNumber": 64
          },
          {
            "lineNumber": 65
          },
          {
            "text": "    # Load phylum counts (pandas can read .xz compressed files directly)",
            "lineNumber": 66
          },
          {
            "text": "    print(\"Loading phylum counts from compressed file...\")",
            "lineNumber": 67
          },
          {
            "text": "    phylum_counts = pd.read_csv(phylum_counts_path, compression=\"xz\")",
            "lineNumber": 68
          },
          {
            "lineNumber": 69
          },
          {
            "text": "    print(f\"Phylum counts shape: {phylum_counts.shape}\")",
            "lineNumber": 70
          },
          {
            "lineNumber": 71
          },
          {
            "text": "    # Join on study_name and sample_id",
            "lineNumber": 72
          },
          {
            "text": "    print(\"Joining datasets...\")",
            "lineNumber": 73
          },
          {
            "text": "    merged_data = sample_data.merge(",
            "lineNumber": 74
          },
          {
            "text": "        phylum_counts,",
            "lineNumber": 75
          },
          {
            "text": "        left_on=[\"study_name\", \"sample_id\"],",
            "lineNumber": 76
          },
          {
            "text": "        right_on=[\"study_name\", \"sample_id\"],",
            "lineNumber": 77
          },
          {
            "text": "        how=\"inner\",",
            "lineNumber": 78
          },
          {
            "text": "    )",
            "lineNumber": 79
          },
          {
            "lineNumber": 80
          },
          {
            "text": "    print(f\"Merged data shape: {merged_data.shape}\")",
            "lineNumber": 81
          },
          {
            "lineNumber": 82
          },
          {
            "text": "    # Identify phylum count columns (all columns starting with 'phylum__')",
            "lineNumber": 83
          },
          {
            "text": "    phylum_cols = [col for col in merged_data.columns if col.startswith(\"phylum__\")]",
            "lineNumber": 84
          },
          {
            "text": "    print(f\"Number of phylum features: {len(phylum_cols)}\")",
            "lineNumber": 85
          },
          {
            "lineNumber": 86
          },
          {
            "text": "    # Extract features (phylum counts) and target (pH)",
            "lineNumber": 87
          },
          {
            "text": "    X = merged_data[phylum_cols].values",
            "lineNumber": 88
          },
          {
            "text": "    y = merged_data[\"pH\"].values",
            "lineNumber": 89
          },
          {
            "text": "    environment = merged_data[\"environment\"].values",
            "lineNumber": 90
          },
          {
            "lineNumber": 91
          },
          {
            "text": "    # Store metadata for test set (study_name and sample_id)",
            "lineNumber": 92
          },
          {
            "text": "    metadata = merged_data[[\"study_name\", \"sample_id\"]].copy()",
            "lineNumber": 93
          },
          {
            "lineNumber": 94
          },
          {
            "text": "    # Create stratified train-test split (80-20)",
            "lineNumber": 95
          }
        ]
      },
      "score": 0.2485080361366272
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./create_hf_dataset.py",
        "range": {
          "startPosition": {
            "line": 88,
            "column": 20
          },
          "endPosition": {
            "line": 149
          }
        },
        "contents": "def load_sample_data(csv_path: Path) -&gt; Dict[Tuple[str, str], Dict]:\n    \"\"\"\n    Load sample data from CSV and return a dictionary mapping (study_name, sample_id) -&gt; data.\n    \"\"\"\n    sample_data = {}\n    with open(csv_path, \"r\") as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            study_name = row[\"study_name\"]\n            sample_id = row[\"sample_id\"]\n            key = (study_name, sample_id)\n            sample_data[key] = {\n                \"study_name\": study_name,\n                \"sample_id\": sample_id,\n                \"pH\": float(row[\"pH\"]) if row[\"pH\"] and row[\"pH\"] != \"NA\" else None,\n                \"sample\": row[\"sample\"],\n                \"environment\": row[\"environment\"],\n                \"domain\": row[\"domain\"],\n            }\n    return sample_data\n\n\ndef validate_fasta_files(\n    fasta_dir: Path, sample_data: Dict[Tuple[str, str], Dict]\n) -&gt; List[Tuple[Path, str, str]]:\n    ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def load_sample_data(csv_path: Path) -&gt; Dict[Tuple[str, str], Dict]:",
            "lineNumber": 92,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 93
          },
          {
            "text": "    Load sample data from CSV and return a dictionary mapping (study_name, sample_id) -&gt; data.",
            "lineNumber": 94
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 95
          },
          {
            "text": "    sample_data = {}",
            "lineNumber": 96
          },
          {
            "text": "    with open(csv_path, \"r\") as f:",
            "lineNumber": 97
          },
          {
            "text": "        reader = csv.DictReader(f)",
            "lineNumber": 98
          },
          {
            "text": "        for row in reader:",
            "lineNumber": 99
          },
          {
            "text": "            study_name = row[\"study_name\"]",
            "lineNumber": 100
          },
          {
            "text": "            sample_id = row[\"sample_id\"]",
            "lineNumber": 101
          },
          {
            "text": "            key = (study_name, sample_id)",
            "lineNumber": 102
          },
          {
            "text": "            sample_data[key] = {",
            "lineNumber": 103
          },
          {
            "text": "                \"study_name\": study_name,",
            "lineNumber": 104
          },
          {
            "text": "                \"sample_id\": sample_id,",
            "lineNumber": 105
          },
          {
            "text": "                \"pH\": float(row[\"pH\"]) if row[\"pH\"] and row[\"pH\"] != \"NA\" else None,",
            "lineNumber": 106
          },
          {
            "text": "                \"sample\": row[\"sample\"],",
            "lineNumber": 107
          },
          {
            "text": "                \"environment\": row[\"environment\"],",
            "lineNumber": 108
          },
          {
            "text": "                \"domain\": row[\"domain\"],",
            "lineNumber": 109
          },
          {
            "text": "            }",
            "lineNumber": 110
          },
          {
            "text": "    return sample_data",
            "lineNumber": 111,
            "isSignature": true
          },
          {
            "lineNumber": 112
          },
          {
            "lineNumber": 113
          },
          {
            "text": "def validate_fasta_files(",
            "lineNumber": 114,
            "isSignature": true
          },
          {
            "text": "    fasta_dir: Path, sample_data: Dict[Tuple[str, str], Dict]",
            "lineNumber": 115,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[Tuple[Path, str, str]]:",
            "lineNumber": 116,
            "isSignature": true
          },
          {
            "text": "    ",
            "lineNumber": 117
          }
        ]
      },
      "score": 0.2190113663673401
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./README.md",
        "range": {
          "startPosition": {
            "line": 32
          },
          "endPosition": {
            "line": 81
          }
        },
        "contents": "- **Incremental updates**: Skips samples that already exist in the dataset, allowing for incremental additions\n- **Size management**: Limits total DNA length per sample to 1 MB to manage dataset size\n- **Metadata integration**: Includes sample metadata (pH, environment, domain) alongside sequences\n\n&lt;/details&gt;\n\n## pH prediction workflow\n\nThe &#96;predict_ph.py&#96; script implements a machine learning pipeline for predicting pH values from bacterial phylum abundances. The workflow includes data loading and preprocessing, train-test split, model training, and output.\n\n&#96;&#96;&#96;bash\n# Test grid search with KNN (default, 4 combinations)\npython predict_ph.py --model knn --grid-search test\n\n# Full grid search with Random Forest (comprehensive)\npython predict_ph.py --model rf --grid-search full\n\n# Skip grid search and use default parameters\npython predict_ph.py --model hgb --grid-search none\n\n# Custom CV folds and random seed\npython predict_ph.py --model knn --grid-search test --cv-folds 10 --random-seed 123\n\n# Specify custom data paths and output directory\npython predict_ph.py --model rf \\\n    --sample-data data/sample_data.csv \\\n    --phylum-counts data/bacteria_phylum_counts.csv.xz \\\n    --output-dir results/rf_experiment\n&#96;&#96;&#96;\n\n&lt;details&gt;\n&lt;summary&gt;Details&lt;/summary&gt;\n\n1. **Data Loading and Preprocessing**:\n   - Filters sample data to include only the domain Bacteria\n   - Removes samples with missing pH values\n   - Joins sample metadata with phylum count data\n   - Performs L1 normalization to convert counts to relative abundances\n\n2. **Train-Test Split**:\n   - Creates an 80-20 stratified split based on environment\n   - Ensures balanced representation of environment types in both sets\n\n3. **Model Training**:\n   - Optional grid search with progress bar for hyperparameter tuning\n   - Uses scikit-learn Pipeline for preprocessing and model fitting\n\n4. **Output**:\n   - Saves test set predictions to CSV files with study names and sample IDs",
        "signatures": {},
        "detailedLines": [
          {
            "text": "- **Incremental updates**: Skips samples that already exist in the dataset, allowing for incremental additions",
            "lineNumber": 33
          },
          {
            "text": "- **Size management**: Limits total DNA length per sample to 1 MB to manage dataset size",
            "lineNumber": 34
          },
          {
            "text": "- **Metadata integration**: Includes sample metadata (pH, environment, domain) alongside sequences",
            "lineNumber": 35
          },
          {
            "lineNumber": 36
          },
          {
            "text": "&lt;/details&gt;",
            "lineNumber": 37
          },
          {
            "lineNumber": 38
          },
          {
            "text": "## pH prediction workflow",
            "lineNumber": 39
          },
          {
            "lineNumber": 40
          },
          {
            "text": "The &#96;predict_ph.py&#96; script implements a machine learning pipeline for predicting pH values from bacterial phylum abundances. The workflow includes data loading and preprocessing, train-test split, model training, and output.",
            "lineNumber": 41
          },
          {
            "lineNumber": 42
          },
          {
            "text": "&#96;&#96;&#96;bash",
            "lineNumber": 43
          },
          {
            "text": "# Test grid search with KNN (default, 4 combinations)",
            "lineNumber": 44
          },
          {
            "text": "python predict_ph.py --model knn --grid-search test",
            "lineNumber": 45
          },
          {
            "lineNumber": 46
          },
          {
            "text": "# Full grid search with Random Forest (comprehensive)",
            "lineNumber": 47
          },
          {
            "text": "python predict_ph.py --model rf --grid-search full",
            "lineNumber": 48
          },
          {
            "lineNumber": 49
          },
          {
            "text": "# Skip grid search and use default parameters",
            "lineNumber": 50
          },
          {
            "text": "python predict_ph.py --model hgb --grid-search none",
            "lineNumber": 51
          },
          {
            "lineNumber": 52
          },
          {
            "text": "# Custom CV folds and random seed",
            "lineNumber": 53
          },
          {
            "text": "python predict_ph.py --model knn --grid-search test --cv-folds 10 --random-seed 123",
            "lineNumber": 54
          },
          {
            "lineNumber": 55
          },
          {
            "text": "# Specify custom data paths and output directory",
            "lineNumber": 56
          },
          {
            "text": "python predict_ph.py --model rf \\",
            "lineNumber": 57
          },
          {
            "text": "    --sample-data data/sample_data.csv \\",
            "lineNumber": 58
          },
          {
            "text": "    --phylum-counts data/bacteria_phylum_counts.csv.xz \\",
            "lineNumber": 59
          },
          {
            "text": "    --output-dir results/rf_experiment",
            "lineNumber": 60
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 61
          },
          {
            "lineNumber": 62
          },
          {
            "text": "&lt;details&gt;",
            "lineNumber": 63
          },
          {
            "text": "&lt;summary&gt;Details&lt;/summary&gt;",
            "lineNumber": 64
          },
          {
            "lineNumber": 65
          },
          {
            "text": "1. **Data Loading and Preprocessing**:",
            "lineNumber": 66
          },
          {
            "text": "   - Filters sample data to include only the domain Bacteria",
            "lineNumber": 67
          },
          {
            "text": "   - Removes samples with missing pH values",
            "lineNumber": 68
          },
          {
            "text": "   - Joins sample metadata with phylum count data",
            "lineNumber": 69
          },
          {
            "text": "   - Performs L1 normalization to convert counts to relative abundances",
            "lineNumber": 70
          },
          {
            "lineNumber": 71
          },
          {
            "text": "2. **Train-Test Split**:",
            "lineNumber": 72
          },
          {
            "text": "   - Creates an 80-20 stratified split based on environment",
            "lineNumber": 73
          },
          {
            "text": "   - Ensures balanced representation of environment types in both sets",
            "lineNumber": 74
          },
          {
            "lineNumber": 75
          },
          {
            "text": "3. **Model Training**:",
            "lineNumber": 76
          },
          {
            "text": "   - Optional grid search with progress bar for hyperparameter tuning",
            "lineNumber": 77
          },
          {
            "text": "   - Uses scikit-learn Pipeline for preprocessing and model fitting",
            "lineNumber": 78
          },
          {
            "lineNumber": 79
          },
          {
            "text": "4. **Output**:",
            "lineNumber": 80
          },
          {
            "text": "   - Saves test set predictions to CSV files with study names and sample IDs",
            "lineNumber": 81
          }
        ]
      },
      "score": 0.2164520025253296
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./predict_ph.py",
        "range": {
          "startPosition": {
            "line": 259,
            "column": 53
          },
          "endPosition": {
            "line": 479
          }
        },
        "contents": "def main():\n    parser = argparse.ArgumentParser(\n        description=\"Predict pH from bacterial abundances using regression models\"\n    )\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        required=True,\n        choices=[\"linear\", \"knn\", \"rf\", \"hgb\"],\n        help=\"Model type: linear (Linear Regression), knn (KNN Regression), \"\n        \"rf (Random Forest), or hgb (HistGradientBoosting)\",\n    )\n    parser.add_argument(\n        \"--sample-data\",\n        type=str,\n        default=\"data/sample_data.csv\",\n        help=\"Path to sample_data.csv (default: data/sample_data.csv)\",\n    )\n    parser.add_argument(\n        \"--phylum-counts\",\n        type=str,\n        default=\"data/bacteria_phylum_counts.csv.xz\",\n        help=\"Path to bacteria_phylum_counts.csv.xz (default: data/bacteria_phylum_counts.csv.xz)\",\n    )\n    parser.add_argument(\n        \"--random-seed\",\n        type=int,\n        default=42,\n        help=\"Random seed for reproducibility (default: 42)\",\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        type=str,\n        default=\"results\",\n        help=\"Output directory for results (default: results)\",\n    )\n    parser.add_argument(\n        \"--grid-search\",\n        type=str,\n        choices=[\"test\", \"full\", \"none\"],\n        default=\"test\",\n        help='Grid search mode: \"test\" for small grids (2-4 combinations), '\n        '\"full\" for comprehensive grids, or \"none\" to skip (default: test)',\n    )\n    parser.add_argument(\n        \"--cv-folds\",\n        type=int,\n        default=5,\n        help=\"Number of CV folds for grid search (default: 5)\",\n    )\n\n    args = parser.parse_args()\n\n    # Create output directory if it doesn't exist\n    output_dir = Path(args.output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Load and preprocess data\n    X_train, X_test, y_train, y_test, feature_names, test_metadata = (\n        load_and_preprocess_data(args.sample_data, args.phylum_counts, args.random_seed)\n    )\n\n    # Create pipeline\n    print(f\"\\nCreating {args.model} pipeline...\")",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def main():",
            "lineNumber": 263,
            "isSignature": true
          },
          {
            "text": "    parser = argparse.ArgumentParser(",
            "lineNumber": 264
          },
          {
            "text": "        description=\"Predict pH from bacterial abundances using regression models\"",
            "lineNumber": 265
          },
          {
            "text": "    )",
            "lineNumber": 266
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 267
          },
          {
            "text": "        \"--model\",",
            "lineNumber": 268
          },
          {
            "text": "        type=str,",
            "lineNumber": 269
          },
          {
            "text": "        required=True,",
            "lineNumber": 270
          },
          {
            "text": "        choices=[\"linear\", \"knn\", \"rf\", \"hgb\"],",
            "lineNumber": 271
          },
          {
            "text": "        help=\"Model type: linear (Linear Regression), knn (KNN Regression), \"",
            "lineNumber": 272
          },
          {
            "text": "        \"rf (Random Forest), or hgb (HistGradientBoosting)\",",
            "lineNumber": 273
          },
          {
            "text": "    )",
            "lineNumber": 274
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 275
          },
          {
            "text": "        \"--sample-data\",",
            "lineNumber": 276
          },
          {
            "text": "        type=str,",
            "lineNumber": 277
          },
          {
            "text": "        default=\"data/sample_data.csv\",",
            "lineNumber": 278
          },
          {
            "text": "        help=\"Path to sample_data.csv (default: data/sample_data.csv)\",",
            "lineNumber": 279
          },
          {
            "text": "    )",
            "lineNumber": 280
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 281
          },
          {
            "text": "        \"--phylum-counts\",",
            "lineNumber": 282
          },
          {
            "text": "        type=str,",
            "lineNumber": 283
          },
          {
            "text": "        default=\"data/bacteria_phylum_counts.csv.xz\",",
            "lineNumber": 284
          },
          {
            "text": "        help=\"Path to bacteria_phylum_counts.csv.xz (default: data/bacteria_phylum_counts.csv.xz)\",",
            "lineNumber": 285
          },
          {
            "text": "    )",
            "lineNumber": 286
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 287
          },
          {
            "text": "        \"--random-seed\",",
            "lineNumber": 288
          },
          {
            "text": "        type=int,",
            "lineNumber": 289
          },
          {
            "text": "        default=42,",
            "lineNumber": 290
          },
          {
            "text": "        help=\"Random seed for reproducibility (default: 42)\",",
            "lineNumber": 291
          },
          {
            "text": "    )",
            "lineNumber": 292
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 293
          },
          {
            "text": "        \"--output-dir\",",
            "lineNumber": 294
          },
          {
            "text": "        type=str,",
            "lineNumber": 295
          },
          {
            "text": "        default=\"results\",",
            "lineNumber": 296
          },
          {
            "text": "        help=\"Output directory for results (default: results)\",",
            "lineNumber": 297
          },
          {
            "text": "    )",
            "lineNumber": 298
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 299
          },
          {
            "text": "        \"--grid-search\",",
            "lineNumber": 300
          },
          {
            "text": "        type=str,",
            "lineNumber": 301
          },
          {
            "text": "        choices=[\"test\", \"full\", \"none\"],",
            "lineNumber": 302
          },
          {
            "text": "        default=\"test\",",
            "lineNumber": 303
          },
          {
            "text": "        help='Grid search mode: \"test\" for small grids (2-4 combinations), '",
            "lineNumber": 304
          },
          {
            "text": "        '\"full\" for comprehensive grids, or \"none\" to skip (default: test)',",
            "lineNumber": 305
          },
          {
            "text": "    )",
            "lineNumber": 306
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 307
          },
          {
            "text": "        \"--cv-folds\",",
            "lineNumber": 308
          },
          {
            "text": "        type=int,",
            "lineNumber": 309
          },
          {
            "text": "        default=5,",
            "lineNumber": 310
          },
          {
            "text": "        help=\"Number of CV folds for grid search (default: 5)\",",
            "lineNumber": 311
          },
          {
            "text": "    )",
            "lineNumber": 312
          },
          {
            "lineNumber": 313
          },
          {
            "text": "    args = parser.parse_args()",
            "lineNumber": 314
          },
          {
            "lineNumber": 315
          },
          {
            "text": "    # Create output directory if it doesn't exist",
            "lineNumber": 316
          },
          {
            "text": "    output_dir = Path(args.output_dir)",
            "lineNumber": 317
          },
          {
            "text": "    output_dir.mkdir(parents=True, exist_ok=True)",
            "lineNumber": 318
          },
          {
            "lineNumber": 319
          },
          {
            "text": "    # Load and preprocess data",
            "lineNumber": 320
          },
          {
            "text": "    X_train, X_test, y_train, y_test, feature_names, test_metadata = (",
            "lineNumber": 321
          },
          {
            "text": "        load_and_preprocess_data(args.sample_data, args.phylum_counts, args.random_seed)",
            "lineNumber": 322
          },
          {
            "text": "    )",
            "lineNumber": 323
          },
          {
            "lineNumber": 324
          },
          {
            "text": "    # Create pipeline",
            "lineNumber": 325
          },
          {
            "text": "    print(f\"\\nCreating {args.model} pipeline...\")",
            "lineNumber": 326
          }
        ]
      },
      "score": 0.21428820490837097
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./process_rdp_data.py",
        "range": {
          "startPosition": {
            "line": 144
          },
          "endPosition": {
            "line": 336,
            "column": 14
          }
        },
        "contents": "def process_rdp_directory(\n    directory: str,\n    domain: str = \"Bacteria\",\n    output_file: Optional[str] = None,\n    taxonomic_level: Optional[str] = None,\n) -&gt; pd.DataFrame:\n\n\n    # Reorder columns alphabetically\n    combined_df = combined_df.reindex(sorted(combined_df.columns), axis=1)\n\n    # Print combined summary\n    print(f\"\\nCombined results:\")\n    print(f\"  Total samples: {len(combined_df)}\")\n    print(f\"  Total taxonomic groups: {len(combined_df.columns)}\")\n    print(f\"  Total reads: {combined_df.sum().sum():,.0f}\")\n    print(f\"  Studies: {combined_df.index.get_level_values('study_name').nunique()}\")\n\n    # Save to CSV\n    if output_file is None:\n        if taxonomic_level:\n            output_file = f\"data/{domain.lower()}_{taxonomic_level}_counts.csv\"\n        else:\n            output_file = f\"data/{domain.lower()}_counts.csv\"\n\n    output_path = Path(output_file)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    print(f\"\\nSaving to: {output_file}\")\n    combined_df.to_csv(output_file)\n    print(f\"✓ Saved successfully!\")\n\n    return combined_df\n\n\nif __name__ == \"__main__\":\n    # Example usage\n    import sys",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 145,
                "column": 1
              },
              "endPosition": {
                "line": 151,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def process_rdp_directory(",
            "lineNumber": 145,
            "isSignature": true
          },
          {
            "text": "    directory: str,",
            "lineNumber": 146,
            "isSignature": true
          },
          {
            "text": "    domain: str = \"Bacteria\",",
            "lineNumber": 147,
            "isSignature": true
          },
          {
            "text": "    output_file: Optional[str] = None,",
            "lineNumber": 148,
            "isSignature": true
          },
          {
            "text": "    taxonomic_level: Optional[str] = None,",
            "lineNumber": 149,
            "isSignature": true
          },
          {
            "text": ") -&gt; pd.DataFrame:",
            "lineNumber": 150,
            "isSignature": true
          },
          {
            "lineNumber": 306
          },
          {
            "lineNumber": 307
          },
          {
            "text": "    # Reorder columns alphabetically",
            "lineNumber": 308
          },
          {
            "text": "    combined_df = combined_df.reindex(sorted(combined_df.columns), axis=1)",
            "lineNumber": 309
          },
          {
            "lineNumber": 310
          },
          {
            "text": "    # Print combined summary",
            "lineNumber": 311
          },
          {
            "text": "    print(f\"\\nCombined results:\")",
            "lineNumber": 312
          },
          {
            "text": "    print(f\"  Total samples: {len(combined_df)}\")",
            "lineNumber": 313
          },
          {
            "text": "    print(f\"  Total taxonomic groups: {len(combined_df.columns)}\")",
            "lineNumber": 314
          },
          {
            "text": "    print(f\"  Total reads: {combined_df.sum().sum():,.0f}\")",
            "lineNumber": 315
          },
          {
            "text": "    print(f\"  Studies: {combined_df.index.get_level_values('study_name').nunique()}\")",
            "lineNumber": 316
          },
          {
            "lineNumber": 317
          },
          {
            "text": "    # Save to CSV",
            "lineNumber": 318
          },
          {
            "text": "    if output_file is None:",
            "lineNumber": 319
          },
          {
            "text": "        if taxonomic_level:",
            "lineNumber": 320
          },
          {
            "text": "            output_file = f\"data/{domain.lower()}_{taxonomic_level}_counts.csv\"",
            "lineNumber": 321
          },
          {
            "text": "        else:",
            "lineNumber": 322
          },
          {
            "text": "            output_file = f\"data/{domain.lower()}_counts.csv\"",
            "lineNumber": 323
          },
          {
            "lineNumber": 324
          },
          {
            "text": "    output_path = Path(output_file)",
            "lineNumber": 325
          },
          {
            "text": "    output_path.parent.mkdir(parents=True, exist_ok=True)",
            "lineNumber": 326
          },
          {
            "lineNumber": 327
          },
          {
            "text": "    print(f\"\\nSaving to: {output_file}\")",
            "lineNumber": 328
          },
          {
            "text": "    combined_df.to_csv(output_file)",
            "lineNumber": 329
          },
          {
            "text": "    print(f\"✓ Saved successfully!\")",
            "lineNumber": 330
          },
          {
            "lineNumber": 331
          },
          {
            "text": "    return combined_df",
            "lineNumber": 332,
            "isSignature": true
          },
          {
            "lineNumber": 333
          },
          {
            "lineNumber": 334
          },
          {
            "text": "if __name__ == \"__main__\":",
            "lineNumber": 335
          },
          {
            "text": "    # Example usage",
            "lineNumber": 336
          },
          {
            "text": "    import sys",
            "lineNumber": 337
          }
        ]
      },
      "score": 0.21303856372833252
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./process_rdp_data.py",
        "range": {
          "startPosition": {
            "line": 96,
            "column": 24
          },
          "endPosition": {
            "line": 331
          }
        },
        "contents": "def parse_lineage(lineage: str) -&gt; list:\n    \"\"\"\n    Parse lineage string to extract taxonomic names at each level.\n\n    Parameters\n    ----------\n    lineage : str\n        Lineage string in format:\n        \"Root;rootrank;{domain};domain;{phylum};phylum;{class};class;...\"\n\n    Returns\n    -------\n    list\n        List of taxonomic names with prefixes (e.g.,\n        [\"phylum__Omnitrophota\", \"class__Koll11\", ...])\n    \"\"\"\n    taxa = []\n\n    # Pattern to match taxonomic levels: {name};{level};\n    # We want to extract pairs where level is phylum, class, order, family, or genus\n    pattern = r\"([^;]+);(phylum|class|order|family|genus);\"\n    matches = re.findall(pattern, lineage)\n\n    for name, level in matches:\n        # Skip empty names\n        if not name or name.strip() == \"\":\n            continue\n        taxon_name = f\"{level}__{name}\"\n        taxa.append(taxon_name)\n\n    # Handle unclassified case: ...;unclassified_XXX;;\n    # For unclassified lineages, we only include the known taxonomic levels\n    # (those that appear before the unclassified marker), not the unclassified itself\n    # The unclassified marker indicates unknown classification at that level,\n    # so we don't create a separate taxon entry for it\n    unclassified_match = re.search(r\";unclassified_([^;]+);;$\", lineage)\n    if unclassified_match:\n        # For unclassified lineages, we've already extracted all the known levels\n        # above. The unclassified itself should NOT be added as a separate taxon.\n        # The counts will contribute to the known levels only.\n        pass\n\n    return taxa\n\n\ndef process_rdp_directory(\n    directory: str,\n    domain: str = \"Bacteria\",\n    output_file: Optional[str] = None,\n    taxonomic_level: Optional[str] = None,\n) -&gt; pd.DataFrame:\n    ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def parse_lineage(lineage: str) -&gt; list:",
            "lineNumber": 100,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 101
          },
          {
            "text": "    Parse lineage string to extract taxonomic names at each level.",
            "lineNumber": 102
          },
          {
            "lineNumber": 103
          },
          {
            "text": "    Parameters",
            "lineNumber": 104
          },
          {
            "text": "    ----------",
            "lineNumber": 105
          },
          {
            "text": "    lineage : str",
            "lineNumber": 106
          },
          {
            "text": "        Lineage string in format:",
            "lineNumber": 107
          },
          {
            "text": "        \"Root;rootrank;{domain};domain;{phylum};phylum;{class};class;...\"",
            "lineNumber": 108
          },
          {
            "lineNumber": 109
          },
          {
            "text": "    Returns",
            "lineNumber": 110
          },
          {
            "text": "    -------",
            "lineNumber": 111
          },
          {
            "text": "    list",
            "lineNumber": 112
          },
          {
            "text": "        List of taxonomic names with prefixes (e.g.,",
            "lineNumber": 113
          },
          {
            "text": "        [\"phylum__Omnitrophota\", \"class__Koll11\", ...])",
            "lineNumber": 114
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 115
          },
          {
            "text": "    taxa = []",
            "lineNumber": 116
          },
          {
            "lineNumber": 117
          },
          {
            "text": "    # Pattern to match taxonomic levels: {name};{level};",
            "lineNumber": 118
          },
          {
            "text": "    # We want to extract pairs where level is phylum, class, order, family, or genus",
            "lineNumber": 119
          },
          {
            "text": "    pattern = r\"([^;]+);(phylum|class|order|family|genus);\"",
            "lineNumber": 120
          },
          {
            "text": "    matches = re.findall(pattern, lineage)",
            "lineNumber": 121
          },
          {
            "lineNumber": 122
          },
          {
            "text": "    for name, level in matches:",
            "lineNumber": 123
          },
          {
            "text": "        # Skip empty names",
            "lineNumber": 124
          },
          {
            "text": "        if not name or name.strip() == \"\":",
            "lineNumber": 125
          },
          {
            "text": "            continue",
            "lineNumber": 126
          },
          {
            "text": "        taxon_name = f\"{level}__{name}\"",
            "lineNumber": 127
          },
          {
            "text": "        taxa.append(taxon_name)",
            "lineNumber": 128
          },
          {
            "lineNumber": 129
          },
          {
            "text": "    # Handle unclassified case: ...;unclassified_XXX;;",
            "lineNumber": 130
          },
          {
            "text": "    # For unclassified lineages, we only include the known taxonomic levels",
            "lineNumber": 131
          },
          {
            "text": "    # (those that appear before the unclassified marker), not the unclassified itself",
            "lineNumber": 132
          },
          {
            "text": "    # The unclassified marker indicates unknown classification at that level,",
            "lineNumber": 133
          },
          {
            "text": "    # so we don't create a separate taxon entry for it",
            "lineNumber": 134
          },
          {
            "text": "    unclassified_match = re.search(r\";unclassified_([^;]+);;$\", lineage)",
            "lineNumber": 135
          },
          {
            "text": "    if unclassified_match:",
            "lineNumber": 136
          },
          {
            "text": "        # For unclassified lineages, we've already extracted all the known levels",
            "lineNumber": 137
          },
          {
            "text": "        # above. The unclassified itself should NOT be added as a separate taxon.",
            "lineNumber": 138
          },
          {
            "text": "        # The counts will contribute to the known levels only.",
            "lineNumber": 139
          },
          {
            "text": "        pass",
            "lineNumber": 140
          },
          {
            "lineNumber": 141
          },
          {
            "text": "    return taxa",
            "lineNumber": 142,
            "isSignature": true
          },
          {
            "lineNumber": 143
          },
          {
            "lineNumber": 144
          },
          {
            "text": "def process_rdp_directory(",
            "lineNumber": 145,
            "isSignature": true
          },
          {
            "text": "    directory: str,",
            "lineNumber": 146,
            "isSignature": true
          },
          {
            "text": "    domain: str = \"Bacteria\",",
            "lineNumber": 147,
            "isSignature": true
          },
          {
            "text": "    output_file: Optional[str] = None,",
            "lineNumber": 148,
            "isSignature": true
          },
          {
            "text": "    taxonomic_level: Optional[str] = None,",
            "lineNumber": 149,
            "isSignature": true
          },
          {
            "text": ") -&gt; pd.DataFrame:",
            "lineNumber": 150,
            "isSignature": true
          },
          {
            "text": "    ",
            "lineNumber": 151
          }
        ]
      },
      "score": 0.20622003078460693
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/inference.py",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 104
          }
        },
        "contents": "# @title Single example\n# From https://colab.research.google.com/drive/1wyVEQd4R3HYLTUOXEEQmp_I8aNC_aLhL\nimport json\nimport os\nimport subprocess\nimport transformers\nfrom transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n\n# HyenaDNA\nfrom standalone_hyenadna import *\nfrom huggingface_wrapper import *\n\n\ndef inference_single():\n    \"\"\"\n    this selects which backbone to use, and grabs weights/ config from HF\n    4 options:\n      'hyenadna-tiny-1k-seqlen'   # fine-tune on colab ok\n      'hyenadna-small-32k-seqlen'\n      'hyenadna-medium-160k-seqlen'  # inference only on colab\n      'hyenadna-medium-450k-seqlen'  # inference only on colab\n      'hyenadna-large-1m-seqlen'  # inference only on colab\n    \"\"\"\n\n    # you only need to select which model to use here, we'll do the rest!\n    pretrained_model_name = \"hyenadna-small-32k-seqlen\"\n\n    max_lengths = {\n        \"hyenadna-tiny-1k-seqlen\": 1024,\n        \"hyenadna-small-32k-seqlen\": 32768,\n        \"hyenadna-medium-160k-seqlen\": 160000,\n        \"hyenadna-medium-450k-seqlen\": 450000,  # T4 up to here\n        \"hyenadna-large-1m-seqlen\": 1_000_000,  # only A100 (paid tier)\n    }\n\n    max_length = max_lengths[pretrained_model_name]  # auto selects\n\n    # data settings:\n    use_padding = True\n    rc_aug = False  # reverse complement augmentation\n    add_eos = False  # add end of sentence token\n\n    # we need these for the decoder head, if using\n    use_head = False\n    n_classes = 2  # not used for embeddings only\n\n    # you can override with your own backbone config here if you want,\n    # otherwise we'll load the HF one in None\n    backbone_cfg = None\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(\"Using device:\", device)\n\n    # instantiate the model (pretrained here)\n    if",
        "signatures": {},
        "detailedLines": [
          {
            "text": "# @title Single example",
            "lineNumber": 1
          },
          {
            "text": "# From https://colab.research.google.com/drive/1wyVEQd4R3HYLTUOXEEQmp_I8aNC_aLhL",
            "lineNumber": 2
          },
          {
            "text": "import json",
            "lineNumber": 3
          },
          {
            "text": "import os",
            "lineNumber": 4
          },
          {
            "text": "import subprocess",
            "lineNumber": 5
          },
          {
            "text": "import transformers",
            "lineNumber": 6
          },
          {
            "text": "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig",
            "lineNumber": 7
          },
          {
            "lineNumber": 8
          },
          {
            "text": "# HyenaDNA",
            "lineNumber": 9
          },
          {
            "text": "from standalone_hyenadna import *",
            "lineNumber": 10
          },
          {
            "text": "from huggingface_wrapper import *",
            "lineNumber": 11
          },
          {
            "lineNumber": 12
          },
          {
            "lineNumber": 13
          },
          {
            "text": "def inference_single():",
            "lineNumber": 14,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 15
          },
          {
            "text": "    this selects which backbone to use, and grabs weights/ config from HF",
            "lineNumber": 16
          },
          {
            "text": "    4 options:",
            "lineNumber": 17
          },
          {
            "text": "      'hyenadna-tiny-1k-seqlen'   # fine-tune on colab ok",
            "lineNumber": 18
          },
          {
            "text": "      'hyenadna-small-32k-seqlen'",
            "lineNumber": 19
          },
          {
            "text": "      'hyenadna-medium-160k-seqlen'  # inference only on colab",
            "lineNumber": 20
          },
          {
            "text": "      'hyenadna-medium-450k-seqlen'  # inference only on colab",
            "lineNumber": 21
          },
          {
            "text": "      'hyenadna-large-1m-seqlen'  # inference only on colab",
            "lineNumber": 22
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 23
          },
          {
            "lineNumber": 24
          },
          {
            "text": "    # you only need to select which model to use here, we'll do the rest!",
            "lineNumber": 25
          },
          {
            "text": "    pretrained_model_name = \"hyenadna-small-32k-seqlen\"",
            "lineNumber": 26
          },
          {
            "lineNumber": 27
          },
          {
            "text": "    max_lengths = {",
            "lineNumber": 28
          },
          {
            "text": "        \"hyenadna-tiny-1k-seqlen\": 1024,",
            "lineNumber": 29
          },
          {
            "text": "        \"hyenadna-small-32k-seqlen\": 32768,",
            "lineNumber": 30
          },
          {
            "text": "        \"hyenadna-medium-160k-seqlen\": 160000,",
            "lineNumber": 31
          },
          {
            "text": "        \"hyenadna-medium-450k-seqlen\": 450000,  # T4 up to here",
            "lineNumber": 32
          },
          {
            "text": "        \"hyenadna-large-1m-seqlen\": 1_000_000,  # only A100 (paid tier)",
            "lineNumber": 33
          },
          {
            "text": "    }",
            "lineNumber": 34
          },
          {
            "lineNumber": 35
          },
          {
            "text": "    max_length = max_lengths[pretrained_model_name]  # auto selects",
            "lineNumber": 36
          },
          {
            "lineNumber": 37
          },
          {
            "text": "    # data settings:",
            "lineNumber": 38
          },
          {
            "text": "    use_padding = True",
            "lineNumber": 39
          },
          {
            "text": "    rc_aug = False  # reverse complement augmentation",
            "lineNumber": 40
          },
          {
            "text": "    add_eos = False  # add end of sentence token",
            "lineNumber": 41
          },
          {
            "lineNumber": 42
          },
          {
            "text": "    # we need these for the decoder head, if using",
            "lineNumber": 43
          },
          {
            "text": "    use_head = False",
            "lineNumber": 44
          },
          {
            "text": "    n_classes = 2  # not used for embeddings only",
            "lineNumber": 45
          },
          {
            "lineNumber": 46
          },
          {
            "text": "    # you can override with your own backbone config here if you want,",
            "lineNumber": 47
          },
          {
            "text": "    # otherwise we'll load the HF one in None",
            "lineNumber": 48
          },
          {
            "text": "    backbone_cfg = None",
            "lineNumber": 49
          },
          {
            "lineNumber": 50
          },
          {
            "text": "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
            "lineNumber": 51
          },
          {
            "text": "    print(\"Using device:\", device)",
            "lineNumber": 52
          },
          {
            "lineNumber": 53
          },
          {
            "text": "    # instantiate the model (pretrained here)",
            "lineNumber": 54
          },
          {
            "text": "    if",
            "lineNumber": 55
          }
        ]
      },
      "score": 0.1961764693260193
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/inference.py",
        "range": {
          "startPosition": {
            "line": 13
          },
          "endPosition": {
            "line": 104,
            "column": 47
          }
        },
        "contents": "def inference_single():\n\n    print(\"Using device:\", device)\n\n    # instantiate the model (pretrained here)\n    if pretrained_model_name in [\n        \"hyenadna-tiny-1k-seqlen\",\n        \"hyenadna-small-32k-seqlen\",\n        \"hyenadna-medium-160k-seqlen\",\n        \"hyenadna-medium-450k-seqlen\",\n        \"hyenadna-large-1m-seqlen\",\n    ]:\n        # use the pretrained Huggingface wrapper instead\n        model = HyenaDNAPreTrainedModel.from_pretrained(\n            \"./checkpoints\",\n            pretrained_model_name,\n            # Change to True to download the checkpoint\n            download=False,\n            config=backbone_cfg,\n            device=device,\n            use_head=use_head,\n            n_classes=n_classes,\n        )\n\n    # from scratch\n    elif pretrained_model_name is None:\n        model = HyenaDNAModel(**backbone_cfg, use_head=use_head, n_classes=n_classes)\n\n    # create tokenizer\n    tokenizer = CharacterTokenizer(\n        characters=[\"A\", \"C\", \"G\", \"T\", \"N\"],  # add DNA characters, N is uncertain\n        model_max_length=max_length + 2,  # to account for special tokens, like EOS\n        # add_special_tokens argument is incompatible with newer tokenizers:\n        # AttributeError: add_special_tokens conflicts with the method add_special_tokens in CharacterTokenizer\n        # add_special_tokens=False,  # we handle special tokens elsewhere\n        padding_side=\"left\",  # since HyenaDNA is causal, we pad on the left\n    )\n\n    #### Single embedding example ####\n\n    # create a sample 450k long, prepare\n    sequence = \"ACTG\" * int(max_length / 4)\n    tok_seq = tokenizer(sequence)\n    tok_seq = tok_seq[\"input_ids\"]  # grab ids\n\n    # place on device, convert to tensor\n    tok_seq = torch.LongTensor(tok_seq).unsqueeze(0)  # unsqueeze for batch dim\n    tok_seq = tok_seq.to(device)\n\n    # prep model and forward\n    model.to(device)\n    model.eval()\n    with torch.inference_mode():\n        embeddings = model(tok_seq)\n\n    print(embeddings.shape)  # embeddings here!",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 14,
                "column": 1
              },
              "endPosition": {
                "line": 15,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def inference_single():",
            "lineNumber": 14,
            "isSignature": true
          },
          {
            "lineNumber": 51
          },
          {
            "text": "    print(\"Using device:\", device)",
            "lineNumber": 52
          },
          {
            "lineNumber": 53
          },
          {
            "text": "    # instantiate the model (pretrained here)",
            "lineNumber": 54
          },
          {
            "text": "    if pretrained_model_name in [",
            "lineNumber": 55
          },
          {
            "text": "        \"hyenadna-tiny-1k-seqlen\",",
            "lineNumber": 56
          },
          {
            "text": "        \"hyenadna-small-32k-seqlen\",",
            "lineNumber": 57
          },
          {
            "text": "        \"hyenadna-medium-160k-seqlen\",",
            "lineNumber": 58
          },
          {
            "text": "        \"hyenadna-medium-450k-seqlen\",",
            "lineNumber": 59
          },
          {
            "text": "        \"hyenadna-large-1m-seqlen\",",
            "lineNumber": 60
          },
          {
            "text": "    ]:",
            "lineNumber": 61
          },
          {
            "text": "        # use the pretrained Huggingface wrapper instead",
            "lineNumber": 62
          },
          {
            "text": "        model = HyenaDNAPreTrainedModel.from_pretrained(",
            "lineNumber": 63
          },
          {
            "text": "            \"./checkpoints\",",
            "lineNumber": 64
          },
          {
            "text": "            pretrained_model_name,",
            "lineNumber": 65
          },
          {
            "text": "            # Change to True to download the checkpoint",
            "lineNumber": 66
          },
          {
            "text": "            download=False,",
            "lineNumber": 67
          },
          {
            "text": "            config=backbone_cfg,",
            "lineNumber": 68
          },
          {
            "text": "            device=device,",
            "lineNumber": 69
          },
          {
            "text": "            use_head=use_head,",
            "lineNumber": 70
          },
          {
            "text": "            n_classes=n_classes,",
            "lineNumber": 71
          },
          {
            "text": "        )",
            "lineNumber": 72
          },
          {
            "lineNumber": 73
          },
          {
            "text": "    # from scratch",
            "lineNumber": 74
          },
          {
            "text": "    elif pretrained_model_name is None:",
            "lineNumber": 75
          },
          {
            "text": "        model = HyenaDNAModel(**backbone_cfg, use_head=use_head, n_classes=n_classes)",
            "lineNumber": 76
          },
          {
            "lineNumber": 77
          },
          {
            "text": "    # create tokenizer",
            "lineNumber": 78
          },
          {
            "text": "    tokenizer = CharacterTokenizer(",
            "lineNumber": 79
          },
          {
            "text": "        characters=[\"A\", \"C\", \"G\", \"T\", \"N\"],  # add DNA characters, N is uncertain",
            "lineNumber": 80
          },
          {
            "text": "        model_max_length=max_length + 2,  # to account for special tokens, like EOS",
            "lineNumber": 81
          },
          {
            "text": "        # add_special_tokens argument is incompatible with newer tokenizers:",
            "lineNumber": 82
          },
          {
            "text": "        # AttributeError: add_special_tokens conflicts with the method add_special_tokens in CharacterTokenizer",
            "lineNumber": 83
          },
          {
            "text": "        # add_special_tokens=False,  # we handle special tokens elsewhere",
            "lineNumber": 84
          },
          {
            "text": "        padding_side=\"left\",  # since HyenaDNA is causal, we pad on the left",
            "lineNumber": 85
          },
          {
            "text": "    )",
            "lineNumber": 86
          },
          {
            "lineNumber": 87
          },
          {
            "text": "    #### Single embedding example ####",
            "lineNumber": 88
          },
          {
            "lineNumber": 89
          },
          {
            "text": "    # create a sample 450k long, prepare",
            "lineNumber": 90
          },
          {
            "text": "    sequence = \"ACTG\" * int(max_length / 4)",
            "lineNumber": 91
          },
          {
            "text": "    tok_seq = tokenizer(sequence)",
            "lineNumber": 92
          },
          {
            "text": "    tok_seq = tok_seq[\"input_ids\"]  # grab ids",
            "lineNumber": 93
          },
          {
            "lineNumber": 94
          },
          {
            "text": "    # place on device, convert to tensor",
            "lineNumber": 95
          },
          {
            "text": "    tok_seq = torch.LongTensor(tok_seq).unsqueeze(0)  # unsqueeze for batch dim",
            "lineNumber": 96
          },
          {
            "text": "    tok_seq = tok_seq.to(device)",
            "lineNumber": 97
          },
          {
            "lineNumber": 98
          },
          {
            "text": "    # prep model and forward",
            "lineNumber": 99
          },
          {
            "text": "    model.to(device)",
            "lineNumber": 100
          },
          {
            "text": "    model.eval()",
            "lineNumber": 101
          },
          {
            "text": "    with torch.inference_mode():",
            "lineNumber": 102
          },
          {
            "text": "        embeddings = model(tok_seq)",
            "lineNumber": 103
          },
          {
            "lineNumber": 104
          },
          {
            "text": "    print(embeddings.shape)  # embeddings here!",
            "lineNumber": 105,
            "isSignature": true
          }
        ]
      },
      "score": 0.19568219780921936
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./.gitignore",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 4
          }
        },
        "contents": "RDP_classifier/\ncheckpoints/\nfasta/\n.env",
        "signatures": {},
        "detailedLines": [
          {
            "text": "RDP_classifier/",
            "lineNumber": 1
          },
          {
            "text": "checkpoints/",
            "lineNumber": 2
          },
          {
            "text": "fasta/",
            "lineNumber": 3
          },
          {
            "text": ".env",
            "lineNumber": 4
          }
        ]
      },
      "score": 0.18031179904937744
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./create_hf_dataset.py",
        "range": {
          "startPosition": {
            "line": 149,
            "column": 22
          },
          "endPosition": {
            "line": 213
          }
        },
        "contents": "def get_existing_records(dataset_repo: str) -&gt; set:\n    \"\"\"\n    Get set of (study_name, sample_id) tuples that already exist in the dataset.\n    \"\"\"\n    try:\n        dataset = load_dataset(dataset_repo, split=\"train\", token=HF_TOKEN)\n        existing = set()\n        for row in dataset:\n            existing.add((row[\"study_name\"], row[\"sample_id\"]))\n        print(f\"✓ Found {len(existing)} existing records in dataset\")\n        return existing\n    except Exception as e:\n        # Dataset doesn't exist yet or is empty\n        print(f\"  Dataset doesn't exist yet or is empty: {e}\")\n        return set()\n\n\ndef create_dataset_records(\n    fasta_files: List[Tuple[Path, str, str]],\n    sample_data: Dict[Tuple[str, str], Dict],\n    existing_records: set,\n) -&gt; List[Dict]:\n    ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def get_existing_records(dataset_repo: str) -&gt; set:",
            "lineNumber": 153,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 154
          },
          {
            "text": "    Get set of (study_name, sample_id) tuples that already exist in the dataset.",
            "lineNumber": 155
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 156
          },
          {
            "text": "    try:",
            "lineNumber": 157
          },
          {
            "text": "        dataset = load_dataset(dataset_repo, split=\"train\", token=HF_TOKEN)",
            "lineNumber": 158
          },
          {
            "text": "        existing = set()",
            "lineNumber": 159
          },
          {
            "text": "        for row in dataset:",
            "lineNumber": 160
          },
          {
            "text": "            existing.add((row[\"study_name\"], row[\"sample_id\"]))",
            "lineNumber": 161
          },
          {
            "text": "        print(f\"✓ Found {len(existing)} existing records in dataset\")",
            "lineNumber": 162
          },
          {
            "text": "        return existing",
            "lineNumber": 163
          },
          {
            "text": "    except Exception as e:",
            "lineNumber": 164
          },
          {
            "text": "        # Dataset doesn't exist yet or is empty",
            "lineNumber": 165
          },
          {
            "text": "        print(f\"  Dataset doesn't exist yet or is empty: {e}\")",
            "lineNumber": 166
          },
          {
            "text": "        return set()",
            "lineNumber": 167,
            "isSignature": true
          },
          {
            "lineNumber": 168
          },
          {
            "lineNumber": 169
          },
          {
            "text": "def create_dataset_records(",
            "lineNumber": 170,
            "isSignature": true
          },
          {
            "text": "    fasta_files: List[Tuple[Path, str, str]],",
            "lineNumber": 171,
            "isSignature": true
          },
          {
            "text": "    sample_data: Dict[Tuple[str, str], Dict],",
            "lineNumber": 172,
            "isSignature": true
          },
          {
            "text": "    existing_records: set,",
            "lineNumber": 173,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[Dict]:",
            "lineNumber": 174,
            "isSignature": true
          },
          {
            "text": "    ",
            "lineNumber": 175
          }
        ]
      },
      "score": 0.17138780653476715
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./predict_ph.py",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 120
          }
        },
        "contents": "\"\"\"\nPredict pH from bacterial abundances using various regression models.\n\nThis script loads and preprocesses bacterial abundance data, performs\ntrain-test splitting, and fits regression models to predict pH values.\n\"\"\"\n\nimport argparse\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport time\nfrom sklearn.model_selection import train_test_split, ParameterGrid, cross_val_score\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.pipeline import Pipeline\nimport warnings\n\n# Try to import tqdm for progress bar, fallback to None if not available\ntry:\n    from tqdm import tqdm\n\n    HAS_TQDM = True\nexcept ImportError:\n    HAS_TQDM = False\n    print(\n        \"Note: tqdm not available. Install with 'pip install tqdm' for progress bars.\"\n    )\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_data(sample_data_path, phylum_counts_path, random_seed=42):\n    ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "\"\"\"",
            "lineNumber": 1
          },
          {
            "text": "Predict pH from bacterial abundances using various regression models.",
            "lineNumber": 2
          },
          {
            "lineNumber": 3
          },
          {
            "text": "This script loads and preprocesses bacterial abundance data, performs",
            "lineNumber": 4
          },
          {
            "text": "train-test splitting, and fits regression models to predict pH values.",
            "lineNumber": 5
          },
          {
            "text": "\"\"\"",
            "lineNumber": 6
          },
          {
            "lineNumber": 7
          },
          {
            "text": "import argparse",
            "lineNumber": 8
          },
          {
            "text": "import pandas as pd",
            "lineNumber": 9
          },
          {
            "text": "import numpy as np",
            "lineNumber": 10
          },
          {
            "text": "from pathlib import Path",
            "lineNumber": 11
          },
          {
            "text": "import time",
            "lineNumber": 12
          },
          {
            "text": "from sklearn.model_selection import train_test_split, ParameterGrid, cross_val_score",
            "lineNumber": 13
          },
          {
            "text": "from sklearn.preprocessing import Normalizer",
            "lineNumber": 14
          },
          {
            "text": "from sklearn.linear_model import LinearRegression",
            "lineNumber": 15
          },
          {
            "text": "from sklearn.neighbors import KNeighborsRegressor",
            "lineNumber": 16
          },
          {
            "text": "from sklearn.ensemble import RandomForestRegressor",
            "lineNumber": 17
          },
          {
            "text": "from sklearn.ensemble import HistGradientBoostingRegressor",
            "lineNumber": 18
          },
          {
            "text": "from sklearn.pipeline import Pipeline",
            "lineNumber": 19
          },
          {
            "text": "import warnings",
            "lineNumber": 20
          },
          {
            "lineNumber": 21
          },
          {
            "text": "# Try to import tqdm for progress bar, fallback to None if not available",
            "lineNumber": 22
          },
          {
            "text": "try:",
            "lineNumber": 23
          },
          {
            "text": "    from tqdm import tqdm",
            "lineNumber": 24
          },
          {
            "lineNumber": 25
          },
          {
            "text": "    HAS_TQDM = True",
            "lineNumber": 26
          },
          {
            "text": "except ImportError:",
            "lineNumber": 27
          },
          {
            "text": "    HAS_TQDM = False",
            "lineNumber": 28
          },
          {
            "text": "    print(",
            "lineNumber": 29
          },
          {
            "text": "        \"Note: tqdm not available. Install with 'pip install tqdm' for progress bars.\"",
            "lineNumber": 30
          },
          {
            "text": "    )",
            "lineNumber": 31
          },
          {
            "lineNumber": 32
          },
          {
            "text": "warnings.filterwarnings(\"ignore\")",
            "lineNumber": 33
          },
          {
            "lineNumber": 34
          },
          {
            "lineNumber": 35
          },
          {
            "text": "def load_and_preprocess_data(sample_data_path, phylum_counts_path, random_seed=42):",
            "lineNumber": 36,
            "isSignature": true
          },
          {
            "text": "    ",
            "lineNumber": 37
          }
        ]
      },
      "score": 0.1608199030160904
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---


Checking for existing requirements or dependencies:


---

<tool-use data-tool-type="generic" data-tool-name="glob_file_search">
<details>
<summary>Tool use: **glob_file_search** • Searched codebase "undefined" • **1** result in **1** directory</summary>

Directory: **undefined** (1 file)

| File |
|------|
| `./hyena-dna/requirements.txt` |

</details>
</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="glob_file_search">
<details>
<summary>Tool use: **glob_file_search** • Searched codebase "undefined" • **0** result in **1** directory</summary>

Directory: **undefined** (0 file)

</details>
</tool-use>

---

<tool-use data-tool-type="unknown" data-tool-name="list_dir_v2">
<details>
<summary>Tool use: **list_dir_v2**</summary>


Parameters:

```json
{
  "targetDirectory": "/home/jedick/github/microbial-pH-predictions"
}
```
Result:

```json
{
  "directoryTreeRoot": {
    "absPath": "/home/jedick/github/microbial-pH-predictions",
    "childrenDirs": [
      {
        "absPath": "/home/jedick/github/microbial-pH-predictions/.cursor",
        "childrenDirs": [
          {
            "absPath": "/home/jedick/github/microbial-pH-predictions/.cursor/rules",
            "childrenFiles": [
              {
                "name": "commit-rules.mdc"
              },
              {
                "name": "python-code-style.mdc"
              },
              {
                "name": "readme-updates.mdc"
              }
            ],
            "childrenWereProcessed": true
          }
        ],
        "childrenWereProcessed": true
      },
      {
        "absPath": "/home/jedick/github/microbial-pH-predictions/.specstory",
        "childrenDirs": [
          {
            "absPath": "/home/jedick/github/microbial-pH-predictions/.specstory/history",
            "childrenFiles": [
              {
                "name": "2026-01-19_04-20Z-new-project-setup-and-cursor-rules.md"
              },
              {
                "name": "2026-01-19_05-39Z-taxonomic-data-processing-and-aggregation.md"
              },
              {
                "name": "2026-01-19_13-20Z-bacterial-abundance-ph-prediction-workflow.md"
              },
              {
                "name": "2026-01-20_08-02Z-charactertokenizer-notimplementederror.md"
              },
              {
                "name": "2026-01-21_11-12Z-hugging-face-dna-dataset-creation.md"
              },
              {
                "name": "2026-01-22_08-33Z-variable-name-consistency-update.md"
              },
              {
                "name": "2026-01-22_09-17Z-ncbi-sra-data-download-script.md"
              }
            ],
            "childrenWereProcessed": true
          }
        ],
        "childrenFiles": [
          {
            "name": ".gitignore"
          }
        ],
        "childrenWereProcessed": true
      },
      {
        "absPath": "/home/jedick/github/microbial-pH-predictions/data",
        "childrenFiles": [
          {
            "name": "bacteria_phylum_counts.csv.xz"
          },
          {
            "name": "sample_data.csv"
          }
        ],
        "childrenWereProcessed": true
      },
      {
        "absPath": "/home/jedick/github/microbial-pH-predictions/hyena-dna",
        "childrenFiles": [
          {
            "name": "huggingface_wrapper.py"
          },
          {
            "name": "inference.py"
          },
          {
            "name": "requirements.txt"
          },
          {
            "name": "standalone_hyenadna.py"
          }
        ],
        "childrenWereProcessed": true
      },
      {
        "absPath": "/home/jedick/github/microbial-pH-predictions/results",
        "childrenFiles": [
          {
            "name": "hgb_summary.txt"
          },
          {
            "name": "hgb_test_predictions.csv"
          },
          {
            "name": "knn_summary.txt"
          },
          {
            "name": "knn_test_predictions.csv"
          },
          {
            "name": "linear_summary.txt"
          },
          {
            "name": "linear_test_predictions.csv"
          },
          {
            "name": "rf_summary.txt"
          },
          {
            "name": "rf_test_predictions.csv"
          }
        ],
        "childrenWereProcessed": true
      }
    ],
    "childrenFiles": [
      {
        "name": ".cursorignore"
      },
      {
        "name": ".cursorindexingignore"
      },
      {
        "name": ".gitignore"
      },
      {
        "name": "create_hf_dataset.py"
      },
      {
        "name": "LICENSE"
      },
      {
        "name": "predict_ph.py"
      },
      {
        "name": "process_rdp_data.py"
      },
      {
        "name": "README.md"
      }
    ],
    "childrenWereProcessed": true
  }
}
```
Status: **completed**


</details>
</tool-use>

---


Checking the specstory for any previous context:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/.specstory/history/2026-01-22_09-17Z-ncbi-sra-data-download-script.md</summary>

</details>
</tool-use>

---


Writing the script to download SRA data:


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /home/jedick/github/microbial-pH-predictions/download_sra_data.py</summary>

</details>

</tool-use>

---


Making the script executable and checking for syntax errors:


---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_command_v2">
<details><summary>Tool use: **run_terminal_command_v2** • Run command: chmod +x /home/jedick/github/microbial-pH-predictions/download_sra_data.py</summary>

```bash
chmod +x /home/jedick/github/microbial-pH-predictions/download_sra_data.py
```
</details>
</tool-use>

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_command_v2">
<details><summary>Tool use: **run_terminal_command_v2** • Run command: cd /home/jedick/github/microbial-pH-predictions && python3 -m py_compile download_sra_data.py</summary>

```bash
cd /home/jedick/github/microbial-pH-predictions && python3 -m py_compile download_sra_data.py
```
</details>
</tool-use>

---


Checking if `requests` is installed:


---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_command_v2">
<details><summary>Tool use: **run_terminal_command_v2** • Run command: cd /home/jedick/github/microbial-pH-predictions && python3 -c "import requests; print('requests is available')"</summary>

```bash
cd /home/jedick/github/microbial-pH-predictions && python3 -c "import requests; print('requests is available')"
```

```
requests is available

```
</details>
</tool-use>

---


Reviewing the script logic, especially the last-sequence removal. Verifying the FASTA parsing:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/download_sra_data.py</summary>

</details>
</tool-use>

---


Updating the script to handle edge cases and ensure the last sequence is always excluded:


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /home/jedick/github/microbial-pH-predictions/download_sra_data.py</summary>

</details>

</tool-use>

---


Verifying the script handles all requirements. Running a quick test to check for import issues:


---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_command_v2">
<details><summary>Tool use: **run_terminal_command_v2** • Run command: cd /home/jedick/github/microbial-pH-predictions && python3 -c "import download_sra_data; print('Script imports successfully')"</summary>

```bash
cd /home/jedick/github/microbial-pH-predictions && python3 -c "import download_sra_data; print('Script imports successfully')"
```

```
Script imports successfully

```
</details>
</tool-use>

---


The script is ready. Summary:

## Script Features

1. **Pattern matching**: Only processes sample_ids matching `SRR*` or `ERR*` pattern
2. **Partial download**: Downloads only the first 1MB using HTTP Range headers
3. **Incomplete gzip handling**: Uses `zcat` to extract incomplete gzip files
4. **Sequence processing**: Removes the last sequence (which may be truncated) from the FASTA file
5. **File organization**: Saves to `data/fasta/<study_name>/<sample_id>.fasta.gz`
6. **Duplicate handling**: Skips samples that have already been downloaded or appear multiple times in the CSV
7. **Progress tracking**: Shows current study_name and sample_id during processing
8. **Cleanup on interruption**: Removes temporary files if the script is interrupted (Ctrl+C)
9. **Error handling**: Handles download failures and processing errors gracefully

## Usage

```bash
python3 download_sra_data.py
```

The script will:
- Read `data/sample_data.csv`
- Process each row with SRR* or ERR* sample_ids
- Skip files that already exist
- Show progress and a summary at the end

The script uses temporary files in the system temp directory and cleans them up automatically, even if interrupted. All temporary files are tracked and removed on exit or interruption.

Ready to use. Should I run a test download, or do you want to review the code first?

---

_**User**_

I've tested it and it works. Can you add a section to the README for this script? be sure to follow the README project rules, especially the documentation format for usage and details.

---

_**Agent (model default, mode Agent)**_


Reviewing the README to understand the format and rules:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/README.md</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/.cursor/rules/readme-updates.mdc</summary>

</details>
</tool-use>

---


Adding a README section for the SRA download script, following the existing format:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/README.md</summary>

</details>
</tool-use>

---


Adding the SRA download section after "Data processing" and before "Hugging Face dataset creation":


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /home/jedick/github/microbial-pH-predictions/README.md</summary>

</details>

</tool-use>

---


Added a README section for the SRA download script. It follows the project's documentation format:

- Sentence case heading: "SRA data download"
- Brief description paragraph
- Usage code block
- Collapsible details section with bullet points

The section is placed after "Data processing" and before "Hugging Face dataset creation" since it downloads the raw data used by the dataset creation script. The documentation covers the key features and requirements.

---

_**User**_

Can you also check the Python code style?

---

_**Agent (model default, mode Agent)**_


Checking Python code style rules and reviewing the script:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/.cursor/rules/python-code-style.mdc</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/download_sra_data.py</summary>

</details>
</tool-use>

---


Checking if the script conforms to Black formatting:


---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_command_v2">
<details><summary>Tool use: **run_terminal_command_v2** • Run command: cd /home/jedick/github/microbial-pH-predictions && black --check download_sra_data.py</summary>

```bash
cd /home/jedick/github/microbial-pH-predictions && black --check download_sra_data.py
```

```
would reformat download_sra_data.py

Oh no! 💥 💔 💥
1 file would be reformatted.

```
</details>
</tool-use>

---


Formatting the script with Black:


---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_command_v2">
<details><summary>Tool use: **run_terminal_command_v2** • Run command: cd /home/jedick/github/microbial-pH-predictions && black download_sra_data.py</summary>

```bash
cd /home/jedick/github/microbial-pH-predictions && black download_sra_data.py
```

```
reformatted download_sra_data.py

All done! ✨ 🍰 ✨
1 file reformatted.

```
</details>
</tool-use>

---


Reviewing the formatted file and checking for other style issues:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/download_sra_data.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_command_v2">
<details><summary>Tool use: **run_terminal_command_v2** • Run command: cd /home/jedick/github/microbial-pH-predictions && python3 -m flake8 --select=E9,F63,F7,F82 --show-source --statistics download_sra_data.py 2>&1 | head -20</summary>

```bash
cd /home/jedick/github/microbial-pH-predictions && python3 -m flake8 --select=E9,F63,F7,F82 --show-source --statistics download_sra_data.py 2>&1 | head -20
```

```
/usr/bin/python3: No module named flake8

```
</details>
</tool-use>

---


Verifying the script still works after formatting:


---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_command_v2">
<details><summary>Tool use: **run_terminal_command_v2** • Run command: cd /home/jedick/github/microbial-pH-predictions && python3 -m py_compile download_sra_data.py && echo "✓ Syntax check passed"</summary>

```bash
cd /home/jedick/github/microbial-pH-predictions && python3 -m py_compile download_sra_data.py && echo "✓ Syntax check passed"
```

```
✓ Syntax check passed

```
</details>
</tool-use>

---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_command_v2">
<details><summary>Tool use: **run_terminal_command_v2** • Run command: cd /home/jedick/github/microbial-pH-predictions && black --check download_sra_data.py && echo "✓ Black formatting check passed"</summary>

```bash
cd /home/jedick/github/microbial-pH-predictions && black --check download_sra_data.py && echo "✓ Black formatting check passed"
```

```
✓ Black formatting check passed
All done! ✨ 🍰 ✨
1 file would be left unchanged.

```
</details>
</tool-use>

---


Formatted the script with Black to match the project's code style. Changes:

1. **Function signature formatting**: Split the `download_first_mb` function signature across multiple lines to stay within the 88-character limit
2. **Long print statements**: Split long print statements across multiple lines for readability
3. **General formatting**: Adjusted spacing and formatting to match Black's conventions

The script now:
- Passes Black formatting checks
- Passes Python syntax validation
- Follows the project's code style rules (Black with 88-character line length)

The code is ready and compliant with the project's Python code style guidelines.

---

_**User**_

Please change the download size from 1MB to 0.25MB.

---

