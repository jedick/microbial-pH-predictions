<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session a2b16552-ddd4-42a8-865a-cf715c79ae44 (2026-01-23 09:18Z) -->

# HyenaDNA training data expansion (2026-01-23 09:18Z)

_**User**_

The HF dataset created by @create_hf_dataset.py has up to 256K nucleotides per sample, but in training HyenaDNA with @train_hyenadna_ph.py  I am limited to 32K max_length with my hardware. Instead of taking only the first set of sequences that fit into max_length, can you generate five non-overlapping sets of sequences for each sample from the HF dataset? Each set should have as many sequences as possible without going over the max_length. The effect is to expand the training, validation, and test datasets by five-fold. For saving the predictions on the test set, add a column for the predicted pH for each set of sequences (1 to 5) to the output, along with the mean value for each sample. The output for the test set should have the same sample_ids and number of rows as now, just more columns for the different sets of sequences used for each sample.

---

_**Agent (model default, mode Agent)**_



---


Examining the current implementation to understand the structure.


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/create_hf_dataset.py</summary>

</details>
</tool-use>

---



---


Implementing 5 non-overlapping sequence sets per sample. Modifying the training script to handle this:


---

<tool-use data-tool-type="unknown" data-tool-name="semantic_search_full">
<details>
<summary>Tool use: **semantic_search_full**</summary>


Parameters:

```json
{
  "query": "How are sequences concatenated and split in the dataset?",
  "topK": 10,
  "explanation": ""
}
```
Result:

```json
{
  "codeResults": [
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 154
          },
          "endPosition": {
            "line": 275
          }
        },
        "contents": "class pHDataset(Dataset):\ndef __getitem__(self, idx):\n,\n            \"ph\": torch.tensor(ph, dtype=torch.float32),\n            \"study_name\": item.get(\"study_name\", \"\"),\n            \"sample_id\": item.get(\"sample_id\", \"\"),\n        }\n\n\ndef concatenate_sequences(\n    sequences: List[str],\n    tokenizer: CharacterTokenizer,\n    max_length: int,\n    sep_token_id: int = SEP_TOKEN_ID,\n) -&gt; List[int]:\n    \"\"\"\n    Concatenate multiple DNA sequences with [SEP] tokens.\n\n    Args:\n        sequences: List of DNA sequence strings\n        tokenizer: CharacterTokenizer instance\n        max_length: Maximum total length (including special tokens)\n        sep_token_id: [SEP] token ID\n\n    Returns:\n        List of token IDs of length &lt;= max_length\n    \"\"\"\n    if not sequences:\n        # Return single [SEP] token if no sequences\n        return [sep_token_id]\n\n    # Filter out empty sequences first\n    valid_sequences = [seq for seq in sequences if seq and len(seq) &gt; 0]\n\n    if not valid_sequences:\n        # No valid sequences, return single [SEP]\n        return [sep_token_id]\n\n    # Precompute how many sequences we need based on character lengths\n    # For character tokenizer, each DNA character maps to one token (after filtering special tokens)\n    # We need to account for: sequence lengths + (n-1) SEP tokens for n sequences\n    cumulative_length = 0\n    sequences_to_use = []\n\n    for seq in valid_sequences:\n        seq_len = len(seq)\n        # Estimate tokens needed: seq_len characters + 1 SEP token (if not first sequence)\n        tokens_needed = seq_len + (1 if sequences_to_use else 0)\n\n        if cumulative_length + tokens_needed &lt;= max_length:\n            sequences_to_use.append(seq)\n            cumulative_length += tokens_needed\n        else:\n            # This sequence would exceed max_length, stop here\n            break\n\n    if not sequences_to_use:\n        # Even the first sequence is too long, we'll truncate it",
        "signatures": {},
        "detailedLines": [
          {
            "text": "class pHDataset(Dataset):",
            "lineNumber": 155,
            "isSignature": true
          },
          {
            "text": "def __getitem__(self, idx):",
            "lineNumber": 182
          },
          {
            "text": ",",
            "lineNumber": 193
          },
          {
            "text": "            \"ph\": torch.tensor(ph, dtype=torch.float32),",
            "lineNumber": 194
          },
          {
            "text": "            \"study_name\": item.get(\"study_name\", \"\"),",
            "lineNumber": 195
          },
          {
            "text": "            \"sample_id\": item.get(\"sample_id\", \"\"),",
            "lineNumber": 196
          },
          {
            "text": "        }",
            "lineNumber": 197,
            "isSignature": true
          },
          {
            "lineNumber": 198
          },
          {
            "lineNumber": 199
          },
          {
            "text": "def concatenate_sequences(",
            "lineNumber": 200,
            "isSignature": true
          },
          {
            "text": "    sequences: List[str],",
            "lineNumber": 201,
            "isSignature": true
          },
          {
            "text": "    tokenizer: CharacterTokenizer,",
            "lineNumber": 202,
            "isSignature": true
          },
          {
            "text": "    max_length: int,",
            "lineNumber": 203,
            "isSignature": true
          },
          {
            "text": "    sep_token_id: int = SEP_TOKEN_ID,",
            "lineNumber": 204,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[int]:",
            "lineNumber": 205,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 206
          },
          {
            "text": "    Concatenate multiple DNA sequences with [SEP] tokens.",
            "lineNumber": 207
          },
          {
            "lineNumber": 208
          },
          {
            "text": "    Args:",
            "lineNumber": 209
          },
          {
            "text": "        sequences: List of DNA sequence strings",
            "lineNumber": 210
          },
          {
            "text": "        tokenizer: CharacterTokenizer instance",
            "lineNumber": 211
          },
          {
            "text": "        max_length: Maximum total length (including special tokens)",
            "lineNumber": 212
          },
          {
            "text": "        sep_token_id: [SEP] token ID",
            "lineNumber": 213
          },
          {
            "lineNumber": 214
          },
          {
            "text": "    Returns:",
            "lineNumber": 215
          },
          {
            "text": "        List of token IDs of length &lt;= max_length",
            "lineNumber": 216
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 217
          },
          {
            "text": "    if not sequences:",
            "lineNumber": 218
          },
          {
            "text": "        # Return single [SEP] token if no sequences",
            "lineNumber": 219
          },
          {
            "text": "        return [sep_token_id]",
            "lineNumber": 220
          },
          {
            "lineNumber": 221
          },
          {
            "text": "    # Filter out empty sequences first",
            "lineNumber": 222
          },
          {
            "text": "    valid_sequences = [seq for seq in sequences if seq and len(seq) &gt; 0]",
            "lineNumber": 223
          },
          {
            "lineNumber": 224
          },
          {
            "text": "    if not valid_sequences:",
            "lineNumber": 225
          },
          {
            "text": "        # No valid sequences, return single [SEP]",
            "lineNumber": 226
          },
          {
            "text": "        return [sep_token_id]",
            "lineNumber": 227
          },
          {
            "lineNumber": 228
          },
          {
            "text": "    # Precompute how many sequences we need based on character lengths",
            "lineNumber": 229
          },
          {
            "text": "    # For character tokenizer, each DNA character maps to one token (after filtering special tokens)",
            "lineNumber": 230
          },
          {
            "text": "    # We need to account for: sequence lengths + (n-1) SEP tokens for n sequences",
            "lineNumber": 231
          },
          {
            "text": "    cumulative_length = 0",
            "lineNumber": 232
          },
          {
            "text": "    sequences_to_use = []",
            "lineNumber": 233
          },
          {
            "lineNumber": 234
          },
          {
            "text": "    for seq in valid_sequences:",
            "lineNumber": 235
          },
          {
            "text": "        seq_len = len(seq)",
            "lineNumber": 236
          },
          {
            "text": "        # Estimate tokens needed: seq_len characters + 1 SEP token (if not first sequence)",
            "lineNumber": 237
          },
          {
            "text": "        tokens_needed = seq_len + (1 if sequences_to_use else 0)",
            "lineNumber": 238
          },
          {
            "lineNumber": 239
          },
          {
            "text": "        if cumulative_length + tokens_needed &lt;= max_length:",
            "lineNumber": 240
          },
          {
            "text": "            sequences_to_use.append(seq)",
            "lineNumber": 241
          },
          {
            "text": "            cumulative_length += tokens_needed",
            "lineNumber": 242
          },
          {
            "text": "        else:",
            "lineNumber": 243
          },
          {
            "text": "            # This sequence would exceed max_length, stop here",
            "lineNumber": 244
          },
          {
            "text": "            break",
            "lineNumber": 245
          },
          {
            "lineNumber": 246
          },
          {
            "text": "    if not sequences_to_use:",
            "lineNumber": 247
          },
          {
            "text": "        # Even the first sequence is too long, we'll truncate it",
            "lineNumber": 248
          }
        ]
      },
      "score": 0.5195240378379822
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 48
          },
          "endPosition": {
            "line": 196
          }
        },
        "contents": "class RegressionHead(nn.Module):\ndef forward(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:\n  # (batch, 1)\n\n        # Squeeze to (batch,)\n        return ph_pred.squeeze(-1)\n\n\nclass pHDataset(Dataset):\n    \"\"\"Dataset for pH prediction from DNA sequences.\"\"\"\n\n    def __init__(\n        self,\n        data: List[Dict],\n        tokenizer: CharacterTokenizer,\n        max_length: int,\n        sep_token_id: int = SEP_TOKEN_ID,\n    ):\n        \"\"\"\n        Initialize dataset.\n\n        Args:\n            data: List of dicts with 'sequences' (list of str) and 'pH' (float)\n            tokenizer: CharacterTokenizer instance\n            max_length: Maximum sequence length\n            sep_token_id: [SEP] token ID for concatenation\n        \"\"\"\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.sep_token_id = sep_token_id\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        sequences = item[\"sequences\"]\n        ph = item[\"pH\"]\n\n        # Concatenate sequences with [SEP] tokens\n        token_ids = concatenate_sequences(\n            sequences, self.tokenizer, self.max_length, self.sep_token_id\n        )\n\n        return {\n            \"input_ids\": torch.LongTensor(token_ids),\n            \"ph\": torch.tensor(ph, dtype=torch.float32),\n            \"study_name\": item.get(\"study_name\", \"\"),",
        "signatures": {},
        "detailedLines": [
          {
            "text": "class RegressionHead(nn.Module):",
            "lineNumber": 49,
            "isSignature": true
          },
          {
            "text": "def forward(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:",
            "lineNumber": 135
          },
          {
            "text": "  # (batch, 1)",
            "lineNumber": 149
          },
          {
            "lineNumber": 150
          },
          {
            "text": "        # Squeeze to (batch,)",
            "lineNumber": 151
          },
          {
            "text": "        return ph_pred.squeeze(-1)",
            "lineNumber": 152,
            "isSignature": true
          },
          {
            "lineNumber": 153
          },
          {
            "lineNumber": 154
          },
          {
            "text": "class pHDataset(Dataset):",
            "lineNumber": 155,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Dataset for pH prediction from DNA sequences.\"\"\"",
            "lineNumber": 156
          },
          {
            "lineNumber": 157
          },
          {
            "text": "    def __init__(",
            "lineNumber": 158
          },
          {
            "text": "        self,",
            "lineNumber": 159
          },
          {
            "text": "        data: List[Dict],",
            "lineNumber": 160
          },
          {
            "text": "        tokenizer: CharacterTokenizer,",
            "lineNumber": 161
          },
          {
            "text": "        max_length: int,",
            "lineNumber": 162
          },
          {
            "text": "        sep_token_id: int = SEP_TOKEN_ID,",
            "lineNumber": 163
          },
          {
            "text": "    ):",
            "lineNumber": 164
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 165
          },
          {
            "text": "        Initialize dataset.",
            "lineNumber": 166
          },
          {
            "lineNumber": 167
          },
          {
            "text": "        Args:",
            "lineNumber": 168
          },
          {
            "text": "            data: List of dicts with 'sequences' (list of str) and 'pH' (float)",
            "lineNumber": 169
          },
          {
            "text": "            tokenizer: CharacterTokenizer instance",
            "lineNumber": 170
          },
          {
            "text": "            max_length: Maximum sequence length",
            "lineNumber": 171
          },
          {
            "text": "            sep_token_id: [SEP] token ID for concatenation",
            "lineNumber": 172
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 173
          },
          {
            "text": "        self.data = data",
            "lineNumber": 174
          },
          {
            "text": "        self.tokenizer = tokenizer",
            "lineNumber": 175
          },
          {
            "text": "        self.max_length = max_length",
            "lineNumber": 176
          },
          {
            "text": "        self.sep_token_id = sep_token_id",
            "lineNumber": 177
          },
          {
            "lineNumber": 178
          },
          {
            "text": "    def __len__(self):",
            "lineNumber": 179
          },
          {
            "text": "        return len(self.data)",
            "lineNumber": 180
          },
          {
            "lineNumber": 181
          },
          {
            "text": "    def __getitem__(self, idx):",
            "lineNumber": 182
          },
          {
            "text": "        item = self.data[idx]",
            "lineNumber": 183
          },
          {
            "text": "        sequences = item[\"sequences\"]",
            "lineNumber": 184
          },
          {
            "text": "        ph = item[\"pH\"]",
            "lineNumber": 185
          },
          {
            "lineNumber": 186
          },
          {
            "text": "        # Concatenate sequences with [SEP] tokens",
            "lineNumber": 187
          },
          {
            "text": "        token_ids = concatenate_sequences(",
            "lineNumber": 188
          },
          {
            "text": "            sequences, self.tokenizer, self.max_length, self.sep_token_id",
            "lineNumber": 189
          },
          {
            "text": "        )",
            "lineNumber": 190
          },
          {
            "lineNumber": 191
          },
          {
            "text": "        return {",
            "lineNumber": 192
          },
          {
            "text": "            \"input_ids\": torch.LongTensor(token_ids),",
            "lineNumber": 193
          },
          {
            "text": "            \"ph\": torch.tensor(ph, dtype=torch.float32),",
            "lineNumber": 194
          },
          {
            "text": "            \"study_name\": item.get(\"study_name\", \"\"),",
            "lineNumber": 195
          }
        ]
      },
      "score": 0.47833216190338135
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./create_hf_dataset.py",
        "range": {
          "startPosition": {
            "line": 33,
            "column": 15
          },
          "endPosition": {
            "line": 115
          }
        },
        "contents": "def read_sequences_from_fasta(\n    fasta_path: Path, max_length: int = MAX_DNA_LENGTH\n) -&gt; List[str]:\n    \"\"\"\n    Read sequences from a gzipped FASTA file until reaching max_length total DNA.\n    Returns a list of sequence strings.\n    \"\"\"\n    sequences = []\n    current_sequence = []\n    current_header = None\n    total_length = 0\n\n    with gzip.open(fasta_path, \"rt\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n\n            if line.startswith(\"&gt;\"):\n                # Save previous sequence if exists\n                if current_sequence:\n                    seq = \"\".join(current_sequence)\n                    seq_len = len(seq)\n                    if total_length + seq_len &lt;= max_length:\n                        sequences.append(seq)\n                        total_length += seq_len\n                    else:\n                        # Take partial sequence if we haven't reached max yet\n                        remaining = max_length - total_length\n                        if remaining &gt; 0:\n                            sequences.append(seq[:remaining])\n                        break\n\n                # Start new sequence\n                current_header = line\n                current_sequence = []\n            else:\n                # Add to current sequence\n                current_sequence.append(line)\n\n        # Handle last sequence\n        if current_sequence and total_length &lt; max_length:\n            seq = \"\".join(current_sequence)\n            seq_len = len(seq)\n            if total_length + seq_len &lt;= max_length:\n                sequences.append(seq)\n                total_length += seq_len\n            else:\n                remaining = max_length - total_length\n                if remaining &gt; 0:\n                    sequences.append(seq[:remaining])\n\n    return sequences\n\n\ndef load_sample_data(csv_path: Path) -&gt; Dict[Tuple[str, str], Dict]:\n    \"\"\"",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def read_sequences_from_fasta(",
            "lineNumber": 37,
            "isSignature": true
          },
          {
            "text": "    fasta_path: Path, max_length: int = MAX_DNA_LENGTH",
            "lineNumber": 38,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[str]:",
            "lineNumber": 39,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 40
          },
          {
            "text": "    Read sequences from a gzipped FASTA file until reaching max_length total DNA.",
            "lineNumber": 41
          },
          {
            "text": "    Returns a list of sequence strings.",
            "lineNumber": 42
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 43
          },
          {
            "text": "    sequences = []",
            "lineNumber": 44
          },
          {
            "text": "    current_sequence = []",
            "lineNumber": 45
          },
          {
            "text": "    current_header = None",
            "lineNumber": 46
          },
          {
            "text": "    total_length = 0",
            "lineNumber": 47
          },
          {
            "lineNumber": 48
          },
          {
            "text": "    with gzip.open(fasta_path, \"rt\") as f:",
            "lineNumber": 49
          },
          {
            "text": "        for line in f:",
            "lineNumber": 50
          },
          {
            "text": "            line = line.strip()",
            "lineNumber": 51
          },
          {
            "text": "            if not line:",
            "lineNumber": 52
          },
          {
            "text": "                continue",
            "lineNumber": 53
          },
          {
            "lineNumber": 54
          },
          {
            "text": "            if line.startswith(\"&gt;\"):",
            "lineNumber": 55
          },
          {
            "text": "                # Save previous sequence if exists",
            "lineNumber": 56
          },
          {
            "text": "                if current_sequence:",
            "lineNumber": 57
          },
          {
            "text": "                    seq = \"\".join(current_sequence)",
            "lineNumber": 58
          },
          {
            "text": "                    seq_len = len(seq)",
            "lineNumber": 59
          },
          {
            "text": "                    if total_length + seq_len &lt;= max_length:",
            "lineNumber": 60
          },
          {
            "text": "                        sequences.append(seq)",
            "lineNumber": 61
          },
          {
            "text": "                        total_length += seq_len",
            "lineNumber": 62
          },
          {
            "text": "                    else:",
            "lineNumber": 63
          },
          {
            "text": "                        # Take partial sequence if we haven't reached max yet",
            "lineNumber": 64
          },
          {
            "text": "                        remaining = max_length - total_length",
            "lineNumber": 65
          },
          {
            "text": "                        if remaining &gt; 0:",
            "lineNumber": 66
          },
          {
            "text": "                            sequences.append(seq[:remaining])",
            "lineNumber": 67
          },
          {
            "text": "                        break",
            "lineNumber": 68
          },
          {
            "lineNumber": 69
          },
          {
            "text": "                # Start new sequence",
            "lineNumber": 70
          },
          {
            "text": "                current_header = line",
            "lineNumber": 71
          },
          {
            "text": "                current_sequence = []",
            "lineNumber": 72
          },
          {
            "text": "            else:",
            "lineNumber": 73
          },
          {
            "text": "                # Add to current sequence",
            "lineNumber": 74
          },
          {
            "text": "                current_sequence.append(line)",
            "lineNumber": 75
          },
          {
            "lineNumber": 76
          },
          {
            "text": "        # Handle last sequence",
            "lineNumber": 77
          },
          {
            "text": "        if current_sequence and total_length &lt; max_length:",
            "lineNumber": 78
          },
          {
            "text": "            seq = \"\".join(current_sequence)",
            "lineNumber": 79
          },
          {
            "text": "            seq_len = len(seq)",
            "lineNumber": 80
          },
          {
            "text": "            if total_length + seq_len &lt;= max_length:",
            "lineNumber": 81
          },
          {
            "text": "                sequences.append(seq)",
            "lineNumber": 82
          },
          {
            "text": "                total_length += seq_len",
            "lineNumber": 83
          },
          {
            "text": "            else:",
            "lineNumber": 84
          },
          {
            "text": "                remaining = max_length - total_length",
            "lineNumber": 85
          },
          {
            "text": "                if remaining &gt; 0:",
            "lineNumber": 86
          },
          {
            "text": "                    sequences.append(seq[:remaining])",
            "lineNumber": 87
          },
          {
            "lineNumber": 88
          },
          {
            "text": "    return sequences",
            "lineNumber": 89,
            "isSignature": true
          },
          {
            "lineNumber": 90
          },
          {
            "lineNumber": 91
          },
          {
            "text": "def load_sample_data(csv_path: Path) -&gt; Dict[Tuple[str, str], Dict]:",
            "lineNumber": 92,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 93
          }
        ]
      },
      "score": 0.42980000376701355
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./.cursor/plans/hyenadna_ph_regression_head_0c97fe04.plan.md",
        "range": {
          "startPosition": {
            "line": 227
          },
          "endPosition": {
            "line": 296
          }
        },
        "contents": "**Data**:\n\n- Train/Val/Test split: &#96;0.8/0.1/0.1&#96;\n- Filter samples with missing pH values\n- Random seed: &#96;42&#96; (for reproducibility)\n\n### Sequence Concatenation Details\n\n**Confirmed Feasibility**: [SEP] token (ID=1) exists in &#96;CharacterTokenizer&#96; and can be used for concatenation.\n\n**Implementation Strategy**:\n\n1. For each sample with multiple sequences:\n\n   - Tokenize each sequence: &#96;tokenizer(seq)[\"input_ids\"]&#96;\n   - Remove special tokens from individual sequences (keep only DNA tokens)\n   - Join sequences with [SEP] token: &#96;seq1_tokens + [1] + seq2_tokens + [1] + ...&#96;\n   - Truncate if total length &gt; &#96;max_length&#96;: keep last &#96;max_length&#96; tokens\n   - Add [CLS] at start if desired (though causal models may not need it)\n\n2. Edge cases:\n\n   - Empty sequence list: return empty or single [SEP] token\n   - Single sequence: no [SEP] needed, just tokenize and truncate\n   - All sequences too long: take last sequence and truncate to max_length\n\n3. Padding:\n\n   - Pad to &#96;max_length&#96; with [PAD] token (ID=4) on the left\n   - Create attention mask to ignore padding tokens in loss\n\n### File Structure\n\n&#96;&#96;&#96;\ntrain_hyenadna_ph.py\n├── Imports (torch, datasets, transformers, etc.)\n├── RegressionHead class\n├── Data loading functions\n│   ├── load_dataset_from_hf()\n│   ├── concatenate_sequences()\n│   ├── create_dataloader()\n│   └── collate_fn() for batching\n├── Model setup functions\n│   ├── setup_model()\n│   └── get_model_config()\n├── Training functions\n│   ├── train_epoch()\n│   ├── evaluate()\n│   └── save_checkpoint()\n└── main() with argument parsing\n&#96;&#96;&#96;\n\n### Dependencies\n\nAdd to project requirements (if not already present):\n\n- &#96;torch&#96; (PyTorch)\n- &#96;datasets&#96; (HuggingFace datasets)\n- &#96;transformers&#96; (for tokenizer compatibility)\n- &#96;numpy&#96;\n- &#96;scikit-learn&#96; (for metrics: R², MSE, MAE)\n- &#96;tqdm&#96; (optional, for progress bars)\n\n### Output Files\n\nThe script should save:\n\n- &#96;best_model.pt&#96;: Best model checkpoint (based on validation loss)\n- &#96;training_log.txt&#96;: Training history (loss, metrics per epoch)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "**Data**:",
            "lineNumber": 228
          },
          {
            "lineNumber": 229
          },
          {
            "text": "- Train/Val/Test split: &#96;0.8/0.1/0.1&#96;",
            "lineNumber": 230
          },
          {
            "text": "- Filter samples with missing pH values",
            "lineNumber": 231
          },
          {
            "text": "- Random seed: &#96;42&#96; (for reproducibility)",
            "lineNumber": 232
          },
          {
            "lineNumber": 233
          },
          {
            "text": "### Sequence Concatenation Details",
            "lineNumber": 234
          },
          {
            "lineNumber": 235
          },
          {
            "text": "**Confirmed Feasibility**: [SEP] token (ID=1) exists in &#96;CharacterTokenizer&#96; and can be used for concatenation.",
            "lineNumber": 236
          },
          {
            "lineNumber": 237
          },
          {
            "text": "**Implementation Strategy**:",
            "lineNumber": 238
          },
          {
            "lineNumber": 239
          },
          {
            "text": "1. For each sample with multiple sequences:",
            "lineNumber": 240
          },
          {
            "lineNumber": 241
          },
          {
            "text": "   - Tokenize each sequence: &#96;tokenizer(seq)[\"input_ids\"]&#96;",
            "lineNumber": 242
          },
          {
            "text": "   - Remove special tokens from individual sequences (keep only DNA tokens)",
            "lineNumber": 243
          },
          {
            "text": "   - Join sequences with [SEP] token: &#96;seq1_tokens + [1] + seq2_tokens + [1] + ...&#96;",
            "lineNumber": 244
          },
          {
            "text": "   - Truncate if total length &gt; &#96;max_length&#96;: keep last &#96;max_length&#96; tokens",
            "lineNumber": 245
          },
          {
            "text": "   - Add [CLS] at start if desired (though causal models may not need it)",
            "lineNumber": 246
          },
          {
            "lineNumber": 247
          },
          {
            "text": "2. Edge cases:",
            "lineNumber": 248
          },
          {
            "lineNumber": 249
          },
          {
            "text": "   - Empty sequence list: return empty or single [SEP] token",
            "lineNumber": 250
          },
          {
            "text": "   - Single sequence: no [SEP] needed, just tokenize and truncate",
            "lineNumber": 251
          },
          {
            "text": "   - All sequences too long: take last sequence and truncate to max_length",
            "lineNumber": 252
          },
          {
            "lineNumber": 253
          },
          {
            "text": "3. Padding:",
            "lineNumber": 254
          },
          {
            "lineNumber": 255
          },
          {
            "text": "   - Pad to &#96;max_length&#96; with [PAD] token (ID=4) on the left",
            "lineNumber": 256
          },
          {
            "text": "   - Create attention mask to ignore padding tokens in loss",
            "lineNumber": 257
          },
          {
            "lineNumber": 258
          },
          {
            "text": "### File Structure",
            "lineNumber": 259
          },
          {
            "lineNumber": 260
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 261
          },
          {
            "text": "train_hyenadna_ph.py",
            "lineNumber": 262
          },
          {
            "text": "├── Imports (torch, datasets, transformers, etc.)",
            "lineNumber": 263
          },
          {
            "text": "├── RegressionHead class",
            "lineNumber": 264
          },
          {
            "text": "├── Data loading functions",
            "lineNumber": 265
          },
          {
            "text": "│   ├── load_dataset_from_hf()",
            "lineNumber": 266
          },
          {
            "text": "│   ├── concatenate_sequences()",
            "lineNumber": 267
          },
          {
            "text": "│   ├── create_dataloader()",
            "lineNumber": 268
          },
          {
            "text": "│   └── collate_fn() for batching",
            "lineNumber": 269
          },
          {
            "text": "├── Model setup functions",
            "lineNumber": 270
          },
          {
            "text": "│   ├── setup_model()",
            "lineNumber": 271
          },
          {
            "text": "│   └── get_model_config()",
            "lineNumber": 272
          },
          {
            "text": "├── Training functions",
            "lineNumber": 273
          },
          {
            "text": "│   ├── train_epoch()",
            "lineNumber": 274
          },
          {
            "text": "│   ├── evaluate()",
            "lineNumber": 275
          },
          {
            "text": "│   └── save_checkpoint()",
            "lineNumber": 276
          },
          {
            "text": "└── main() with argument parsing",
            "lineNumber": 277
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 278
          },
          {
            "lineNumber": 279
          },
          {
            "text": "### Dependencies",
            "lineNumber": 280
          },
          {
            "lineNumber": 281
          },
          {
            "text": "Add to project requirements (if not already present):",
            "lineNumber": 282
          },
          {
            "lineNumber": 283
          },
          {
            "text": "- &#96;torch&#96; (PyTorch)",
            "lineNumber": 284
          },
          {
            "text": "- &#96;datasets&#96; (HuggingFace datasets)",
            "lineNumber": 285
          },
          {
            "text": "- &#96;transformers&#96; (for tokenizer compatibility)",
            "lineNumber": 286
          },
          {
            "text": "- &#96;numpy&#96;",
            "lineNumber": 287
          },
          {
            "text": "- &#96;scikit-learn&#96; (for metrics: R², MSE, MAE)",
            "lineNumber": 288
          },
          {
            "text": "- &#96;tqdm&#96; (optional, for progress bars)",
            "lineNumber": 289
          },
          {
            "lineNumber": 290
          },
          {
            "text": "### Output Files",
            "lineNumber": 291
          },
          {
            "lineNumber": 292
          },
          {
            "text": "The script should save:",
            "lineNumber": 293
          },
          {
            "lineNumber": 294
          },
          {
            "text": "- &#96;best_model.pt&#96;: Best model checkpoint (based on validation loss)",
            "lineNumber": 295
          },
          {
            "text": "- &#96;training_log.txt&#96;: Training history (loss, metrics per epoch)",
            "lineNumber": 296
          }
        ]
      },
      "score": 0.42188289761543274
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./create_hf_dataset.py",
        "range": {
          "startPosition": {
            "line": 155
          },
          "endPosition": {
            "line": 216,
            "column": 18
          }
        },
        "contents": "def get_existing_records(dataset_repo: str) -&gt; set:\n\n        # Dataset doesn't exist yet or is empty\n        print(f\"  Dataset doesn't exist yet or is empty: {e}\")\n        return set()\n\n\ndef create_dataset_records(\n    fasta_files: List[Tuple[Path, str, str]],\n    sample_data: Dict[Tuple[str, str], Dict],\n    existing_records: set,\n) -&gt; List[Dict]:\n    \"\"\"\n    Create dataset records from fasta files.\n    Returns a list of dictionaries with study_name, sample_id, pH, and sequences.\n    \"\"\"\n    records = []\n\n    for fasta_path, study_name, sample_id in fasta_files:\n        key = (study_name, sample_id)\n\n        # Skip if already exists\n        if key in existing_records:\n            print(f\"  Skipping {study_name}/{sample_id} (already exists)\")\n            continue\n\n        print(f\"  Processing {study_name}/{sample_id}...\")\n\n        # Read sequences\n        sequences = read_sequences_from_fasta(fasta_path)\n        total_length = sum(len(seq) for seq in sequences)\n\n        # Get sample data\n        data = sample_data[key]\n\n        # Create record\n        record = {\n            \"study_name\": study_name,\n            \"sample_id\": sample_id,\n            \"pH\": data[\"pH\"],\n            \"sequences\": sequences,  # List of strings for easy slicing\n            \"num_sequences\": len(sequences),\n            \"total_dna_length\": total_length,\n            \"sample\": data[\"sample\"],\n            \"environment\": data[\"environment\"],\n            \"domain\": data[\"domain\"],\n        }\n\n        records.append(record)\n        print(f\"    Read {len(sequences)} sequences ({total_length:,} nucleotides)\")\n\n    return records",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def get_existing_records(dataset_repo: str) -&gt; set:",
            "lineNumber": 156,
            "isSignature": true
          },
          {
            "lineNumber": 167
          },
          {
            "text": "        # Dataset doesn't exist yet or is empty",
            "lineNumber": 168
          },
          {
            "text": "        print(f\"  Dataset doesn't exist yet or is empty: {e}\")",
            "lineNumber": 169
          },
          {
            "text": "        return set()",
            "lineNumber": 170,
            "isSignature": true
          },
          {
            "lineNumber": 171
          },
          {
            "lineNumber": 172
          },
          {
            "text": "def create_dataset_records(",
            "lineNumber": 173,
            "isSignature": true
          },
          {
            "text": "    fasta_files: List[Tuple[Path, str, str]],",
            "lineNumber": 174,
            "isSignature": true
          },
          {
            "text": "    sample_data: Dict[Tuple[str, str], Dict],",
            "lineNumber": 175,
            "isSignature": true
          },
          {
            "text": "    existing_records: set,",
            "lineNumber": 176,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[Dict]:",
            "lineNumber": 177,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 178
          },
          {
            "text": "    Create dataset records from fasta files.",
            "lineNumber": 179
          },
          {
            "text": "    Returns a list of dictionaries with study_name, sample_id, pH, and sequences.",
            "lineNumber": 180
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 181
          },
          {
            "text": "    records = []",
            "lineNumber": 182
          },
          {
            "lineNumber": 183
          },
          {
            "text": "    for fasta_path, study_name, sample_id in fasta_files:",
            "lineNumber": 184
          },
          {
            "text": "        key = (study_name, sample_id)",
            "lineNumber": 185
          },
          {
            "lineNumber": 186
          },
          {
            "text": "        # Skip if already exists",
            "lineNumber": 187
          },
          {
            "text": "        if key in existing_records:",
            "lineNumber": 188
          },
          {
            "text": "            print(f\"  Skipping {study_name}/{sample_id} (already exists)\")",
            "lineNumber": 189
          },
          {
            "text": "            continue",
            "lineNumber": 190
          },
          {
            "lineNumber": 191
          },
          {
            "text": "        print(f\"  Processing {study_name}/{sample_id}...\")",
            "lineNumber": 192
          },
          {
            "lineNumber": 193
          },
          {
            "text": "        # Read sequences",
            "lineNumber": 194
          },
          {
            "text": "        sequences = read_sequences_from_fasta(fasta_path)",
            "lineNumber": 195
          },
          {
            "text": "        total_length = sum(len(seq) for seq in sequences)",
            "lineNumber": 196
          },
          {
            "lineNumber": 197
          },
          {
            "text": "        # Get sample data",
            "lineNumber": 198
          },
          {
            "text": "        data = sample_data[key]",
            "lineNumber": 199
          },
          {
            "lineNumber": 200
          },
          {
            "text": "        # Create record",
            "lineNumber": 201
          },
          {
            "text": "        record = {",
            "lineNumber": 202
          },
          {
            "text": "            \"study_name\": study_name,",
            "lineNumber": 203
          },
          {
            "text": "            \"sample_id\": sample_id,",
            "lineNumber": 204
          },
          {
            "text": "            \"pH\": data[\"pH\"],",
            "lineNumber": 205
          },
          {
            "text": "            \"sequences\": sequences,  # List of strings for easy slicing",
            "lineNumber": 206
          },
          {
            "text": "            \"num_sequences\": len(sequences),",
            "lineNumber": 207
          },
          {
            "text": "            \"total_dna_length\": total_length,",
            "lineNumber": 208
          },
          {
            "text": "            \"sample\": data[\"sample\"],",
            "lineNumber": 209
          },
          {
            "text": "            \"environment\": data[\"environment\"],",
            "lineNumber": 210
          },
          {
            "text": "            \"domain\": data[\"domain\"],",
            "lineNumber": 211
          },
          {
            "text": "        }",
            "lineNumber": 212
          },
          {
            "lineNumber": 213
          },
          {
            "text": "        records.append(record)",
            "lineNumber": 214
          },
          {
            "text": "        print(f\"    Read {len(sequences)} sequences ({total_length:,} nucleotides)\")",
            "lineNumber": 215
          },
          {
            "lineNumber": 216
          },
          {
            "text": "    return records",
            "lineNumber": 217,
            "isSignature": true
          }
        ]
      },
      "score": 0.38301679491996765
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 278
          },
          "endPosition": {
            "line": 315,
            "column": 5
          }
        },
        "contents": "def collate_fn(batch: List[Dict]) -&gt; Dict:\n\n    Collate function for DataLoader.\n\n    Args:\n        batch: List of dicts with 'input_ids', 'ph', 'study_name', 'sample_id'\n\n    Returns:\n        Dictionary with batched tensors (input_ids, attention_mask, ph) and\n        lists (study_name, sample_id)\n    \"\"\"\n    input_ids = [item[\"input_ids\"] for item in batch]\n    ph_values = torch.stack([item[\"ph\"] for item in batch])\n    study_names = [item.get(\"study_name\", \"\") for item in batch]\n    sample_ids = [item.get(\"sample_id\", \"\") for item in batch]\n\n    # Find max length in batch\n    max_len = max(len(ids) for ids in input_ids)\n\n    # Pad sequences to max length (left padding for causal model)\n    padded_ids = []\n    attention_masks = []\n    for ids in input_ids:\n        pad_length = max_len - len(ids)\n        # Left pad with PAD_TOKEN_ID\n        padded = [PAD_TOKEN_ID] * pad_length + ids.tolist()\n        # Create attention mask (1 for real tokens, 0 for padding)\n        mask = [0] * pad_length + [1] * len(ids)\n        padded_ids.append(padded)\n        attention_masks.append(mask)\n\n    return {\n        \"input_ids\": torch.LongTensor(padded_ids),\n        \"attention_mask\": torch.LongTensor(attention_masks),\n        \"ph\": ph_values,\n        \"study_name\": study_names,\n        \"sample_id\": sample_ids,\n    }",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def collate_fn(batch: List[Dict]) -&gt; Dict:",
            "lineNumber": 279,
            "isSignature": true
          },
          {
            "lineNumber": 280
          },
          {
            "text": "    Collate function for DataLoader.",
            "lineNumber": 281
          },
          {
            "lineNumber": 282
          },
          {
            "text": "    Args:",
            "lineNumber": 283
          },
          {
            "text": "        batch: List of dicts with 'input_ids', 'ph', 'study_name', 'sample_id'",
            "lineNumber": 284
          },
          {
            "lineNumber": 285
          },
          {
            "text": "    Returns:",
            "lineNumber": 286
          },
          {
            "text": "        Dictionary with batched tensors (input_ids, attention_mask, ph) and",
            "lineNumber": 287
          },
          {
            "text": "        lists (study_name, sample_id)",
            "lineNumber": 288
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 289
          },
          {
            "text": "    input_ids = [item[\"input_ids\"] for item in batch]",
            "lineNumber": 290
          },
          {
            "text": "    ph_values = torch.stack([item[\"ph\"] for item in batch])",
            "lineNumber": 291
          },
          {
            "text": "    study_names = [item.get(\"study_name\", \"\") for item in batch]",
            "lineNumber": 292
          },
          {
            "text": "    sample_ids = [item.get(\"sample_id\", \"\") for item in batch]",
            "lineNumber": 293
          },
          {
            "lineNumber": 294
          },
          {
            "text": "    # Find max length in batch",
            "lineNumber": 295
          },
          {
            "text": "    max_len = max(len(ids) for ids in input_ids)",
            "lineNumber": 296
          },
          {
            "lineNumber": 297
          },
          {
            "text": "    # Pad sequences to max length (left padding for causal model)",
            "lineNumber": 298
          },
          {
            "text": "    padded_ids = []",
            "lineNumber": 299
          },
          {
            "text": "    attention_masks = []",
            "lineNumber": 300
          },
          {
            "text": "    for ids in input_ids:",
            "lineNumber": 301
          },
          {
            "text": "        pad_length = max_len - len(ids)",
            "lineNumber": 302
          },
          {
            "text": "        # Left pad with PAD_TOKEN_ID",
            "lineNumber": 303
          },
          {
            "text": "        padded = [PAD_TOKEN_ID] * pad_length + ids.tolist()",
            "lineNumber": 304
          },
          {
            "text": "        # Create attention mask (1 for real tokens, 0 for padding)",
            "lineNumber": 305
          },
          {
            "text": "        mask = [0] * pad_length + [1] * len(ids)",
            "lineNumber": 306
          },
          {
            "text": "        padded_ids.append(padded)",
            "lineNumber": 307
          },
          {
            "text": "        attention_masks.append(mask)",
            "lineNumber": 308
          },
          {
            "lineNumber": 309
          },
          {
            "text": "    return {",
            "lineNumber": 310
          },
          {
            "text": "        \"input_ids\": torch.LongTensor(padded_ids),",
            "lineNumber": 311
          },
          {
            "text": "        \"attention_mask\": torch.LongTensor(attention_masks),",
            "lineNumber": 312
          },
          {
            "text": "        \"ph\": ph_values,",
            "lineNumber": 313
          },
          {
            "text": "        \"study_name\": study_names,",
            "lineNumber": 314
          },
          {
            "text": "        \"sample_id\": sample_ids,",
            "lineNumber": 315
          },
          {
            "text": "    }",
            "lineNumber": 316,
            "isSignature": true
          }
        ]
      },
      "score": 0.3448795974254608
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 199
          },
          "endPosition": {
            "line": 315
          }
        },
        "contents": "def concatenate_sequences(\n    sequences: List[str],\n    tokenizer: CharacterTokenizer,\n    max_length: int,\n    sep_token_id: int = SEP_TOKEN_ID,\n) -&gt; List[int]:\n\n\n    if not sequences_to_use:\n        # Even the first sequence is too long, we'll truncate it\n        sequences_to_use = [valid_sequences[0]]\n\n    # Now tokenize only the sequences we'll use\n    tokenized_seqs = []\n    for seq in sequences_to_use:\n        # Tokenize and extract input_ids\n        tokens = tokenizer(seq)[\"input_ids\"]\n        # Remove special tokens (keep only DNA character tokens, which have IDs &gt;= 7)\n        dna_tokens = [t for t in tokens if t &gt;= 7]\n        if dna_tokens:\n            tokenized_seqs.append(dna_tokens)\n\n    if not tokenized_seqs:\n        # No valid tokens after filtering, return single [SEP]\n        return [sep_token_id]\n\n    # Concatenate with [SEP] tokens between sequences\n    result = []\n    for i, seq_tokens in enumerate(tokenized_seqs):\n        if i &gt; 0:\n            result.append(sep_token_id)\n        result.extend(seq_tokens)\n\n    # Truncate to max_length if needed (keep last part for causal model)\n    if len(result) &gt; max_length:\n        result = result[-max_length:]\n\n    return result\n\n\ndef collate_fn(batch: List[Dict]) -&gt; Dict:\n    \"\"\"\n    Collate function for DataLoader.\n\n    Args:\n        batch: List of dicts with 'input_ids', 'ph', 'study_name', 'sample_id'\n\n    Returns:\n        Dictionary with batched tensors (input_ids, attention_mask, ph) and\n        lists (study_name, sample_id)\n    ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def concatenate_sequences(",
            "lineNumber": 200,
            "isSignature": true
          },
          {
            "text": "    sequences: List[str],",
            "lineNumber": 201,
            "isSignature": true
          },
          {
            "text": "    tokenizer: CharacterTokenizer,",
            "lineNumber": 202,
            "isSignature": true
          },
          {
            "text": "    max_length: int,",
            "lineNumber": 203,
            "isSignature": true
          },
          {
            "text": "    sep_token_id: int = SEP_TOKEN_ID,",
            "lineNumber": 204,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[int]:",
            "lineNumber": 205,
            "isSignature": true
          },
          {
            "lineNumber": 245
          },
          {
            "lineNumber": 246
          },
          {
            "text": "    if not sequences_to_use:",
            "lineNumber": 247
          },
          {
            "text": "        # Even the first sequence is too long, we'll truncate it",
            "lineNumber": 248
          },
          {
            "text": "        sequences_to_use = [valid_sequences[0]]",
            "lineNumber": 249
          },
          {
            "lineNumber": 250
          },
          {
            "text": "    # Now tokenize only the sequences we'll use",
            "lineNumber": 251
          },
          {
            "text": "    tokenized_seqs = []",
            "lineNumber": 252
          },
          {
            "text": "    for seq in sequences_to_use:",
            "lineNumber": 253
          },
          {
            "text": "        # Tokenize and extract input_ids",
            "lineNumber": 254
          },
          {
            "text": "        tokens = tokenizer(seq)[\"input_ids\"]",
            "lineNumber": 255
          },
          {
            "text": "        # Remove special tokens (keep only DNA character tokens, which have IDs &gt;= 7)",
            "lineNumber": 256
          },
          {
            "text": "        dna_tokens = [t for t in tokens if t &gt;= 7]",
            "lineNumber": 257
          },
          {
            "text": "        if dna_tokens:",
            "lineNumber": 258
          },
          {
            "text": "            tokenized_seqs.append(dna_tokens)",
            "lineNumber": 259
          },
          {
            "lineNumber": 260
          },
          {
            "text": "    if not tokenized_seqs:",
            "lineNumber": 261
          },
          {
            "text": "        # No valid tokens after filtering, return single [SEP]",
            "lineNumber": 262
          },
          {
            "text": "        return [sep_token_id]",
            "lineNumber": 263
          },
          {
            "lineNumber": 264
          },
          {
            "text": "    # Concatenate with [SEP] tokens between sequences",
            "lineNumber": 265
          },
          {
            "text": "    result = []",
            "lineNumber": 266
          },
          {
            "text": "    for i, seq_tokens in enumerate(tokenized_seqs):",
            "lineNumber": 267
          },
          {
            "text": "        if i &gt; 0:",
            "lineNumber": 268
          },
          {
            "text": "            result.append(sep_token_id)",
            "lineNumber": 269
          },
          {
            "text": "        result.extend(seq_tokens)",
            "lineNumber": 270
          },
          {
            "lineNumber": 271
          },
          {
            "text": "    # Truncate to max_length if needed (keep last part for causal model)",
            "lineNumber": 272
          },
          {
            "text": "    if len(result) &gt; max_length:",
            "lineNumber": 273
          },
          {
            "text": "        result = result[-max_length:]",
            "lineNumber": 274
          },
          {
            "lineNumber": 275
          },
          {
            "text": "    return result",
            "lineNumber": 276,
            "isSignature": true
          },
          {
            "lineNumber": 277
          },
          {
            "lineNumber": 278
          },
          {
            "text": "def collate_fn(batch: List[Dict]) -&gt; Dict:",
            "lineNumber": 279,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 280
          },
          {
            "text": "    Collate function for DataLoader.",
            "lineNumber": 281
          },
          {
            "lineNumber": 282
          },
          {
            "text": "    Args:",
            "lineNumber": 283
          },
          {
            "text": "        batch: List of dicts with 'input_ids', 'ph', 'study_name', 'sample_id'",
            "lineNumber": 284
          },
          {
            "lineNumber": 285
          },
          {
            "text": "    Returns:",
            "lineNumber": 286
          },
          {
            "text": "        Dictionary with batched tensors (input_ids, attention_mask, ph) and",
            "lineNumber": 287
          },
          {
            "text": "        lists (study_name, sample_id)",
            "lineNumber": 288
          },
          {
            "text": "    ",
            "lineNumber": 289
          }
        ]
      },
      "score": 0.333270788192749
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./download_sra_data.py",
        "range": {
          "startPosition": {
            "line": 104,
            "column": 20
          },
          "endPosition": {
            "line": 165
          }
        },
        "contents": "def remove_last_sequence(fasta_path: Path, output_path: Path) -&gt; bool:\n    \"\"\"\n    Remove the last sequence from a FASTA file.\n\n    The last sequence may be truncated due to incomplete download.\n    \"\"\"\n    try:\n        sequences = []\n        current_header = None\n        current_sequence = []\n\n        with open(fasta_path, \"r\") as f:\n            for line in f:\n                line = line.rstrip()\n                if line.startswith(\"&gt;\"):\n                    # Save previous sequence if exists\n                    if current_header is not None and current_sequence:\n                        sequences.append((current_header, \"\".join(current_sequence)))\n                    # Start new sequence\n                    current_header = line\n                    current_sequence = []\n                else:\n                    if line:  # Skip empty lines\n                        current_sequence.append(line)\n\n        # Don't include the last sequence (may be truncated)\n        # The last sequence is the one we're currently building (current_header, current_sequence)\n        # which we intentionally don't add to sequences\n\n        if len(sequences) == 0:\n            print(\n                \"    Warning: No complete sequences found in file (only one sequence which may be truncated)\"\n            )\n            return False\n\n        # Write all sequences (the last one is already excluded)\n        with open(output_path, \"w\") as f:\n            for header, sequence in sequences:\n                f.write(f\"{header}\\n\")\n                # Write sequence in chunks of 80 characters (FASTA format)\n                for i in range(0, len(sequence), 80):\n                    f.write(f\"{sequence[i:i+80]}\\n\")\n\n        return True\n    except Exception as e:\n        print(f\"    Error processing FASTA: {e}\")\n        return False\n\n\ndef compress_fasta(fasta_path: Path, output_path: Path) -&gt; bool:\n    \"\"\"Compress a FASTA file using gzip.\"\"\"",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def remove_last_sequence(fasta_path: Path, output_path: Path) -&gt; bool:",
            "lineNumber": 108,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 109
          },
          {
            "text": "    Remove the last sequence from a FASTA file.",
            "lineNumber": 110
          },
          {
            "lineNumber": 111
          },
          {
            "text": "    The last sequence may be truncated due to incomplete download.",
            "lineNumber": 112
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 113
          },
          {
            "text": "    try:",
            "lineNumber": 114
          },
          {
            "text": "        sequences = []",
            "lineNumber": 115
          },
          {
            "text": "        current_header = None",
            "lineNumber": 116
          },
          {
            "text": "        current_sequence = []",
            "lineNumber": 117
          },
          {
            "lineNumber": 118
          },
          {
            "text": "        with open(fasta_path, \"r\") as f:",
            "lineNumber": 119
          },
          {
            "text": "            for line in f:",
            "lineNumber": 120
          },
          {
            "text": "                line = line.rstrip()",
            "lineNumber": 121
          },
          {
            "text": "                if line.startswith(\"&gt;\"):",
            "lineNumber": 122
          },
          {
            "text": "                    # Save previous sequence if exists",
            "lineNumber": 123
          },
          {
            "text": "                    if current_header is not None and current_sequence:",
            "lineNumber": 124
          },
          {
            "text": "                        sequences.append((current_header, \"\".join(current_sequence)))",
            "lineNumber": 125
          },
          {
            "text": "                    # Start new sequence",
            "lineNumber": 126
          },
          {
            "text": "                    current_header = line",
            "lineNumber": 127
          },
          {
            "text": "                    current_sequence = []",
            "lineNumber": 128
          },
          {
            "text": "                else:",
            "lineNumber": 129
          },
          {
            "text": "                    if line:  # Skip empty lines",
            "lineNumber": 130
          },
          {
            "text": "                        current_sequence.append(line)",
            "lineNumber": 131
          },
          {
            "lineNumber": 132
          },
          {
            "text": "        # Don't include the last sequence (may be truncated)",
            "lineNumber": 133
          },
          {
            "text": "        # The last sequence is the one we're currently building (current_header, current_sequence)",
            "lineNumber": 134
          },
          {
            "text": "        # which we intentionally don't add to sequences",
            "lineNumber": 135
          },
          {
            "lineNumber": 136
          },
          {
            "text": "        if len(sequences) == 0:",
            "lineNumber": 137
          },
          {
            "text": "            print(",
            "lineNumber": 138
          },
          {
            "text": "                \"    Warning: No complete sequences found in file (only one sequence which may be truncated)\"",
            "lineNumber": 139
          },
          {
            "text": "            )",
            "lineNumber": 140
          },
          {
            "text": "            return False",
            "lineNumber": 141
          },
          {
            "lineNumber": 142
          },
          {
            "text": "        # Write all sequences (the last one is already excluded)",
            "lineNumber": 143
          },
          {
            "text": "        with open(output_path, \"w\") as f:",
            "lineNumber": 144
          },
          {
            "text": "            for header, sequence in sequences:",
            "lineNumber": 145
          },
          {
            "text": "                f.write(f\"{header}\\n\")",
            "lineNumber": 146
          },
          {
            "text": "                # Write sequence in chunks of 80 characters (FASTA format)",
            "lineNumber": 147
          },
          {
            "text": "                for i in range(0, len(sequence), 80):",
            "lineNumber": 148
          },
          {
            "text": "                    f.write(f\"{sequence[i:i+80]}\\n\")",
            "lineNumber": 149
          },
          {
            "lineNumber": 150
          },
          {
            "text": "        return True",
            "lineNumber": 151
          },
          {
            "text": "    except Exception as e:",
            "lineNumber": 152
          },
          {
            "text": "        print(f\"    Error processing FASTA: {e}\")",
            "lineNumber": 153
          },
          {
            "text": "        return False",
            "lineNumber": 154,
            "isSignature": true
          },
          {
            "lineNumber": 155
          },
          {
            "lineNumber": 156
          },
          {
            "text": "def compress_fasta(fasta_path: Path, output_path: Path) -&gt; bool:",
            "lineNumber": 157,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Compress a FASTA file using gzip.\"\"\"",
            "lineNumber": 158
          }
        ]
      },
      "score": 0.32523036003112793
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 45,
            "column": 16
          }
        },
        "contents": "#!/usr/bin/env python3\n\"\"\"\nTrain HyenaDNA model for pH prediction from DNA sequences.\n\nThis script implements a regression head for HyenaDNA to predict continuous pH values.\nIt loads data from HuggingFace, concatenates multiple sequences with [SEP] tokens,\nand trains using pretrained HyenaDNA weights.\n\"\"\"\n\nimport argparse\nimport json\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_dataset\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\n\n# Add hyena-dna to path\nsys.path.insert(0, str(Path(__file__).parent / \"hyena-dna\"))\n\nfrom huggingface_wrapper import HyenaDNAPreTrainedModel\nfrom standalone_hyenadna import CharacterTokenizer, HyenaDNAModel\n\n# Model size configurations\nMODEL_CONFIGS = {\n    \"hyenadna-tiny-1k-seqlen\": {\"max_length\": 1024},\n    \"hyenadna-small-32k-seqlen\": {\"max_length\": 32768},\n    \"hyenadna-medium-160k-seqlen\": {\"max_length\": 160000},\n    \"hyenadna-medium-450k-seqlen\": {\"max_length\": 450000},\n    \"hyenadna-large-1m-seqlen\": {\"max_length\": 1_000_000},\n}\n\n# Special token IDs\nSEP_TOKEN_ID = 1\nPAD_TOKEN_ID = 4\nCLS_TOKEN_ID = 0",
        "signatures": {},
        "detailedLines": [
          {
            "text": "#!/usr/bin/env python3",
            "lineNumber": 1
          },
          {
            "text": "\"\"\"",
            "lineNumber": 2
          },
          {
            "text": "Train HyenaDNA model for pH prediction from DNA sequences.",
            "lineNumber": 3
          },
          {
            "lineNumber": 4
          },
          {
            "text": "This script implements a regression head for HyenaDNA to predict continuous pH values.",
            "lineNumber": 5
          },
          {
            "text": "It loads data from HuggingFace, concatenates multiple sequences with [SEP] tokens,",
            "lineNumber": 6
          },
          {
            "text": "and trains using pretrained HyenaDNA weights.",
            "lineNumber": 7
          },
          {
            "text": "\"\"\"",
            "lineNumber": 8
          },
          {
            "lineNumber": 9
          },
          {
            "text": "import argparse",
            "lineNumber": 10
          },
          {
            "text": "import json",
            "lineNumber": 11
          },
          {
            "text": "import os",
            "lineNumber": 12
          },
          {
            "text": "import sys",
            "lineNumber": 13
          },
          {
            "text": "from pathlib import Path",
            "lineNumber": 14
          },
          {
            "text": "from typing import Dict, List, Optional, Tuple",
            "lineNumber": 15
          },
          {
            "lineNumber": 16
          },
          {
            "text": "import numpy as np",
            "lineNumber": 17
          },
          {
            "text": "import torch",
            "lineNumber": 18
          },
          {
            "text": "import torch.nn as nn",
            "lineNumber": 19
          },
          {
            "text": "import torch.nn.functional as F",
            "lineNumber": 20
          },
          {
            "text": "from datasets import load_dataset",
            "lineNumber": 21
          },
          {
            "text": "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score",
            "lineNumber": 22
          },
          {
            "text": "from sklearn.model_selection import train_test_split",
            "lineNumber": 23
          },
          {
            "lineNumber": 24
          },
          {
            "text": "from torch.utils.data import DataLoader, Dataset",
            "lineNumber": 25
          },
          {
            "text": "from tqdm import tqdm",
            "lineNumber": 26
          },
          {
            "lineNumber": 27
          },
          {
            "text": "# Add hyena-dna to path",
            "lineNumber": 28
          },
          {
            "text": "sys.path.insert(0, str(Path(__file__).parent / \"hyena-dna\"))",
            "lineNumber": 29
          },
          {
            "lineNumber": 30
          },
          {
            "text": "from huggingface_wrapper import HyenaDNAPreTrainedModel",
            "lineNumber": 31
          },
          {
            "text": "from standalone_hyenadna import CharacterTokenizer, HyenaDNAModel",
            "lineNumber": 32
          },
          {
            "lineNumber": 33
          },
          {
            "text": "# Model size configurations",
            "lineNumber": 34
          },
          {
            "text": "MODEL_CONFIGS = {",
            "lineNumber": 35
          },
          {
            "text": "    \"hyenadna-tiny-1k-seqlen\": {\"max_length\": 1024},",
            "lineNumber": 36
          },
          {
            "text": "    \"hyenadna-small-32k-seqlen\": {\"max_length\": 32768},",
            "lineNumber": 37
          },
          {
            "text": "    \"hyenadna-medium-160k-seqlen\": {\"max_length\": 160000},",
            "lineNumber": 38
          },
          {
            "text": "    \"hyenadna-medium-450k-seqlen\": {\"max_length\": 450000},",
            "lineNumber": 39
          },
          {
            "text": "    \"hyenadna-large-1m-seqlen\": {\"max_length\": 1_000_000},",
            "lineNumber": 40
          },
          {
            "text": "}",
            "lineNumber": 41
          },
          {
            "lineNumber": 42
          },
          {
            "text": "# Special token IDs",
            "lineNumber": 43
          },
          {
            "text": "SEP_TOKEN_ID = 1",
            "lineNumber": 44
          },
          {
            "text": "PAD_TOKEN_ID = 4",
            "lineNumber": 45
          },
          {
            "text": "CLS_TOKEN_ID = 0",
            "lineNumber": 46
          }
        ]
      },
      "score": 0.3002311885356903
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./create_hf_dataset.py",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 88
          }
        },
        "contents": "#!/usr/bin/env python3\n\"\"\"\nCreate a Hugging Face dataset from DNA sequences in fasta files and sample data.\n\"\"\"\n\nimport os\nimport gzip\nimport csv\nimport re\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple, Optional\nfrom dotenv import load_dotenv\nfrom datasets import Dataset, DatasetDict, load_dataset\nfrom huggingface_hub import HfApi, login\n\n# Load environment variables\nload_dotenv()\n\nHF_TOKEN = os.getenv(\"HF_TOKEN\")\nHF_USERNAME = \"jedick\"\nHF_DATASET_NAME = \"microbial-DNA-pH\"\nHF_DATASET_REPO = f\"{HF_USERNAME}/{HF_DATASET_NAME}\"\n\n# Constants\nMAX_DNA_LENGTH = 1 * 256 * 1024  # 1 MB in bytes (262,144 bytes)\n\n\ndef parse_fasta_header(header: str) -&gt; Optional[str]:\n    \"\"\"Extract sequence identifier from FASTA header.\"\"\"\n    # Headers look like: &gt;SRR7775171.1 1 length=251\n    match = re.match(r\"&gt;(\\S+)\", header)\n    if match:\n        return match.group(1)\n    return None\n\n\ndef read_sequences_from_fasta(\n    fasta_path: Path, max_length: int = MAX_DNA_LENGTH\n) -&gt; List[str]:\n    ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "#!/usr/bin/env python3",
            "lineNumber": 1
          },
          {
            "text": "\"\"\"",
            "lineNumber": 2
          },
          {
            "text": "Create a Hugging Face dataset from DNA sequences in fasta files and sample data.",
            "lineNumber": 3
          },
          {
            "text": "\"\"\"",
            "lineNumber": 4
          },
          {
            "lineNumber": 5
          },
          {
            "text": "import os",
            "lineNumber": 6
          },
          {
            "text": "import gzip",
            "lineNumber": 7
          },
          {
            "text": "import csv",
            "lineNumber": 8
          },
          {
            "text": "import re",
            "lineNumber": 9
          },
          {
            "text": "from pathlib import Path",
            "lineNumber": 10
          },
          {
            "text": "from typing import Dict, List, Tuple, Optional",
            "lineNumber": 11
          },
          {
            "text": "from dotenv import load_dotenv",
            "lineNumber": 12
          },
          {
            "text": "from datasets import Dataset, DatasetDict, load_dataset",
            "lineNumber": 13
          },
          {
            "text": "from huggingface_hub import HfApi, login",
            "lineNumber": 14
          },
          {
            "lineNumber": 15
          },
          {
            "text": "# Load environment variables",
            "lineNumber": 16
          },
          {
            "text": "load_dotenv()",
            "lineNumber": 17
          },
          {
            "lineNumber": 18
          },
          {
            "text": "HF_TOKEN = os.getenv(\"HF_TOKEN\")",
            "lineNumber": 19
          },
          {
            "text": "HF_USERNAME = \"jedick\"",
            "lineNumber": 20
          },
          {
            "text": "HF_DATASET_NAME = \"microbial-DNA-pH\"",
            "lineNumber": 21
          },
          {
            "text": "HF_DATASET_REPO = f\"{HF_USERNAME}/{HF_DATASET_NAME}\"",
            "lineNumber": 22
          },
          {
            "lineNumber": 23
          },
          {
            "text": "# Constants",
            "lineNumber": 24
          },
          {
            "text": "MAX_DNA_LENGTH = 1 * 256 * 1024  # 1 MB in bytes (262,144 bytes)",
            "lineNumber": 25
          },
          {
            "lineNumber": 26
          },
          {
            "lineNumber": 27
          },
          {
            "text": "def parse_fasta_header(header: str) -&gt; Optional[str]:",
            "lineNumber": 28,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Extract sequence identifier from FASTA header.\"\"\"",
            "lineNumber": 29
          },
          {
            "text": "    # Headers look like: &gt;SRR7775171.1 1 length=251",
            "lineNumber": 30
          },
          {
            "text": "    match = re.match(r\"&gt;(\\S+)\", header)",
            "lineNumber": 31
          },
          {
            "text": "    if match:",
            "lineNumber": 32
          },
          {
            "text": "        return match.group(1)",
            "lineNumber": 33
          },
          {
            "text": "    return None",
            "lineNumber": 34,
            "isSignature": true
          },
          {
            "lineNumber": 35
          },
          {
            "lineNumber": 36
          },
          {
            "text": "def read_sequences_from_fasta(",
            "lineNumber": 37,
            "isSignature": true
          },
          {
            "text": "    fasta_path: Path, max_length: int = MAX_DNA_LENGTH",
            "lineNumber": 38,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[str]:",
            "lineNumber": 39,
            "isSignature": true
          },
          {
            "text": "    ",
            "lineNumber": 40
          }
        ]
      },
      "score": 0.29980653524398804
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./README.md",
        "range": {
          "startPosition": {
            "line": 30
          },
          "endPosition": {
            "line": 68
          }
        },
        "contents": "- **Cleanup on interruption**: Removes temporary files if the script is interrupted (Ctrl+C), preventing leftover files\n\n**Output Structure:**\n- Files are saved to &#96;data/fasta/{study_name}/{sample_id}.fasta.gz&#96;\n- Each file contains complete DNA sequences (except the last one, which is removed to avoid truncation artifacts)\n\n**Requirements:**\n- &#96;requests&#96; library for HTTP downloads\n- &#96;zcat&#96; command (part of gzip utilities) for extracting incomplete gzip files\n\n&lt;/details&gt;\n\n## Hugging Face dataset creation\n\nThe &#96;create_hf_dataset.py&#96; script creates a Hugging Face dataset from DNA sequences stored in gzipped FASTA files and sample metadata. This dataset serves as the single source of truth for sample data across all downstream tasks, ensuring consistent train-test splits between traditional ML and deep learning workflows.\n\n**Important**: After this step, &#96;sample_data.csv&#96; is no longer required for downstream tasks. The HuggingFace dataset contains all necessary sample metadata (pH, environment, domain, study_name, sample_id) and is used by both the traditional ML workflow (&#96;predict_ph.py&#96;) and the deep learning workflow (&#96;train_hyenadna_ph.py&#96;) to maintain consistent data splits.\n\n&#96;&#96;&#96;bash\n# Requires HF_TOKEN in .env file\npython create_hf_dataset.py\n&#96;&#96;&#96;\n\n&lt;details&gt;\n&lt;summary&gt;Details&lt;/summary&gt;\n\nThe script processes DNA sequences from FASTA files organized by study (&#96;data/fasta/{study_name}/{sample_id}.fasta.gz&#96;) and combines them with metadata from &#96;sample_data.csv&#96; to create a structured dataset suitable for machine learning workflows.\n\n- **Automatic validation**: Ensures all FASTA files have corresponding entries in &#96;sample_data.csv&#96;\n- **Incremental updates**: Skips samples that already exist in the dataset, allowing for incremental additions\n- **Size management**: Limits total DNA length per sample to 0.25 MB to manage dataset size\n- **Metadata integration**: Includes sample metadata (pH, environment, domain) alongside sequences\n- **Consistent splits**: Used by both workflows to ensure identical train-test splits (same random seed, same test set)\n\n&lt;/details&gt;\n\n## RDP phylum counts (features)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "- **Cleanup on interruption**: Removes temporary files if the script is interrupted (Ctrl+C), preventing leftover files",
            "lineNumber": 31
          },
          {
            "lineNumber": 32
          },
          {
            "text": "**Output Structure:**",
            "lineNumber": 33
          },
          {
            "text": "- Files are saved to &#96;data/fasta/{study_name}/{sample_id}.fasta.gz&#96;",
            "lineNumber": 34
          },
          {
            "text": "- Each file contains complete DNA sequences (except the last one, which is removed to avoid truncation artifacts)",
            "lineNumber": 35
          },
          {
            "lineNumber": 36
          },
          {
            "text": "**Requirements:**",
            "lineNumber": 37
          },
          {
            "text": "- &#96;requests&#96; library for HTTP downloads",
            "lineNumber": 38
          },
          {
            "text": "- &#96;zcat&#96; command (part of gzip utilities) for extracting incomplete gzip files",
            "lineNumber": 39
          },
          {
            "lineNumber": 40
          },
          {
            "text": "&lt;/details&gt;",
            "lineNumber": 41
          },
          {
            "lineNumber": 42
          },
          {
            "text": "## Hugging Face dataset creation",
            "lineNumber": 43
          },
          {
            "lineNumber": 44
          },
          {
            "text": "The &#96;create_hf_dataset.py&#96; script creates a Hugging Face dataset from DNA sequences stored in gzipped FASTA files and sample metadata. This dataset serves as the single source of truth for sample data across all downstream tasks, ensuring consistent train-test splits between traditional ML and deep learning workflows.",
            "lineNumber": 45
          },
          {
            "lineNumber": 46
          },
          {
            "text": "**Important**: After this step, &#96;sample_data.csv&#96; is no longer required for downstream tasks. The HuggingFace dataset contains all necessary sample metadata (pH, environment, domain, study_name, sample_id) and is used by both the traditional ML workflow (&#96;predict_ph.py&#96;) and the deep learning workflow (&#96;train_hyenadna_ph.py&#96;) to maintain consistent data splits.",
            "lineNumber": 47
          },
          {
            "lineNumber": 48
          },
          {
            "text": "&#96;&#96;&#96;bash",
            "lineNumber": 49
          },
          {
            "text": "# Requires HF_TOKEN in .env file",
            "lineNumber": 50
          },
          {
            "text": "python create_hf_dataset.py",
            "lineNumber": 51
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 52
          },
          {
            "lineNumber": 53
          },
          {
            "text": "&lt;details&gt;",
            "lineNumber": 54
          },
          {
            "text": "&lt;summary&gt;Details&lt;/summary&gt;",
            "lineNumber": 55
          },
          {
            "lineNumber": 56
          },
          {
            "text": "The script processes DNA sequences from FASTA files organized by study (&#96;data/fasta/{study_name}/{sample_id}.fasta.gz&#96;) and combines them with metadata from &#96;sample_data.csv&#96; to create a structured dataset suitable for machine learning workflows.",
            "lineNumber": 57
          },
          {
            "lineNumber": 58
          },
          {
            "text": "- **Automatic validation**: Ensures all FASTA files have corresponding entries in &#96;sample_data.csv&#96;",
            "lineNumber": 59
          },
          {
            "text": "- **Incremental updates**: Skips samples that already exist in the dataset, allowing for incremental additions",
            "lineNumber": 60
          },
          {
            "text": "- **Size management**: Limits total DNA length per sample to 0.25 MB to manage dataset size",
            "lineNumber": 61
          },
          {
            "text": "- **Metadata integration**: Includes sample metadata (pH, environment, domain) alongside sequences",
            "lineNumber": 62
          },
          {
            "text": "- **Consistent splits**: Used by both workflows to ensure identical train-test splits (same random seed, same test set)",
            "lineNumber": 63
          },
          {
            "lineNumber": 64
          },
          {
            "text": "&lt;/details&gt;",
            "lineNumber": 65
          },
          {
            "lineNumber": 66
          },
          {
            "text": "## RDP phylum counts (features)",
            "lineNumber": 67
          }
        ]
      },
      "score": 0.2883470058441162
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/inference.py",
        "range": {
          "startPosition": {
            "line": 13
          },
          "endPosition": {
            "line": 104,
            "column": 47
          }
        },
        "contents": "def inference_single():\n\n    print(\"Using device:\", device)\n\n    # instantiate the model (pretrained here)\n    if pretrained_model_name in [\n        \"hyenadna-tiny-1k-seqlen\",\n        \"hyenadna-small-32k-seqlen\",\n        \"hyenadna-medium-160k-seqlen\",\n        \"hyenadna-medium-450k-seqlen\",\n        \"hyenadna-large-1m-seqlen\",\n    ]:\n        # use the pretrained Huggingface wrapper instead\n        model = HyenaDNAPreTrainedModel.from_pretrained(\n            \"./checkpoints\",\n            pretrained_model_name,\n            # Change to True to download the checkpoint\n            download=False,\n            config=backbone_cfg,\n            device=device,\n            use_head=use_head,\n            n_classes=n_classes,\n        )\n\n    # from scratch\n    elif pretrained_model_name is None:\n        model = HyenaDNAModel(**backbone_cfg, use_head=use_head, n_classes=n_classes)\n\n    # create tokenizer\n    tokenizer = CharacterTokenizer(\n        characters=[\"A\", \"C\", \"G\", \"T\", \"N\"],  # add DNA characters, N is uncertain\n        model_max_length=max_length + 2,  # to account for special tokens, like EOS\n        # add_special_tokens argument is incompatible with newer tokenizers:\n        # AttributeError: add_special_tokens conflicts with the method add_special_tokens in CharacterTokenizer\n        # add_special_tokens=False,  # we handle special tokens elsewhere\n        padding_side=\"left\",  # since HyenaDNA is causal, we pad on the left\n    )\n\n    #### Single embedding example ####\n\n    # create a sample 450k long, prepare\n    sequence = \"ACTG\" * int(max_length / 4)\n    tok_seq = tokenizer(sequence)\n    tok_seq = tok_seq[\"input_ids\"]  # grab ids\n\n    # place on device, convert to tensor\n    tok_seq = torch.LongTensor(tok_seq).unsqueeze(0)  # unsqueeze for batch dim\n    tok_seq = tok_seq.to(device)\n\n    # prep model and forward\n    model.to(device)\n    model.eval()\n    with torch.inference_mode():\n        embeddings = model(tok_seq)\n\n    print(embeddings.shape)  # embeddings here!",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 14,
                "column": 1
              },
              "endPosition": {
                "line": 15,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def inference_single():",
            "lineNumber": 14,
            "isSignature": true
          },
          {
            "lineNumber": 51
          },
          {
            "text": "    print(\"Using device:\", device)",
            "lineNumber": 52
          },
          {
            "lineNumber": 53
          },
          {
            "text": "    # instantiate the model (pretrained here)",
            "lineNumber": 54
          },
          {
            "text": "    if pretrained_model_name in [",
            "lineNumber": 55
          },
          {
            "text": "        \"hyenadna-tiny-1k-seqlen\",",
            "lineNumber": 56
          },
          {
            "text": "        \"hyenadna-small-32k-seqlen\",",
            "lineNumber": 57
          },
          {
            "text": "        \"hyenadna-medium-160k-seqlen\",",
            "lineNumber": 58
          },
          {
            "text": "        \"hyenadna-medium-450k-seqlen\",",
            "lineNumber": 59
          },
          {
            "text": "        \"hyenadna-large-1m-seqlen\",",
            "lineNumber": 60
          },
          {
            "text": "    ]:",
            "lineNumber": 61
          },
          {
            "text": "        # use the pretrained Huggingface wrapper instead",
            "lineNumber": 62
          },
          {
            "text": "        model = HyenaDNAPreTrainedModel.from_pretrained(",
            "lineNumber": 63
          },
          {
            "text": "            \"./checkpoints\",",
            "lineNumber": 64
          },
          {
            "text": "            pretrained_model_name,",
            "lineNumber": 65
          },
          {
            "text": "            # Change to True to download the checkpoint",
            "lineNumber": 66
          },
          {
            "text": "            download=False,",
            "lineNumber": 67
          },
          {
            "text": "            config=backbone_cfg,",
            "lineNumber": 68
          },
          {
            "text": "            device=device,",
            "lineNumber": 69
          },
          {
            "text": "            use_head=use_head,",
            "lineNumber": 70
          },
          {
            "text": "            n_classes=n_classes,",
            "lineNumber": 71
          },
          {
            "text": "        )",
            "lineNumber": 72
          },
          {
            "lineNumber": 73
          },
          {
            "text": "    # from scratch",
            "lineNumber": 74
          },
          {
            "text": "    elif pretrained_model_name is None:",
            "lineNumber": 75
          },
          {
            "text": "        model = HyenaDNAModel(**backbone_cfg, use_head=use_head, n_classes=n_classes)",
            "lineNumber": 76
          },
          {
            "lineNumber": 77
          },
          {
            "text": "    # create tokenizer",
            "lineNumber": 78
          },
          {
            "text": "    tokenizer = CharacterTokenizer(",
            "lineNumber": 79
          },
          {
            "text": "        characters=[\"A\", \"C\", \"G\", \"T\", \"N\"],  # add DNA characters, N is uncertain",
            "lineNumber": 80
          },
          {
            "text": "        model_max_length=max_length + 2,  # to account for special tokens, like EOS",
            "lineNumber": 81
          },
          {
            "text": "        # add_special_tokens argument is incompatible with newer tokenizers:",
            "lineNumber": 82
          },
          {
            "text": "        # AttributeError: add_special_tokens conflicts with the method add_special_tokens in CharacterTokenizer",
            "lineNumber": 83
          },
          {
            "text": "        # add_special_tokens=False,  # we handle special tokens elsewhere",
            "lineNumber": 84
          },
          {
            "text": "        padding_side=\"left\",  # since HyenaDNA is causal, we pad on the left",
            "lineNumber": 85
          },
          {
            "text": "    )",
            "lineNumber": 86
          },
          {
            "lineNumber": 87
          },
          {
            "text": "    #### Single embedding example ####",
            "lineNumber": 88
          },
          {
            "lineNumber": 89
          },
          {
            "text": "    # create a sample 450k long, prepare",
            "lineNumber": 90
          },
          {
            "text": "    sequence = \"ACTG\" * int(max_length / 4)",
            "lineNumber": 91
          },
          {
            "text": "    tok_seq = tokenizer(sequence)",
            "lineNumber": 92
          },
          {
            "text": "    tok_seq = tok_seq[\"input_ids\"]  # grab ids",
            "lineNumber": 93
          },
          {
            "lineNumber": 94
          },
          {
            "text": "    # place on device, convert to tensor",
            "lineNumber": 95
          },
          {
            "text": "    tok_seq = torch.LongTensor(tok_seq).unsqueeze(0)  # unsqueeze for batch dim",
            "lineNumber": 96
          },
          {
            "text": "    tok_seq = tok_seq.to(device)",
            "lineNumber": 97
          },
          {
            "lineNumber": 98
          },
          {
            "text": "    # prep model and forward",
            "lineNumber": 99
          },
          {
            "text": "    model.to(device)",
            "lineNumber": 100
          },
          {
            "text": "    model.eval()",
            "lineNumber": 101
          },
          {
            "text": "    with torch.inference_mode():",
            "lineNumber": 102
          },
          {
            "text": "        embeddings = model(tok_seq)",
            "lineNumber": 103
          },
          {
            "lineNumber": 104
          },
          {
            "text": "    print(embeddings.shape)  # embeddings here!",
            "lineNumber": 105,
            "isSignature": true
          }
        ]
      },
      "score": 0.2835942506790161
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 575
          },
          "endPosition": {
            "line": 870
          }
        },
        "contents": "def main():\n\n    parser.add_argument(\n        \"--pooling-mode\",\n        type=str,\n        default=\"pool\",\n        choices=[\"pool\", \"last\", \"first\", \"sum\"],\n        help=\"Pooling mode for regression head\",\n    )\n    parser.add_argument(\n        \"--freeze-backbone\",\n        action=\"store_true\",\n        help=\"Freeze backbone weights (only train head)\",\n    )\n\n    # Training arguments\n    parser.add_argument(\"--batch-size\", type=int, default=4, help=\"Batch size\")\n    parser.add_argument(\n        \"--learning-rate\", type=float, default=1e-4, help=\"Learning rate\"\n    )\n    parser.add_argument(\"--num-epochs\", type=int, default=10, help=\"Number of epochs\")\n    parser.add_argument(\n        \"--loss-function\",\n        type=str,\n        default=\"smooth_l1\",\n        choices=[\"mse\", \"mae\", \"smooth_l1\", \"huber\"],\n        help=\"Loss function\",\n    )\n    parser.add_argument(\n        \"--weight-decay\", type=float, default=0.01, help=\"Weight decay for AdamW\"\n    )\n\n    # Data arguments\n    parser.add_argument(\n        \"--dataset-repo\",\n        type=str,\n        default=\"jedick/microbial-DNA-pH\",\n        help=\"HuggingFace dataset repository\",\n    )\n    parser.add_argument(\n        \"--random-seed\",\n        type=int,\n        default=42,\n        help=\"Random seed for reproducibility (default: 42)\",\n    )\n\n    # Output arguments\n    parser.add_argument(\n        \"--output-dir\",\n        type=str,\n        default=\"results/hyenadna_ph\",\n        help=\"Output directory for results\",\n    )\n    parser.add_argument(\n        \"--checkpoint-dir\",\n        type=str,\n        default=\"./checkpoints\",\n        help=\"Directory for model checkpoints\",\n    )\n    parser.add_argument(\n        \"--download-model\",\n        action=\"store_true\",\n        help=\"Download model from HuggingFace if not present\",\n    )\n\n    args = parser.parse_args()\n\n    # Set random seeds\n    torch.manual_seed(args.random_seed)",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 530,
                "column": 1
              },
              "endPosition": {
                "line": 531,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def main():",
            "lineNumber": 576,
            "isSignature": true
          },
          {
            "lineNumber": 593
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 594
          },
          {
            "text": "        \"--pooling-mode\",",
            "lineNumber": 595
          },
          {
            "text": "        type=str,",
            "lineNumber": 596
          },
          {
            "text": "        default=\"pool\",",
            "lineNumber": 597
          },
          {
            "text": "        choices=[\"pool\", \"last\", \"first\", \"sum\"],",
            "lineNumber": 598
          },
          {
            "text": "        help=\"Pooling mode for regression head\",",
            "lineNumber": 599
          },
          {
            "text": "    )",
            "lineNumber": 600
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 601
          },
          {
            "text": "        \"--freeze-backbone\",",
            "lineNumber": 602
          },
          {
            "text": "        action=\"store_true\",",
            "lineNumber": 603
          },
          {
            "text": "        help=\"Freeze backbone weights (only train head)\",",
            "lineNumber": 604
          },
          {
            "text": "    )",
            "lineNumber": 605
          },
          {
            "lineNumber": 606
          },
          {
            "text": "    # Training arguments",
            "lineNumber": 607
          },
          {
            "text": "    parser.add_argument(\"--batch-size\", type=int, default=4, help=\"Batch size\")",
            "lineNumber": 608
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 609
          },
          {
            "text": "        \"--learning-rate\", type=float, default=1e-4, help=\"Learning rate\"",
            "lineNumber": 610
          },
          {
            "text": "    )",
            "lineNumber": 611
          },
          {
            "text": "    parser.add_argument(\"--num-epochs\", type=int, default=10, help=\"Number of epochs\")",
            "lineNumber": 612
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 613
          },
          {
            "text": "        \"--loss-function\",",
            "lineNumber": 614
          },
          {
            "text": "        type=str,",
            "lineNumber": 615
          },
          {
            "text": "        default=\"smooth_l1\",",
            "lineNumber": 616
          },
          {
            "text": "        choices=[\"mse\", \"mae\", \"smooth_l1\", \"huber\"],",
            "lineNumber": 617
          },
          {
            "text": "        help=\"Loss function\",",
            "lineNumber": 618
          },
          {
            "text": "    )",
            "lineNumber": 619
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 620
          },
          {
            "text": "        \"--weight-decay\", type=float, default=0.01, help=\"Weight decay for AdamW\"",
            "lineNumber": 621
          },
          {
            "text": "    )",
            "lineNumber": 622
          },
          {
            "lineNumber": 623
          },
          {
            "text": "    # Data arguments",
            "lineNumber": 624
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 625
          },
          {
            "text": "        \"--dataset-repo\",",
            "lineNumber": 626
          },
          {
            "text": "        type=str,",
            "lineNumber": 627
          },
          {
            "text": "        default=\"jedick/microbial-DNA-pH\",",
            "lineNumber": 628
          },
          {
            "text": "        help=\"HuggingFace dataset repository\",",
            "lineNumber": 629
          },
          {
            "text": "    )",
            "lineNumber": 630
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 631
          },
          {
            "text": "        \"--random-seed\",",
            "lineNumber": 632
          },
          {
            "text": "        type=int,",
            "lineNumber": 633
          },
          {
            "text": "        default=42,",
            "lineNumber": 634
          },
          {
            "text": "        help=\"Random seed for reproducibility (default: 42)\",",
            "lineNumber": 635
          },
          {
            "text": "    )",
            "lineNumber": 636
          },
          {
            "lineNumber": 637
          },
          {
            "text": "    # Output arguments",
            "lineNumber": 638
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 639
          },
          {
            "text": "        \"--output-dir\",",
            "lineNumber": 640
          },
          {
            "text": "        type=str,",
            "lineNumber": 641
          },
          {
            "text": "        default=\"results/hyenadna_ph\",",
            "lineNumber": 642
          },
          {
            "text": "        help=\"Output directory for results\",",
            "lineNumber": 643
          },
          {
            "text": "    )",
            "lineNumber": 644
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 645
          },
          {
            "text": "        \"--checkpoint-dir\",",
            "lineNumber": 646
          },
          {
            "text": "        type=str,",
            "lineNumber": 647
          },
          {
            "text": "        default=\"./checkpoints\",",
            "lineNumber": 648
          },
          {
            "text": "        help=\"Directory for model checkpoints\",",
            "lineNumber": 649
          },
          {
            "text": "    )",
            "lineNumber": 650
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 651
          },
          {
            "text": "        \"--download-model\",",
            "lineNumber": 652
          },
          {
            "text": "        action=\"store_true\",",
            "lineNumber": 653
          },
          {
            "text": "        help=\"Download model from HuggingFace if not present\",",
            "lineNumber": 654
          },
          {
            "text": "    )",
            "lineNumber": 655
          },
          {
            "lineNumber": 656
          },
          {
            "text": "    args = parser.parse_args()",
            "lineNumber": 657
          },
          {
            "lineNumber": 658
          },
          {
            "text": "    # Set random seeds",
            "lineNumber": 659
          },
          {
            "text": "    torch.manual_seed(args.random_seed)",
            "lineNumber": 660
          }
        ]
      },
      "score": 0.27665770053863525
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/standalone_hyenadna.py",
        "range": {
          "startPosition": {
            "line": 921
          },
          "endPosition": {
            "line": 1027
          }
        },
        "contents": "class SequenceDecoder(nn.Module):\n\n\n    def forward(self, x, state=None, lengths=None, l_output=None):\n        \"\"\"\n        x: (n_batch, l_seq, d_model)\n        Returns: (n_batch, l_output, d_output)\n        \"\"\"\n\n        if self.l_output is None:\n            if l_output is not None:\n                assert isinstance(l_output, int)  # Override by pass in\n            else:\n                # Grab entire output\n                l_output = x.size(-2)\n            squeeze = False\n        else:\n            l_output = self.l_output\n            squeeze = self.squeeze\n\n        if self.mode == \"last\":\n            restrict = lambda x: x[..., -l_output:, :]\n        elif self.mode == \"first\":\n            restrict = lambda x: x[..., :l_output, :]\n        elif self.mode == \"pool\":\n            restrict = lambda x: (\n                torch.cumsum(x, dim=-2)\n                / torch.arange(\n                    1, 1 + x.size(-2), device=x.device, dtype=x.dtype\n                ).unsqueeze(-1)\n            )[..., -l_output:, :]\n\n            def restrict(x):\n                L = x.size(-2)\n                s = x.sum(dim=-2, keepdim=True)\n                if l_output &gt; 1:\n                    c = torch.cumsum(x[..., -(l_output - 1) :, :].flip(-2), dim=-2)\n                    c = F.pad(c, (0, 0, 1, 0))\n                    s = s - c  # (B, l_output, D)\n                    s = s.flip(-2)\n                denom = torch.arange(\n                    L - l_output + 1, L + 1, dtype=x.dtype, device=x.device\n                )\n                s = s / denom\n                return s\n\n        elif self.mode == \"sum\":\n            restrict = lambda x: torch.cumsum(x, dim=-2)[..., -l_output:, :]\n            # TODO use same restrict function as pool case\n        elif self.mode == \"ragged\":\n            assert lengths is not None, \"lengths must be provided for ragged mode\"\n            # remove any additional padding (beyond max length of any sequence in the batch)\n            restrict = lambda x: x[..., : max(lengths), :]\n        else:\n            raise NotImplementedError",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 922,
                "column": 1
              },
              "endPosition": {
                "line": 923,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class SequenceDecoder(nn.Module):",
            "lineNumber": 922,
            "isSignature": true
          },
          {
            "lineNumber": 948
          },
          {
            "lineNumber": 949
          },
          {
            "text": "    def forward(self, x, state=None, lengths=None, l_output=None):",
            "lineNumber": 950
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 951
          },
          {
            "text": "        x: (n_batch, l_seq, d_model)",
            "lineNumber": 952
          },
          {
            "text": "        Returns: (n_batch, l_output, d_output)",
            "lineNumber": 953
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 954
          },
          {
            "lineNumber": 955
          },
          {
            "text": "        if self.l_output is None:",
            "lineNumber": 956
          },
          {
            "text": "            if l_output is not None:",
            "lineNumber": 957
          },
          {
            "text": "                assert isinstance(l_output, int)  # Override by pass in",
            "lineNumber": 958
          },
          {
            "text": "            else:",
            "lineNumber": 959
          },
          {
            "text": "                # Grab entire output",
            "lineNumber": 960
          },
          {
            "text": "                l_output = x.size(-2)",
            "lineNumber": 961
          },
          {
            "text": "            squeeze = False",
            "lineNumber": 962
          },
          {
            "text": "        else:",
            "lineNumber": 963
          },
          {
            "text": "            l_output = self.l_output",
            "lineNumber": 964
          },
          {
            "text": "            squeeze = self.squeeze",
            "lineNumber": 965
          },
          {
            "lineNumber": 966
          },
          {
            "text": "        if self.mode == \"last\":",
            "lineNumber": 967
          },
          {
            "text": "            restrict = lambda x: x[..., -l_output:, :]",
            "lineNumber": 968
          },
          {
            "text": "        elif self.mode == \"first\":",
            "lineNumber": 969
          },
          {
            "text": "            restrict = lambda x: x[..., :l_output, :]",
            "lineNumber": 970
          },
          {
            "text": "        elif self.mode == \"pool\":",
            "lineNumber": 971
          },
          {
            "text": "            restrict = lambda x: (",
            "lineNumber": 972
          },
          {
            "text": "                torch.cumsum(x, dim=-2)",
            "lineNumber": 973
          },
          {
            "text": "                / torch.arange(",
            "lineNumber": 974
          },
          {
            "text": "                    1, 1 + x.size(-2), device=x.device, dtype=x.dtype",
            "lineNumber": 975
          },
          {
            "text": "                ).unsqueeze(-1)",
            "lineNumber": 976
          },
          {
            "text": "            )[..., -l_output:, :]",
            "lineNumber": 977
          },
          {
            "lineNumber": 978
          },
          {
            "text": "            def restrict(x):",
            "lineNumber": 979
          },
          {
            "text": "                L = x.size(-2)",
            "lineNumber": 980
          },
          {
            "text": "                s = x.sum(dim=-2, keepdim=True)",
            "lineNumber": 981
          },
          {
            "text": "                if l_output &gt; 1:",
            "lineNumber": 982
          },
          {
            "text": "                    c = torch.cumsum(x[..., -(l_output - 1) :, :].flip(-2), dim=-2)",
            "lineNumber": 983
          },
          {
            "text": "                    c = F.pad(c, (0, 0, 1, 0))",
            "lineNumber": 984
          },
          {
            "text": "                    s = s - c  # (B, l_output, D)",
            "lineNumber": 985
          },
          {
            "text": "                    s = s.flip(-2)",
            "lineNumber": 986
          },
          {
            "text": "                denom = torch.arange(",
            "lineNumber": 987
          },
          {
            "text": "                    L - l_output + 1, L + 1, dtype=x.dtype, device=x.device",
            "lineNumber": 988
          },
          {
            "text": "                )",
            "lineNumber": 989
          },
          {
            "text": "                s = s / denom",
            "lineNumber": 990
          },
          {
            "text": "                return s",
            "lineNumber": 991
          },
          {
            "lineNumber": 992
          },
          {
            "text": "        elif self.mode == \"sum\":",
            "lineNumber": 993
          },
          {
            "text": "            restrict = lambda x: torch.cumsum(x, dim=-2)[..., -l_output:, :]",
            "lineNumber": 994
          },
          {
            "text": "            # TODO use same restrict function as pool case",
            "lineNumber": 995
          },
          {
            "text": "        elif self.mode == \"ragged\":",
            "lineNumber": 996
          },
          {
            "text": "            assert lengths is not None, \"lengths must be provided for ragged mode\"",
            "lineNumber": 997
          },
          {
            "text": "            # remove any additional padding (beyond max length of any sequence in the batch)",
            "lineNumber": 998
          },
          {
            "text": "            restrict = lambda x: x[..., : max(lengths), :]",
            "lineNumber": 999
          },
          {
            "text": "        else:",
            "lineNumber": 1000
          },
          {
            "text": "            raise NotImplementedError",
            "lineNumber": 1001
          }
        ]
      },
      "score": 0.2754163146018982
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./README.md",
        "range": {
          "startPosition": {
            "line": 154
          },
          "endPosition": {
            "line": 182
          }
        },
        "contents": "python train_hyenadna_ph.py --freeze-backbone --download-model\n&#96;&#96;&#96;\n\n&lt;details&gt;\n&lt;summary&gt;Details&lt;/summary&gt;\n\nThe script implements a separate &#96;RegressionHead&#96; class that maintains separation from the HyenaDNA codebase. It loads DNA sequences from the HuggingFace dataset (&#96;jedick/microbial-DNA-pH&#96;), concatenates multiple sequences per sample with [SEP] tokens up to the model's maximum length, and fine-tunes pretrained HyenaDNA weights for pH prediction.\n\n- **Model options**: Supports all HyenaDNA model sizes (tiny-1k to large-1m-seqlen), default is tiny-1k-seqlen for testing\n- **Regression head architectures**: Linear, 2-layer MLP (default), 3-layer MLP, or 2-layer MLP with LayerNorm\n- **Pooling modes**: Mean pooling (default), last token, first token, or sum pooling\n- **Sequence handling**: Concatenates multiple DNA sequences per sample with [SEP] tokens, truncates to max_length if needed\n- **Training options**: Can freeze backbone (only train head) or fine-tune entire model, configurable loss functions (MSE, MAE, SmoothL1, Huber)\n- **Evaluation**: Computes MSE, RMSE, MAE, and R² metrics on train/val/test splits\n- **Data split**: Creates a 75:5:20 train-val-test split using the same random seed (42) and test set as the traditional ML workflow\n\n**Output Files:**\n- &#96;best_model.pt&#96;: Best model checkpoint based on validation loss\n- &#96;training_log.json&#96;: Training history with metrics per epoch\n- &#96;test_predictions.csv&#96;: Test set predictions with true and predicted pH values\n- &#96;config.json&#96;: Training configuration used\n\n**Key Hyperparameters:**\n- Default batch size: 4 (adjust based on GPU memory and sequence length)\n- Default learning rate: 1e-4 (typical for fine-tuning)\n- Default loss: SmoothL1Loss (good balance between MSE and MAE)\n- Train/val/test split: 75:5:20 (same test set as traditional ML workflow)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "python train_hyenadna_ph.py --freeze-backbone --download-model",
            "lineNumber": 155
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 156
          },
          {
            "lineNumber": 157
          },
          {
            "text": "&lt;details&gt;",
            "lineNumber": 158
          },
          {
            "text": "&lt;summary&gt;Details&lt;/summary&gt;",
            "lineNumber": 159
          },
          {
            "lineNumber": 160
          },
          {
            "text": "The script implements a separate &#96;RegressionHead&#96; class that maintains separation from the HyenaDNA codebase. It loads DNA sequences from the HuggingFace dataset (&#96;jedick/microbial-DNA-pH&#96;), concatenates multiple sequences per sample with [SEP] tokens up to the model's maximum length, and fine-tunes pretrained HyenaDNA weights for pH prediction.",
            "lineNumber": 161
          },
          {
            "lineNumber": 162
          },
          {
            "text": "- **Model options**: Supports all HyenaDNA model sizes (tiny-1k to large-1m-seqlen), default is tiny-1k-seqlen for testing",
            "lineNumber": 163
          },
          {
            "text": "- **Regression head architectures**: Linear, 2-layer MLP (default), 3-layer MLP, or 2-layer MLP with LayerNorm",
            "lineNumber": 164
          },
          {
            "text": "- **Pooling modes**: Mean pooling (default), last token, first token, or sum pooling",
            "lineNumber": 165
          },
          {
            "text": "- **Sequence handling**: Concatenates multiple DNA sequences per sample with [SEP] tokens, truncates to max_length if needed",
            "lineNumber": 166
          },
          {
            "text": "- **Training options**: Can freeze backbone (only train head) or fine-tune entire model, configurable loss functions (MSE, MAE, SmoothL1, Huber)",
            "lineNumber": 167
          },
          {
            "text": "- **Evaluation**: Computes MSE, RMSE, MAE, and R² metrics on train/val/test splits",
            "lineNumber": 168
          },
          {
            "text": "- **Data split**: Creates a 75:5:20 train-val-test split using the same random seed (42) and test set as the traditional ML workflow",
            "lineNumber": 169
          },
          {
            "lineNumber": 170
          },
          {
            "text": "**Output Files:**",
            "lineNumber": 171
          },
          {
            "text": "- &#96;best_model.pt&#96;: Best model checkpoint based on validation loss",
            "lineNumber": 172
          },
          {
            "text": "- &#96;training_log.json&#96;: Training history with metrics per epoch",
            "lineNumber": 173
          },
          {
            "text": "- &#96;test_predictions.csv&#96;: Test set predictions with true and predicted pH values",
            "lineNumber": 174
          },
          {
            "text": "- &#96;config.json&#96;: Training configuration used",
            "lineNumber": 175
          },
          {
            "lineNumber": 176
          },
          {
            "text": "**Key Hyperparameters:**",
            "lineNumber": 177
          },
          {
            "text": "- Default batch size: 4 (adjust based on GPU memory and sequence length)",
            "lineNumber": 178
          },
          {
            "text": "- Default learning rate: 1e-4 (typical for fine-tuning)",
            "lineNumber": 179
          },
          {
            "text": "- Default loss: SmoothL1Loss (good balance between MSE and MAE)",
            "lineNumber": 180
          },
          {
            "text": "- Train/val/test split: 75:5:20 (same test set as traditional ML workflow)",
            "lineNumber": 181
          }
        ]
      },
      "score": 0.2662034034729004
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./predict_ph.py",
        "range": {
          "startPosition": {
            "line": 31,
            "column": 85
          },
          "endPosition": {
            "line": 149
          }
        },
        "contents": "\"\n    )\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef load_and_preprocess_data(dataset_repo, phylum_counts_path, random_seed=42):\n    \"\"\"\n    Load and preprocess data for pH prediction.\n\n    Parameters\n    ----------\n    dataset_repo : str\n        HuggingFace dataset repository (e.g., \"jedick/microbial-DNA-pH\")\n    phylum_counts_path : str\n        Path to bacteria_phylum_counts.csv.xz (compressed)\n    random_seed : int\n        Random seed for reproducibility (default: 42)\n\n    Returns\n    -------\n    tuple\n        (X_train, X_test, y_train, y_test, feature_names, test_metadata)\n        where test_metadata is a DataFrame with study_name and sample_id for test set\n    \"\"\"\n    # Load sample data from HF dataset\n    print(f\"Loading dataset from {dataset_repo}...\")\n    dataset = load_dataset(dataset_repo, split=\"train\")\n\n    # Convert to DataFrame and filter out null pH values\n    print(\"Filtering data...\")\n    sample_data_list = []\n    for item in dataset:\n        if item[\"pH\"] is not None:  # Filter out null pH values\n            sample_data_list.append(\n                {\n                    \"study_name\": item[\"study_name\"],\n                    \"sample_id\": item[\"sample_id\"],\n                    \"pH\": item[\"pH\"],\n                    \"environment\": item.get(\"environment\", \"unknown\"),\n                }\n            )\n\n    sample_data = pd.DataFrame(sample_data_list)\n    # Sort by sample_id to ensure consistent ordering (same as train_hyenadna_ph.py)\n    sample_data = sample_data.sort_values(\"sample_id\").reset_index(drop=True)\n    print(f\"Sample data shape after filtering (null pH removed): {sample_data.shape}\")\n\n    # Create stratified 80:20 train-test split on HF dataset (before merging with phylum counts)\n    # This ensures the same test split as train_hyenadna_ph.py\n    print(\"Creating stratified train-test split (80:20)...\")\n    sample_ids = sample_data[\"sample_id\"].values\n    environment = sample_data[\"environment\"].values\n\n    ids_train, ids_test = train_test_split(\n        sample_ids.tolist(),\n        test_size=0.2,\n        random_state=random_seed,\n        stratify=environment,\n    )\n\n    # Convert to sets for easier filtering\n    ids_train_set = set(ids_train)\n    ids_test_set = set(ids_test)\n\n    # Load phylum counts (pandas can read .xz compressed files directly)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "\"",
            "lineNumber": 32
          },
          {
            "text": "    )",
            "lineNumber": 33
          },
          {
            "lineNumber": 34
          },
          {
            "text": "warnings.filterwarnings(\"ignore\")",
            "lineNumber": 35
          },
          {
            "lineNumber": 36
          },
          {
            "lineNumber": 37
          },
          {
            "text": "def load_and_preprocess_data(dataset_repo, phylum_counts_path, random_seed=42):",
            "lineNumber": 38,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 39
          },
          {
            "text": "    Load and preprocess data for pH prediction.",
            "lineNumber": 40
          },
          {
            "lineNumber": 41
          },
          {
            "text": "    Parameters",
            "lineNumber": 42
          },
          {
            "text": "    ----------",
            "lineNumber": 43
          },
          {
            "text": "    dataset_repo : str",
            "lineNumber": 44
          },
          {
            "text": "        HuggingFace dataset repository (e.g., \"jedick/microbial-DNA-pH\")",
            "lineNumber": 45
          },
          {
            "text": "    phylum_counts_path : str",
            "lineNumber": 46
          },
          {
            "text": "        Path to bacteria_phylum_counts.csv.xz (compressed)",
            "lineNumber": 47
          },
          {
            "text": "    random_seed : int",
            "lineNumber": 48
          },
          {
            "text": "        Random seed for reproducibility (default: 42)",
            "lineNumber": 49
          },
          {
            "lineNumber": 50
          },
          {
            "text": "    Returns",
            "lineNumber": 51
          },
          {
            "text": "    -------",
            "lineNumber": 52
          },
          {
            "text": "    tuple",
            "lineNumber": 53
          },
          {
            "text": "        (X_train, X_test, y_train, y_test, feature_names, test_metadata)",
            "lineNumber": 54
          },
          {
            "text": "        where test_metadata is a DataFrame with study_name and sample_id for test set",
            "lineNumber": 55
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 56
          },
          {
            "text": "    # Load sample data from HF dataset",
            "lineNumber": 57
          },
          {
            "text": "    print(f\"Loading dataset from {dataset_repo}...\")",
            "lineNumber": 58
          },
          {
            "text": "    dataset = load_dataset(dataset_repo, split=\"train\")",
            "lineNumber": 59
          },
          {
            "lineNumber": 60
          },
          {
            "text": "    # Convert to DataFrame and filter out null pH values",
            "lineNumber": 61
          },
          {
            "text": "    print(\"Filtering data...\")",
            "lineNumber": 62
          },
          {
            "text": "    sample_data_list = []",
            "lineNumber": 63
          },
          {
            "text": "    for item in dataset:",
            "lineNumber": 64
          },
          {
            "text": "        if item[\"pH\"] is not None:  # Filter out null pH values",
            "lineNumber": 65
          },
          {
            "text": "            sample_data_list.append(",
            "lineNumber": 66
          },
          {
            "text": "                {",
            "lineNumber": 67
          },
          {
            "text": "                    \"study_name\": item[\"study_name\"],",
            "lineNumber": 68
          },
          {
            "text": "                    \"sample_id\": item[\"sample_id\"],",
            "lineNumber": 69
          },
          {
            "text": "                    \"pH\": item[\"pH\"],",
            "lineNumber": 70
          },
          {
            "text": "                    \"environment\": item.get(\"environment\", \"unknown\"),",
            "lineNumber": 71
          },
          {
            "text": "                }",
            "lineNumber": 72
          },
          {
            "text": "            )",
            "lineNumber": 73
          },
          {
            "lineNumber": 74
          },
          {
            "text": "    sample_data = pd.DataFrame(sample_data_list)",
            "lineNumber": 75
          },
          {
            "text": "    # Sort by sample_id to ensure consistent ordering (same as train_hyenadna_ph.py)",
            "lineNumber": 76
          },
          {
            "text": "    sample_data = sample_data.sort_values(\"sample_id\").reset_index(drop=True)",
            "lineNumber": 77
          },
          {
            "text": "    print(f\"Sample data shape after filtering (null pH removed): {sample_data.shape}\")",
            "lineNumber": 78
          },
          {
            "lineNumber": 79
          },
          {
            "text": "    # Create stratified 80:20 train-test split on HF dataset (before merging with phylum counts)",
            "lineNumber": 80
          },
          {
            "text": "    # This ensures the same test split as train_hyenadna_ph.py",
            "lineNumber": 81
          },
          {
            "text": "    print(\"Creating stratified train-test split (80:20)...\")",
            "lineNumber": 82
          },
          {
            "text": "    sample_ids = sample_data[\"sample_id\"].values",
            "lineNumber": 83
          },
          {
            "text": "    environment = sample_data[\"environment\"].values",
            "lineNumber": 84
          },
          {
            "lineNumber": 85
          },
          {
            "text": "    ids_train, ids_test = train_test_split(",
            "lineNumber": 86
          },
          {
            "text": "        sample_ids.tolist(),",
            "lineNumber": 87
          },
          {
            "text": "        test_size=0.2,",
            "lineNumber": 88
          },
          {
            "text": "        random_state=random_seed,",
            "lineNumber": 89
          },
          {
            "text": "        stratify=environment,",
            "lineNumber": 90
          },
          {
            "text": "    )",
            "lineNumber": 91
          },
          {
            "lineNumber": 92
          },
          {
            "text": "    # Convert to sets for easier filtering",
            "lineNumber": 93
          },
          {
            "text": "    ids_train_set = set(ids_train)",
            "lineNumber": 94
          },
          {
            "text": "    ids_test_set = set(ids_test)",
            "lineNumber": 95
          },
          {
            "lineNumber": 96
          },
          {
            "text": "    # Load phylum counts (pandas can read .xz compressed files directly)",
            "lineNumber": 97
          }
        ]
      },
      "score": 0.26458489894866943
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 575
          },
          "endPosition": {
            "line": 870
          }
        },
        "contents": "def main():\n\n\n    args = parser.parse_args()\n\n    # Set random seeds\n    torch.manual_seed(args.random_seed)\n    np.random.seed(args.random_seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(args.random_seed)\n\n    # Setup device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n\n    # Create output directory\n    output_dir = Path(args.output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Save configuration\n    config_path = output_dir / \"config.json\"\n    with open(config_path, \"w\") as f:\n        json.dump(vars(args), f, indent=2)\n    print(f\"Saved configuration to {config_path}\")\n\n    # Load dataset (HF has SRA samples only; includes study_name, sample_id)\n    data = load_dataset_from_hf(args.dataset_repo, filter_missing_ph=True)\n\n    # Create stratified 75:5:20 split (same test set as traditional ML)\n    # First split 80:20 (train+val : test) with same random seed\n    # Then split 80% into 75% train and 5% val\n    print(\"Creating stratified train-val-test split (75:5:20)...\")\n\n    # Extract sample_ids, pH, and environment for stratification\n    # Data is already sorted by sample_id from load_dataset_from_hf\n    sample_ids = [d[\"sample_id\"] for d in data]\n    ph_values = [d[\"pH\"] for d in data]\n    environments = [d[\"environment\"] for d in data]\n\n    # First split: 80% train+val, 20% test (same as traditional ML)\n    ids_trainval, ids_test, data_trainval, data_test, env_trainval, env_test = (\n        train_test_split(\n            sample_ids,\n            data,\n            environments,\n            test_size=0.2,\n            random_state=args.random_seed,\n            stratify=environments,\n        )\n    )\n\n    # Second split: split train+val into 75% train and 5% val\n    # val_size = 5% of total = 5/80 = 0.0625 of trainval\n    n_total = len(data)\n    n_val = int(round(n_total * 0.05))\n    val_size = n_val / len(data_trainval)\n\n    ids_train, ids_val, data_train, data_val =",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 530,
                "column": 1
              },
              "endPosition": {
                "line": 531,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def main():",
            "lineNumber": 576,
            "isSignature": true
          },
          {
            "lineNumber": 655
          },
          {
            "lineNumber": 656
          },
          {
            "text": "    args = parser.parse_args()",
            "lineNumber": 657
          },
          {
            "lineNumber": 658
          },
          {
            "text": "    # Set random seeds",
            "lineNumber": 659
          },
          {
            "text": "    torch.manual_seed(args.random_seed)",
            "lineNumber": 660
          },
          {
            "text": "    np.random.seed(args.random_seed)",
            "lineNumber": 661
          },
          {
            "text": "    if torch.cuda.is_available():",
            "lineNumber": 662
          },
          {
            "text": "        torch.cuda.manual_seed_all(args.random_seed)",
            "lineNumber": 663
          },
          {
            "lineNumber": 664
          },
          {
            "text": "    # Setup device",
            "lineNumber": 665
          },
          {
            "text": "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
            "lineNumber": 666
          },
          {
            "text": "    print(f\"Using device: {device}\")",
            "lineNumber": 667
          },
          {
            "lineNumber": 668
          },
          {
            "text": "    # Create output directory",
            "lineNumber": 669
          },
          {
            "text": "    output_dir = Path(args.output_dir)",
            "lineNumber": 670
          },
          {
            "text": "    output_dir.mkdir(parents=True, exist_ok=True)",
            "lineNumber": 671
          },
          {
            "lineNumber": 672
          },
          {
            "text": "    # Save configuration",
            "lineNumber": 673
          },
          {
            "text": "    config_path = output_dir / \"config.json\"",
            "lineNumber": 674
          },
          {
            "text": "    with open(config_path, \"w\") as f:",
            "lineNumber": 675
          },
          {
            "text": "        json.dump(vars(args), f, indent=2)",
            "lineNumber": 676
          },
          {
            "text": "    print(f\"Saved configuration to {config_path}\")",
            "lineNumber": 677
          },
          {
            "lineNumber": 678
          },
          {
            "text": "    # Load dataset (HF has SRA samples only; includes study_name, sample_id)",
            "lineNumber": 679
          },
          {
            "text": "    data = load_dataset_from_hf(args.dataset_repo, filter_missing_ph=True)",
            "lineNumber": 680
          },
          {
            "lineNumber": 681
          },
          {
            "text": "    # Create stratified 75:5:20 split (same test set as traditional ML)",
            "lineNumber": 682
          },
          {
            "text": "    # First split 80:20 (train+val : test) with same random seed",
            "lineNumber": 683
          },
          {
            "text": "    # Then split 80% into 75% train and 5% val",
            "lineNumber": 684
          },
          {
            "text": "    print(\"Creating stratified train-val-test split (75:5:20)...\")",
            "lineNumber": 685
          },
          {
            "lineNumber": 686
          },
          {
            "text": "    # Extract sample_ids, pH, and environment for stratification",
            "lineNumber": 687
          },
          {
            "text": "    # Data is already sorted by sample_id from load_dataset_from_hf",
            "lineNumber": 688
          },
          {
            "text": "    sample_ids = [d[\"sample_id\"] for d in data]",
            "lineNumber": 689
          },
          {
            "text": "    ph_values = [d[\"pH\"] for d in data]",
            "lineNumber": 690
          },
          {
            "text": "    environments = [d[\"environment\"] for d in data]",
            "lineNumber": 691
          },
          {
            "lineNumber": 692
          },
          {
            "text": "    # First split: 80% train+val, 20% test (same as traditional ML)",
            "lineNumber": 693
          },
          {
            "text": "    ids_trainval, ids_test, data_trainval, data_test, env_trainval, env_test = (",
            "lineNumber": 694
          },
          {
            "text": "        train_test_split(",
            "lineNumber": 695
          },
          {
            "text": "            sample_ids,",
            "lineNumber": 696
          },
          {
            "text": "            data,",
            "lineNumber": 697
          },
          {
            "text": "            environments,",
            "lineNumber": 698
          },
          {
            "text": "            test_size=0.2,",
            "lineNumber": 699
          },
          {
            "text": "            random_state=args.random_seed,",
            "lineNumber": 700
          },
          {
            "text": "            stratify=environments,",
            "lineNumber": 701
          },
          {
            "text": "        )",
            "lineNumber": 702
          },
          {
            "text": "    )",
            "lineNumber": 703
          },
          {
            "lineNumber": 704
          },
          {
            "text": "    # Second split: split train+val into 75% train and 5% val",
            "lineNumber": 705
          },
          {
            "text": "    # val_size = 5% of total = 5/80 = 0.0625 of trainval",
            "lineNumber": 706
          },
          {
            "text": "    n_total = len(data)",
            "lineNumber": 707
          },
          {
            "text": "    n_val = int(round(n_total * 0.05))",
            "lineNumber": 708
          },
          {
            "text": "    val_size = n_val / len(data_trainval)",
            "lineNumber": 709
          },
          {
            "lineNumber": 710
          },
          {
            "text": "    ids_train, ids_val, data_train, data_val =",
            "lineNumber": 711
          }
        ]
      },
      "score": 0.2576620578765869
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./.cursor/plans/hyenadna_ph_regression_head_0c97fe04.plan.md",
        "range": {
          "startPosition": {
            "line": 41
          },
          "endPosition": {
            "line": 119
          }
        },
        "contents": "- Uses &#96;SequenceDecoder&#96; for classification (we will NOT modify this)\n- &#96;CharacterTokenizer&#96; supports [SEP] token (ID=1) - **confirmed feasible for concatenation**\n- Model uses causal attention with left padding\n- Pretrained weights available on HuggingFace\n\nThe dataset structure (from [&#96;create_hf_dataset.py&#96;](create_hf_dataset.py)):\n\n- DNA sequences stored as lists of strings per sample\n- pH values are continuous floats (can be &#96;None&#96; for missing data)\n- Each sample has multiple sequences (up to 1MB total DNA length)\n- Available on HuggingFace: &#96;jedick/microbial-DNA-pH&#96;\n\n## Architecture Design\n\n### RegressionHead Class\n\nCreate a new &#96;RegressionHead&#96; class that:\n\n- Takes hidden states from HyenaDNA backbone (shape: &#96;(batch, seq_len, d_model)&#96;)\n- Applies pooling to get sequence-level representation (similar to &#96;SequenceDecoder&#96;)\n- Maps pooled features to single pH value via MLP\n- Supports multiple architecture options\n\n**Architecture Options**:\n\n1. **&#96;linear&#96;**: Simple linear layer\n   &#96;&#96;&#96;python\n   nn.Linear(d_model, 1)\n   &#96;&#96;&#96;\n\n2. **&#96;mlp2&#96;** (Default): 2-layer MLP\n   &#96;&#96;&#96;python\n   nn.Sequential(\n       nn.Linear(d_model, d_model // 2),\n       nn.ReLU(),\n       nn.Dropout(0.1),\n       nn.Linear(d_model // 2, 1)\n   )\n   &#96;&#96;&#96;\n\n3. **&#96;mlp3&#96;**: 3-layer MLP\n   &#96;&#96;&#96;python\n   nn.Sequential(\n       nn.Linear(d_model, d_model),\n       nn.ReLU(),\n       nn.Dropout(0.1),\n       nn.Linear(d_model, d_model // 2),\n       nn.ReLU(),\n       nn.Dropout(0.1),\n       nn.Linear(d_model // 2, 1)\n   )\n   &#96;&#96;&#96;\n\n4. **&#96;mlp2_ln&#96;**: 2-layer MLP with LayerNorm\n   &#96;&#96;&#96;python\n   nn.Sequential(\n       nn.LayerNorm(d_model),\n       nn.Linear(d_model, d_model // 2),\n       nn.ReLU(),\n       nn.Dropout(0.1),\n       nn.Linear(d_model // 2, 1)\n   )\n   &#96;&#96;&#96;\n\n\n**Pooling Modes** (reuse logic from &#96;SequenceDecoder&#96;):\n\n- &#96;pool&#96;: Mean pooling (default) - works well for variable-length sequences\n- &#96;last&#96;: Use last token representation\n- &#96;first&#96;: Use first token representation\n- &#96;sum&#96;: Sum all tokens\n\n**Output**: Single scalar pH value per sample &#96;(batch,)&#96; or &#96;(batch, 1)&#96;\n\n### Model Integration Strategy\n\nSince we're keeping separation from HyenaDNA codebase:",
        "signatures": {},
        "detailedLines": [
          {
            "text": "- Uses &#96;SequenceDecoder&#96; for classification (we will NOT modify this)",
            "lineNumber": 43
          },
          {
            "text": "- &#96;CharacterTokenizer&#96; supports [SEP] token (ID=1) - **confirmed feasible for concatenation**",
            "lineNumber": 44
          },
          {
            "text": "- Model uses causal attention with left padding",
            "lineNumber": 45
          },
          {
            "text": "- Pretrained weights available on HuggingFace",
            "lineNumber": 46
          },
          {
            "lineNumber": 47
          },
          {
            "text": "The dataset structure (from [&#96;create_hf_dataset.py&#96;](create_hf_dataset.py)):",
            "lineNumber": 48
          },
          {
            "lineNumber": 49
          },
          {
            "text": "- DNA sequences stored as lists of strings per sample",
            "lineNumber": 50
          },
          {
            "text": "- pH values are continuous floats (can be &#96;None&#96; for missing data)",
            "lineNumber": 51
          },
          {
            "text": "- Each sample has multiple sequences (up to 1MB total DNA length)",
            "lineNumber": 52
          },
          {
            "text": "- Available on HuggingFace: &#96;jedick/microbial-DNA-pH&#96;",
            "lineNumber": 53
          },
          {
            "lineNumber": 54
          },
          {
            "text": "## Architecture Design",
            "lineNumber": 55
          },
          {
            "lineNumber": 56
          },
          {
            "text": "### RegressionHead Class",
            "lineNumber": 57
          },
          {
            "lineNumber": 58
          },
          {
            "text": "Create a new &#96;RegressionHead&#96; class that:",
            "lineNumber": 59
          },
          {
            "lineNumber": 60
          },
          {
            "text": "- Takes hidden states from HyenaDNA backbone (shape: &#96;(batch, seq_len, d_model)&#96;)",
            "lineNumber": 61
          },
          {
            "text": "- Applies pooling to get sequence-level representation (similar to &#96;SequenceDecoder&#96;)",
            "lineNumber": 62
          },
          {
            "text": "- Maps pooled features to single pH value via MLP",
            "lineNumber": 63
          },
          {
            "text": "- Supports multiple architecture options",
            "lineNumber": 64
          },
          {
            "lineNumber": 65
          },
          {
            "text": "**Architecture Options**:",
            "lineNumber": 66
          },
          {
            "lineNumber": 67
          },
          {
            "text": "1. **&#96;linear&#96;**: Simple linear layer",
            "lineNumber": 68
          },
          {
            "text": "   &#96;&#96;&#96;python",
            "lineNumber": 69
          },
          {
            "text": "   nn.Linear(d_model, 1)",
            "lineNumber": 70
          },
          {
            "text": "   &#96;&#96;&#96;",
            "lineNumber": 71
          },
          {
            "lineNumber": 72
          },
          {
            "text": "2. **&#96;mlp2&#96;** (Default): 2-layer MLP",
            "lineNumber": 73
          },
          {
            "text": "   &#96;&#96;&#96;python",
            "lineNumber": 74
          },
          {
            "text": "   nn.Sequential(",
            "lineNumber": 75
          },
          {
            "text": "       nn.Linear(d_model, d_model // 2),",
            "lineNumber": 76
          },
          {
            "text": "       nn.ReLU(),",
            "lineNumber": 77
          },
          {
            "text": "       nn.Dropout(0.1),",
            "lineNumber": 78
          },
          {
            "text": "       nn.Linear(d_model // 2, 1)",
            "lineNumber": 79
          },
          {
            "text": "   )",
            "lineNumber": 80
          },
          {
            "text": "   &#96;&#96;&#96;",
            "lineNumber": 81
          },
          {
            "lineNumber": 82
          },
          {
            "text": "3. **&#96;mlp3&#96;**: 3-layer MLP",
            "lineNumber": 83
          },
          {
            "text": "   &#96;&#96;&#96;python",
            "lineNumber": 84
          },
          {
            "text": "   nn.Sequential(",
            "lineNumber": 85
          },
          {
            "text": "       nn.Linear(d_model, d_model),",
            "lineNumber": 86
          },
          {
            "text": "       nn.ReLU(),",
            "lineNumber": 87
          },
          {
            "text": "       nn.Dropout(0.1),",
            "lineNumber": 88
          },
          {
            "text": "       nn.Linear(d_model, d_model // 2),",
            "lineNumber": 89
          },
          {
            "text": "       nn.ReLU(),",
            "lineNumber": 90
          },
          {
            "text": "       nn.Dropout(0.1),",
            "lineNumber": 91
          },
          {
            "text": "       nn.Linear(d_model // 2, 1)",
            "lineNumber": 92
          },
          {
            "text": "   )",
            "lineNumber": 93
          },
          {
            "text": "   &#96;&#96;&#96;",
            "lineNumber": 94
          },
          {
            "lineNumber": 95
          },
          {
            "text": "4. **&#96;mlp2_ln&#96;**: 2-layer MLP with LayerNorm",
            "lineNumber": 96
          },
          {
            "text": "   &#96;&#96;&#96;python",
            "lineNumber": 97
          },
          {
            "text": "   nn.Sequential(",
            "lineNumber": 98
          },
          {
            "text": "       nn.LayerNorm(d_model),",
            "lineNumber": 99
          },
          {
            "text": "       nn.Linear(d_model, d_model // 2),",
            "lineNumber": 100
          },
          {
            "text": "       nn.ReLU(),",
            "lineNumber": 101
          },
          {
            "text": "       nn.Dropout(0.1),",
            "lineNumber": 102
          },
          {
            "text": "       nn.Linear(d_model // 2, 1)",
            "lineNumber": 103
          },
          {
            "text": "   )",
            "lineNumber": 104
          },
          {
            "text": "   &#96;&#96;&#96;",
            "lineNumber": 105
          },
          {
            "lineNumber": 106
          },
          {
            "lineNumber": 107
          },
          {
            "text": "**Pooling Modes** (reuse logic from &#96;SequenceDecoder&#96;):",
            "lineNumber": 108
          },
          {
            "lineNumber": 109
          },
          {
            "text": "- &#96;pool&#96;: Mean pooling (default) - works well for variable-length sequences",
            "lineNumber": 110
          },
          {
            "text": "- &#96;last&#96;: Use last token representation",
            "lineNumber": 111
          },
          {
            "text": "- &#96;first&#96;: Use first token representation",
            "lineNumber": 112
          },
          {
            "text": "- &#96;sum&#96;: Sum all tokens",
            "lineNumber": 113
          },
          {
            "lineNumber": 114
          },
          {
            "text": "**Output**: Single scalar pH value per sample &#96;(batch,)&#96; or &#96;(batch, 1)&#96;",
            "lineNumber": 115
          },
          {
            "lineNumber": 116
          },
          {
            "text": "### Model Integration Strategy",
            "lineNumber": 117
          },
          {
            "lineNumber": 118
          },
          {
            "text": "Since we're keeping separation from HyenaDNA codebase:",
            "lineNumber": 119
          }
        ]
      },
      "score": 0.2568795084953308
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./create_hf_dataset.py",
        "range": {
          "startPosition": {
            "line": 172
          },
          "endPosition": {
            "line": 244
          }
        },
        "contents": "def create_dataset_records(\n    fasta_files: List[Tuple[Path, str, str]],\n    sample_data: Dict[Tuple[str, str], Dict],\n    existing_records: set,\n) -&gt; List[Dict]:\n\n        print(f\"    Read {len(sequences)} sequences ({total_length:,} nucleotides)\")\n\n    return records\n\n\ndef upload_dataset(records: List[Dict], dataset_repo: str):\n    \"\"\"\n    Create or update the Hugging Face dataset with new records.\n    \"\"\"\n    if not records:\n        print(\"No new records to upload\")\n        return\n\n    print(f\"\\nCreating dataset with {len(records)} records...\")\n\n    # Check if dataset already exists and needs merging\n    try:\n        existing_dataset = load_dataset(dataset_repo, split=\"train\", token=HF_TOKEN)\n        print(f\"  Merging with {len(existing_dataset)} existing records...\")\n        # Convert existing dataset to list and append new records\n        all_records = list(existing_dataset) + records\n        dataset = Dataset.from_list(all_records)\n    except Exception:\n        # Dataset doesn't exist, create new one\n        print(\"  Creating new dataset...\")\n        dataset = Dataset.from_list(records)\n\n    # Push to Hugging Face\n    print(f\"Uploading to {dataset_repo}...\")\n    dataset.push_to_hub(dataset_repo, token=HF_TOKEN, private=False)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def create_dataset_records(",
            "lineNumber": 173,
            "isSignature": true
          },
          {
            "text": "    fasta_files: List[Tuple[Path, str, str]],",
            "lineNumber": 174,
            "isSignature": true
          },
          {
            "text": "    sample_data: Dict[Tuple[str, str], Dict],",
            "lineNumber": 175,
            "isSignature": true
          },
          {
            "text": "    existing_records: set,",
            "lineNumber": 176,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[Dict]:",
            "lineNumber": 177,
            "isSignature": true
          },
          {
            "lineNumber": 214
          },
          {
            "text": "        print(f\"    Read {len(sequences)} sequences ({total_length:,} nucleotides)\")",
            "lineNumber": 215
          },
          {
            "lineNumber": 216
          },
          {
            "text": "    return records",
            "lineNumber": 217,
            "isSignature": true
          },
          {
            "lineNumber": 218
          },
          {
            "lineNumber": 219
          },
          {
            "text": "def upload_dataset(records: List[Dict], dataset_repo: str):",
            "lineNumber": 220,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 221
          },
          {
            "text": "    Create or update the Hugging Face dataset with new records.",
            "lineNumber": 222
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 223
          },
          {
            "text": "    if not records:",
            "lineNumber": 224
          },
          {
            "text": "        print(\"No new records to upload\")",
            "lineNumber": 225
          },
          {
            "text": "        return",
            "lineNumber": 226
          },
          {
            "lineNumber": 227
          },
          {
            "text": "    print(f\"\\nCreating dataset with {len(records)} records...\")",
            "lineNumber": 228
          },
          {
            "lineNumber": 229
          },
          {
            "text": "    # Check if dataset already exists and needs merging",
            "lineNumber": 230
          },
          {
            "text": "    try:",
            "lineNumber": 231
          },
          {
            "text": "        existing_dataset = load_dataset(dataset_repo, split=\"train\", token=HF_TOKEN)",
            "lineNumber": 232
          },
          {
            "text": "        print(f\"  Merging with {len(existing_dataset)} existing records...\")",
            "lineNumber": 233
          },
          {
            "text": "        # Convert existing dataset to list and append new records",
            "lineNumber": 234
          },
          {
            "text": "        all_records = list(existing_dataset) + records",
            "lineNumber": 235
          },
          {
            "text": "        dataset = Dataset.from_list(all_records)",
            "lineNumber": 236
          },
          {
            "text": "    except Exception:",
            "lineNumber": 237
          },
          {
            "text": "        # Dataset doesn't exist, create new one",
            "lineNumber": 238
          },
          {
            "text": "        print(\"  Creating new dataset...\")",
            "lineNumber": 239
          },
          {
            "text": "        dataset = Dataset.from_list(records)",
            "lineNumber": 240
          },
          {
            "lineNumber": 241
          },
          {
            "text": "    # Push to Hugging Face",
            "lineNumber": 242
          },
          {
            "text": "    print(f\"Uploading to {dataset_repo}...\")",
            "lineNumber": 243
          },
          {
            "text": "    dataset.push_to_hub(dataset_repo, token=HF_TOKEN, private=False)",
            "lineNumber": 244
          }
        ]
      },
      "score": 0.2557004690170288
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/standalone_hyenadna.py",
        "range": {
          "startPosition": {
            "line": 1149
          },
          "endPosition": {
            "line": 1295
          }
        },
        "contents": "class CharacterTokenizer(PreTrainedTokenizer):\ndef __init__(\n        self,\n        characters: Sequence[str],\n        model_max_length: int,\n        padding_side: str = \"left\",\n        **kwargs,\n    ):\n\n\n        # Create vocabulary dictionaries BEFORE calling super().__init__()\n        # because get_vocab() will be called during initialization\n        self._vocab_str_to_int = {\n            \"[CLS]\": 0,\n            \"[SEP]\": 1,\n            \"[BOS]\": 2,\n            \"[MASK]\": 3,\n            \"[PAD]\": 4,\n            \"[RESERVED]\": 5,\n            \"[UNK]\": 6,\n            **{ch: i + 7 for i, ch in enumerate(characters)},\n        }\n        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n\n        super().__init__(\n            bos_token=bos_token,\n            eos_token=sep_token,\n            sep_token=sep_token,\n            cls_token=cls_token,\n            pad_token=pad_token,\n            mask_token=mask_token,\n            unk_token=unk_token,\n            add_prefix_space=False,\n            model_max_length=model_max_length,\n            padding_side=padding_side,\n            **kwargs,\n        )\n\n    @property\n    def vocab_size(self) -&gt; int:\n        return len(self._vocab_str_to_int)\n\n    def get_vocab(self) -&gt; Dict[str, int]:\n        \"\"\"Returns the vocabulary as a dictionary of token to index.\"\"\"\n        return self._vocab_str_to_int.copy()\n\n    def _tokenize(self, text: str) -&gt; List[str]:\n        return list(text)\n\n    def _convert_token_to_id(self, token: str) -&gt; int:\n        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n\n    def _convert_id_to_token(self, index: int) -&gt; str:\n        return self._vocab_int_to_str[index]\n\n    def convert_tokens_to_string(self, tokens):\n        return \"\".join(tokens)\n\n    def build_inputs_with_special_tokens(\n        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n    ) -&gt; List[int]:\n        sep = [self.sep_token_id]\n        cls = [self.cls_token_id]\n        result = cls + token_ids_0 + sep\n        if token_ids_1 is not None:\n            result += token_ids_1 + sep\n        return result\n\n    def get_special_tokens_mask(\n        self,\n        token_ids_0: List[int],\n        token_ids_1: Optional[List[int]] = None,\n        already_has_special_tokens: bool = False,\n    ) -&gt; List[int]:\n        ",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 1150,
                "column": 1
              },
              "endPosition": {
                "line": 1151,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 1151,
                "column": 5
              },
              "endPosition": {
                "line": 1158,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class CharacterTokenizer(PreTrainedTokenizer):",
            "lineNumber": 1150,
            "isSignature": true
          },
          {
            "text": "def __init__(",
            "lineNumber": 1151
          },
          {
            "text": "        self,",
            "lineNumber": 1152
          },
          {
            "text": "        characters: Sequence[str],",
            "lineNumber": 1153
          },
          {
            "text": "        model_max_length: int,",
            "lineNumber": 1154
          },
          {
            "text": "        padding_side: str = \"left\",",
            "lineNumber": 1155
          },
          {
            "text": "        **kwargs,",
            "lineNumber": 1156
          },
          {
            "text": "    ):",
            "lineNumber": 1157
          },
          {
            "lineNumber": 1183
          },
          {
            "lineNumber": 1184
          },
          {
            "text": "        # Create vocabulary dictionaries BEFORE calling super().__init__()",
            "lineNumber": 1185
          },
          {
            "text": "        # because get_vocab() will be called during initialization",
            "lineNumber": 1186
          },
          {
            "text": "        self._vocab_str_to_int = {",
            "lineNumber": 1187
          },
          {
            "text": "            \"[CLS]\": 0,",
            "lineNumber": 1188
          },
          {
            "text": "            \"[SEP]\": 1,",
            "lineNumber": 1189
          },
          {
            "text": "            \"[BOS]\": 2,",
            "lineNumber": 1190
          },
          {
            "text": "            \"[MASK]\": 3,",
            "lineNumber": 1191
          },
          {
            "text": "            \"[PAD]\": 4,",
            "lineNumber": 1192
          },
          {
            "text": "            \"[RESERVED]\": 5,",
            "lineNumber": 1193
          },
          {
            "text": "            \"[UNK]\": 6,",
            "lineNumber": 1194
          },
          {
            "text": "            **{ch: i + 7 for i, ch in enumerate(characters)},",
            "lineNumber": 1195
          },
          {
            "text": "        }",
            "lineNumber": 1196
          },
          {
            "text": "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}",
            "lineNumber": 1197
          },
          {
            "lineNumber": 1198
          },
          {
            "text": "        super().__init__(",
            "lineNumber": 1199
          },
          {
            "text": "            bos_token=bos_token,",
            "lineNumber": 1200
          },
          {
            "text": "            eos_token=sep_token,",
            "lineNumber": 1201
          },
          {
            "text": "            sep_token=sep_token,",
            "lineNumber": 1202
          },
          {
            "text": "            cls_token=cls_token,",
            "lineNumber": 1203
          },
          {
            "text": "            pad_token=pad_token,",
            "lineNumber": 1204
          },
          {
            "text": "            mask_token=mask_token,",
            "lineNumber": 1205
          },
          {
            "text": "            unk_token=unk_token,",
            "lineNumber": 1206
          },
          {
            "text": "            add_prefix_space=False,",
            "lineNumber": 1207
          },
          {
            "text": "            model_max_length=model_max_length,",
            "lineNumber": 1208
          },
          {
            "text": "            padding_side=padding_side,",
            "lineNumber": 1209
          },
          {
            "text": "            **kwargs,",
            "lineNumber": 1210
          },
          {
            "text": "        )",
            "lineNumber": 1211
          },
          {
            "lineNumber": 1212
          },
          {
            "text": "    @property",
            "lineNumber": 1213
          },
          {
            "text": "    def vocab_size(self) -&gt; int:",
            "lineNumber": 1214
          },
          {
            "text": "        return len(self._vocab_str_to_int)",
            "lineNumber": 1215
          },
          {
            "lineNumber": 1216
          },
          {
            "text": "    def get_vocab(self) -&gt; Dict[str, int]:",
            "lineNumber": 1217
          },
          {
            "text": "        \"\"\"Returns the vocabulary as a dictionary of token to index.\"\"\"",
            "lineNumber": 1218
          },
          {
            "text": "        return self._vocab_str_to_int.copy()",
            "lineNumber": 1219
          },
          {
            "lineNumber": 1220
          },
          {
            "text": "    def _tokenize(self, text: str) -&gt; List[str]:",
            "lineNumber": 1221
          },
          {
            "text": "        return list(text)",
            "lineNumber": 1222
          },
          {
            "lineNumber": 1223
          },
          {
            "text": "    def _convert_token_to_id(self, token: str) -&gt; int:",
            "lineNumber": 1224
          },
          {
            "text": "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])",
            "lineNumber": 1225
          },
          {
            "lineNumber": 1226
          },
          {
            "text": "    def _convert_id_to_token(self, index: int) -&gt; str:",
            "lineNumber": 1227
          },
          {
            "text": "        return self._vocab_int_to_str[index]",
            "lineNumber": 1228
          },
          {
            "lineNumber": 1229
          },
          {
            "text": "    def convert_tokens_to_string(self, tokens):",
            "lineNumber": 1230
          },
          {
            "text": "        return \"\".join(tokens)",
            "lineNumber": 1231
          },
          {
            "lineNumber": 1232
          },
          {
            "text": "    def build_inputs_with_special_tokens(",
            "lineNumber": 1233
          },
          {
            "text": "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None",
            "lineNumber": 1234
          },
          {
            "text": "    ) -&gt; List[int]:",
            "lineNumber": 1235
          },
          {
            "text": "        sep = [self.sep_token_id]",
            "lineNumber": 1236
          },
          {
            "text": "        cls = [self.cls_token_id]",
            "lineNumber": 1237
          },
          {
            "text": "        result = cls + token_ids_0 + sep",
            "lineNumber": 1238
          },
          {
            "text": "        if token_ids_1 is not None:",
            "lineNumber": 1239
          },
          {
            "text": "            result += token_ids_1 + sep",
            "lineNumber": 1240
          },
          {
            "text": "        return result",
            "lineNumber": 1241
          },
          {
            "lineNumber": 1242
          },
          {
            "text": "    def get_special_tokens_mask(",
            "lineNumber": 1243
          },
          {
            "text": "        self,",
            "lineNumber": 1244
          },
          {
            "text": "        token_ids_0: List[int],",
            "lineNumber": 1245
          },
          {
            "text": "        token_ids_1: Optional[List[int]] = None,",
            "lineNumber": 1246
          },
          {
            "text": "        already_has_special_tokens: bool = False,",
            "lineNumber": 1247
          },
          {
            "text": "    ) -&gt; List[int]:",
            "lineNumber": 1248
          },
          {
            "text": "        ",
            "lineNumber": 1249
          }
        ]
      },
      "score": 0.2509441375732422
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/standalone_hyenadna.py",
        "range": {
          "startPosition": {
            "line": 1044
          },
          "endPosition": {
            "line": 1139,
            "column": 3
          }
        },
        "contents": "class HyenaDNAModel(nn.Module):\ndef __init__(\n        self,\n        d_model: int,\n        n_layer: int,\n        d_inner: int,\n        vocab_size: int,\n        layer=None,\n        attn_layer_idx=None,\n        attn_cfg=None,\n        max_position_embeddings=0,\n        resid_dropout: float = 0.0,\n        embed_dropout: float = 0.1,\n        layer_norm_epsilon: float = 1e-5,\n        initializer_cfg=None,\n        residual_in_fp32=False,\n        pad_vocab_size_multiple: int = 1,\n        use_head=False,\n        n_classes: int = 2,\n        device=None,\n        dtype=None,\n        **kwargs,\n    ) -&gt; None:\n\n\n        self.backbone = LMBackbone(\n            d_model=d_model,\n            n_layer=n_layer,\n            d_inner=d_inner,\n            vocab_size=vocab_size,\n            layer=layer,\n            attn_layer_idx=attn_layer_idx,\n            attn_cfg=attn_cfg,\n            max_position_embeddings=max_position_embeddings,\n            resid_dropout=resid_dropout,\n            embed_dropout=embed_dropout,\n            layer_norm_epsilon=layer_norm_epsilon,\n            initializer_cfg=initializer_cfg,\n            residual_in_fp32=residual_in_fp32,\n            **factory_kwargs,\n            **kwargs,\n        )\n\n        # we only need a head if doing classification, otherwise we'll use the\n        # hidden states as embeddings\n        if self.use_head:\n            self.head = SequenceDecoder(\n                d_model=d_model, d_output=n_classes, l_output=0, mode=\"pool\"\n            )\n\n        # Initialize weights and apply final processing\n        self.apply(\n            partial(\n                _init_weights,\n                n_layer=n_layer,\n                **(initializer_cfg if initializer_cfg is not None else {}),\n            )\n        )\n\n        # if self.use_head:\n        #     self.tie_weights()\n\n    # def tie_weights(self):\n    #     self.head.weight = self.backbone.embeddings.word_embeddings.weight\n\n    def forward(\n        self, input_ids, position_ids=None, state=None\n    ):  # state for the repo interface\n        hidden_states = self.backbone(input_ids, position_ids=position_ids)\n\n        if self.use_head:\n            return self.head(hidden_states)\n        else:\n            return hidden_states\n\n\n\"\"\"# Data pipeline\n\n\n\"\"\"\n\n# @title Tokenizer\n\n\"\"\"",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 1045,
                "column": 1
              },
              "endPosition": {
                "line": 1047,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 1047,
                "column": 5
              },
              "endPosition": {
                "line": 1069,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class HyenaDNAModel(nn.Module):",
            "lineNumber": 1045,
            "isSignature": true
          },
          {
            "text": "def __init__(",
            "lineNumber": 1047
          },
          {
            "text": "        self,",
            "lineNumber": 1048
          },
          {
            "text": "        d_model: int,",
            "lineNumber": 1049
          },
          {
            "text": "        n_layer: int,",
            "lineNumber": 1050
          },
          {
            "text": "        d_inner: int,",
            "lineNumber": 1051
          },
          {
            "text": "        vocab_size: int,",
            "lineNumber": 1052
          },
          {
            "text": "        layer=None,",
            "lineNumber": 1053
          },
          {
            "text": "        attn_layer_idx=None,",
            "lineNumber": 1054
          },
          {
            "text": "        attn_cfg=None,",
            "lineNumber": 1055
          },
          {
            "text": "        max_position_embeddings=0,",
            "lineNumber": 1056
          },
          {
            "text": "        resid_dropout: float = 0.0,",
            "lineNumber": 1057
          },
          {
            "text": "        embed_dropout: float = 0.1,",
            "lineNumber": 1058
          },
          {
            "text": "        layer_norm_epsilon: float = 1e-5,",
            "lineNumber": 1059
          },
          {
            "text": "        initializer_cfg=None,",
            "lineNumber": 1060
          },
          {
            "text": "        residual_in_fp32=False,",
            "lineNumber": 1061
          },
          {
            "text": "        pad_vocab_size_multiple: int = 1,",
            "lineNumber": 1062
          },
          {
            "text": "        use_head=False,",
            "lineNumber": 1063
          },
          {
            "text": "        n_classes: int = 2,",
            "lineNumber": 1064
          },
          {
            "text": "        device=None,",
            "lineNumber": 1065
          },
          {
            "text": "        dtype=None,",
            "lineNumber": 1066
          },
          {
            "text": "        **kwargs,",
            "lineNumber": 1067
          },
          {
            "text": "    ) -&gt; None:",
            "lineNumber": 1068
          },
          {
            "lineNumber": 1080
          },
          {
            "lineNumber": 1081
          },
          {
            "text": "        self.backbone = LMBackbone(",
            "lineNumber": 1082
          },
          {
            "text": "            d_model=d_model,",
            "lineNumber": 1083
          },
          {
            "text": "            n_layer=n_layer,",
            "lineNumber": 1084
          },
          {
            "text": "            d_inner=d_inner,",
            "lineNumber": 1085
          },
          {
            "text": "            vocab_size=vocab_size,",
            "lineNumber": 1086
          },
          {
            "text": "            layer=layer,",
            "lineNumber": 1087
          },
          {
            "text": "            attn_layer_idx=attn_layer_idx,",
            "lineNumber": 1088
          },
          {
            "text": "            attn_cfg=attn_cfg,",
            "lineNumber": 1089
          },
          {
            "text": "            max_position_embeddings=max_position_embeddings,",
            "lineNumber": 1090
          },
          {
            "text": "            resid_dropout=resid_dropout,",
            "lineNumber": 1091
          },
          {
            "text": "            embed_dropout=embed_dropout,",
            "lineNumber": 1092
          },
          {
            "text": "            layer_norm_epsilon=layer_norm_epsilon,",
            "lineNumber": 1093
          },
          {
            "text": "            initializer_cfg=initializer_cfg,",
            "lineNumber": 1094
          },
          {
            "text": "            residual_in_fp32=residual_in_fp32,",
            "lineNumber": 1095
          },
          {
            "text": "            **factory_kwargs,",
            "lineNumber": 1096
          },
          {
            "text": "            **kwargs,",
            "lineNumber": 1097
          },
          {
            "text": "        )",
            "lineNumber": 1098
          },
          {
            "lineNumber": 1099
          },
          {
            "text": "        # we only need a head if doing classification, otherwise we'll use the",
            "lineNumber": 1100
          },
          {
            "text": "        # hidden states as embeddings",
            "lineNumber": 1101
          },
          {
            "text": "        if self.use_head:",
            "lineNumber": 1102
          },
          {
            "text": "            self.head = SequenceDecoder(",
            "lineNumber": 1103
          },
          {
            "text": "                d_model=d_model, d_output=n_classes, l_output=0, mode=\"pool\"",
            "lineNumber": 1104
          },
          {
            "text": "            )",
            "lineNumber": 1105
          },
          {
            "lineNumber": 1106
          },
          {
            "text": "        # Initialize weights and apply final processing",
            "lineNumber": 1107
          },
          {
            "text": "        self.apply(",
            "lineNumber": 1108
          },
          {
            "text": "            partial(",
            "lineNumber": 1109
          },
          {
            "text": "                _init_weights,",
            "lineNumber": 1110
          },
          {
            "text": "                n_layer=n_layer,",
            "lineNumber": 1111
          },
          {
            "text": "                **(initializer_cfg if initializer_cfg is not None else {}),",
            "lineNumber": 1112
          },
          {
            "text": "            )",
            "lineNumber": 1113
          },
          {
            "text": "        )",
            "lineNumber": 1114
          },
          {
            "lineNumber": 1115
          },
          {
            "text": "        # if self.use_head:",
            "lineNumber": 1116
          },
          {
            "text": "        #     self.tie_weights()",
            "lineNumber": 1117
          },
          {
            "lineNumber": 1118
          },
          {
            "text": "    # def tie_weights(self):",
            "lineNumber": 1119
          },
          {
            "text": "    #     self.head.weight = self.backbone.embeddings.word_embeddings.weight",
            "lineNumber": 1120
          },
          {
            "lineNumber": 1121
          },
          {
            "text": "    def forward(",
            "lineNumber": 1122
          },
          {
            "text": "        self, input_ids, position_ids=None, state=None",
            "lineNumber": 1123
          },
          {
            "text": "    ):  # state for the repo interface",
            "lineNumber": 1124
          },
          {
            "text": "        hidden_states = self.backbone(input_ids, position_ids=position_ids)",
            "lineNumber": 1125
          },
          {
            "lineNumber": 1126
          },
          {
            "text": "        if self.use_head:",
            "lineNumber": 1127
          },
          {
            "text": "            return self.head(hidden_states)",
            "lineNumber": 1128
          },
          {
            "text": "        else:",
            "lineNumber": 1129
          },
          {
            "text": "            return hidden_states",
            "lineNumber": 1130,
            "isSignature": true
          },
          {
            "lineNumber": 1131
          },
          {
            "lineNumber": 1132
          },
          {
            "text": "\"\"\"# Data pipeline",
            "lineNumber": 1133
          },
          {
            "lineNumber": 1134
          },
          {
            "lineNumber": 1135
          },
          {
            "text": "\"\"\"",
            "lineNumber": 1136
          },
          {
            "lineNumber": 1137
          },
          {
            "text": "# @title Tokenizer",
            "lineNumber": 1138
          },
          {
            "lineNumber": 1139
          },
          {
            "text": "\"\"\"",
            "lineNumber": 1140
          }
        ]
      },
      "score": 0.24062480032444
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./create_hf_dataset.py",
        "range": {
          "startPosition": {
            "line": 219
          },
          "endPosition": {
            "line": 286,
            "column": 24
          }
        },
        "contents": "def upload_dataset(records: List[Dict], dataset_repo: str):\n\n\n    # Push to Hugging Face\n    print(f\"Uploading to {dataset_repo}...\")\n    dataset.push_to_hub(dataset_repo, token=HF_TOKEN, private=False)\n    print(f\"✓ Successfully uploaded {len(records)} new records to {dataset_repo}\")\n\n\ndef main():\n    \"\"\"Main function to orchestrate the dataset creation.\"\"\"\n    # Paths\n    project_root = Path(__file__).parent\n    fasta_dir = project_root / \"data\" / \"fasta\"\n    sample_data_path = project_root / \"data\" / \"sample_data.csv\"\n\n    # Check HF_TOKEN\n    if not HF_TOKEN:\n        raise ValueError(\"HF_TOKEN not found in .env file\")\n\n    # Login to Hugging Face\n    print(\"Logging in to Hugging Face...\")\n    login(token=HF_TOKEN)\n\n    # Load sample data\n    print(\"\\nLoading sample data...\")\n    sample_data = load_sample_data(sample_data_path)\n    print(f\"✓ Loaded {len(sample_data)} sample records\")\n\n    # Find fasta files with matching entries in sample_data\n    print(\"\\nFinding fasta files...\")\n    fasta_files = validate_fasta_files(fasta_dir, sample_data)\n\n    # Check existing records\n    print(\"\\nChecking existing dataset records...\")\n    existing_records = get_existing_records(HF_DATASET_REPO)\n\n    # Create dataset records\n    print(\"\\nProcessing fasta files...\")\n    records = create_dataset_records(fasta_files, sample_data, existing_records)\n\n    # Upload dataset\n    if records:\n        print(\"\\nUploading dataset...\")\n        upload_dataset(records, HF_DATASET_REPO)\n    else:\n        print(\"\\nAll records already exist in the dataset. Nothing to upload.\")\n\n    print(\"\\n✓ Done!\")",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def upload_dataset(records: List[Dict], dataset_repo: str):",
            "lineNumber": 220,
            "isSignature": true
          },
          {
            "lineNumber": 240
          },
          {
            "lineNumber": 241
          },
          {
            "text": "    # Push to Hugging Face",
            "lineNumber": 242
          },
          {
            "text": "    print(f\"Uploading to {dataset_repo}...\")",
            "lineNumber": 243
          },
          {
            "text": "    dataset.push_to_hub(dataset_repo, token=HF_TOKEN, private=False)",
            "lineNumber": 244
          },
          {
            "text": "    print(f\"✓ Successfully uploaded {len(records)} new records to {dataset_repo}\")",
            "lineNumber": 245,
            "isSignature": true
          },
          {
            "lineNumber": 246
          },
          {
            "lineNumber": 247
          },
          {
            "text": "def main():",
            "lineNumber": 248,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Main function to orchestrate the dataset creation.\"\"\"",
            "lineNumber": 249
          },
          {
            "text": "    # Paths",
            "lineNumber": 250
          },
          {
            "text": "    project_root = Path(__file__).parent",
            "lineNumber": 251
          },
          {
            "text": "    fasta_dir = project_root / \"data\" / \"fasta\"",
            "lineNumber": 252
          },
          {
            "text": "    sample_data_path = project_root / \"data\" / \"sample_data.csv\"",
            "lineNumber": 253
          },
          {
            "lineNumber": 254
          },
          {
            "text": "    # Check HF_TOKEN",
            "lineNumber": 255
          },
          {
            "text": "    if not HF_TOKEN:",
            "lineNumber": 256
          },
          {
            "text": "        raise ValueError(\"HF_TOKEN not found in .env file\")",
            "lineNumber": 257
          },
          {
            "lineNumber": 258
          },
          {
            "text": "    # Login to Hugging Face",
            "lineNumber": 259
          },
          {
            "text": "    print(\"Logging in to Hugging Face...\")",
            "lineNumber": 260
          },
          {
            "text": "    login(token=HF_TOKEN)",
            "lineNumber": 261
          },
          {
            "lineNumber": 262
          },
          {
            "text": "    # Load sample data",
            "lineNumber": 263
          },
          {
            "text": "    print(\"\\nLoading sample data...\")",
            "lineNumber": 264
          },
          {
            "text": "    sample_data = load_sample_data(sample_data_path)",
            "lineNumber": 265
          },
          {
            "text": "    print(f\"✓ Loaded {len(sample_data)} sample records\")",
            "lineNumber": 266
          },
          {
            "lineNumber": 267
          },
          {
            "text": "    # Find fasta files with matching entries in sample_data",
            "lineNumber": 268
          },
          {
            "text": "    print(\"\\nFinding fasta files...\")",
            "lineNumber": 269
          },
          {
            "text": "    fasta_files = validate_fasta_files(fasta_dir, sample_data)",
            "lineNumber": 270
          },
          {
            "lineNumber": 271
          },
          {
            "text": "    # Check existing records",
            "lineNumber": 272
          },
          {
            "text": "    print(\"\\nChecking existing dataset records...\")",
            "lineNumber": 273
          },
          {
            "text": "    existing_records = get_existing_records(HF_DATASET_REPO)",
            "lineNumber": 274
          },
          {
            "lineNumber": 275
          },
          {
            "text": "    # Create dataset records",
            "lineNumber": 276
          },
          {
            "text": "    print(\"\\nProcessing fasta files...\")",
            "lineNumber": 277
          },
          {
            "text": "    records = create_dataset_records(fasta_files, sample_data, existing_records)",
            "lineNumber": 278
          },
          {
            "lineNumber": 279
          },
          {
            "text": "    # Upload dataset",
            "lineNumber": 280
          },
          {
            "text": "    if records:",
            "lineNumber": 281
          },
          {
            "text": "        print(\"\\nUploading dataset...\")",
            "lineNumber": 282
          },
          {
            "text": "        upload_dataset(records, HF_DATASET_REPO)",
            "lineNumber": 283
          },
          {
            "text": "    else:",
            "lineNumber": 284
          },
          {
            "text": "        print(\"\\nAll records already exist in the dataset. Nothing to upload.\")",
            "lineNumber": 285
          },
          {
            "lineNumber": 286
          },
          {
            "text": "    print(\"\\n✓ Done!\")",
            "lineNumber": 287,
            "isSignature": true
          }
        ]
      },
      "score": 0.24049609899520874
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./download_sra_data.py",
        "range": {
          "startPosition": {
            "line": 165,
            "column": 20
          },
          "endPosition": {
            "line": 324
          }
        },
        "contents": "def process_sample(study_name: str, sample_id: str, output_dir: Path) -&gt; bool:\n    \"\"\"\n    Download and process a single SRA sample.\n\n    Returns True if successful, False otherwise.\n    \"\"\"\n    # Check if output file already exists\n    output_file = output_dir / f\"{sample_id}.fasta.gz\"\n    if output_file.exists():\n        return True  # Already downloaded\n\n    # Create output directory if it doesn't exist\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Construct download URL\n    url = f\"https://trace.ncbi.nlm.nih.gov/Traces/sra-reads-be/fasta?acc={sample_id}\"\n\n    # Create temporary files\n    temp_dir = Path(tempfile.gettempdir())\n    temp_gzip = temp_dir / f\"sra_download_{sample_id}_{os.getpid()}.gz\"\n    temp_fasta = temp_dir / f\"sra_extracted_{sample_id}_{os.getpid()}.fasta\"\n    temp_fasta_processed = temp_dir / f\"sra_processed_{sample_id}_{os.getpid()}.fasta\"\n\n    temp_files.add(temp_gzip)\n    temp_files.add(temp_fasta)\n    temp_files.add(temp_fasta_processed)\n\n    try:\n        # Step 1: Download first 0.25MB\n        if not download_first_mb(url, temp_gzip):\n            return False\n\n        # Step 2: Extract incomplete gzip\n        if not extract_incomplete_gzip(temp_gzip, temp_fasta):\n            return False\n\n        # Step 3: Remove last sequence\n        if not remove_last_sequence(temp_fasta, temp_fasta_processed):\n            return False\n\n        # Step 4: Compress and save\n        if not compress_fasta(temp_fasta_processed, output_file):\n            return False\n\n        return True\n\n    except Exception as e:\n        print(f\"    Error processing {sample_id}: {e}\")\n        return False\n    finally:\n        # Clean up temporary files\n        for temp_file in [temp_gzip, temp_fasta, temp_fasta_processed]:\n            try:\n                if temp_file.exists():\n                    temp_file.unlink()\n                    temp_files.discard(temp_file)\n            except Exception:\n                pass\n\n\ndef main():\n    \"\"\"Main function to process all samples from sample_data.csv.\"\"\"",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def process_sample(study_name: str, sample_id: str, output_dir: Path) -&gt; bool:",
            "lineNumber": 169,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 170
          },
          {
            "text": "    Download and process a single SRA sample.",
            "lineNumber": 171
          },
          {
            "lineNumber": 172
          },
          {
            "text": "    Returns True if successful, False otherwise.",
            "lineNumber": 173
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 174
          },
          {
            "text": "    # Check if output file already exists",
            "lineNumber": 175
          },
          {
            "text": "    output_file = output_dir / f\"{sample_id}.fasta.gz\"",
            "lineNumber": 176
          },
          {
            "text": "    if output_file.exists():",
            "lineNumber": 177
          },
          {
            "text": "        return True  # Already downloaded",
            "lineNumber": 178
          },
          {
            "lineNumber": 179
          },
          {
            "text": "    # Create output directory if it doesn't exist",
            "lineNumber": 180
          },
          {
            "text": "    output_dir.mkdir(parents=True, exist_ok=True)",
            "lineNumber": 181
          },
          {
            "lineNumber": 182
          },
          {
            "text": "    # Construct download URL",
            "lineNumber": 183
          },
          {
            "text": "    url = f\"https://trace.ncbi.nlm.nih.gov/Traces/sra-reads-be/fasta?acc={sample_id}\"",
            "lineNumber": 184
          },
          {
            "lineNumber": 185
          },
          {
            "text": "    # Create temporary files",
            "lineNumber": 186
          },
          {
            "text": "    temp_dir = Path(tempfile.gettempdir())",
            "lineNumber": 187
          },
          {
            "text": "    temp_gzip = temp_dir / f\"sra_download_{sample_id}_{os.getpid()}.gz\"",
            "lineNumber": 188
          },
          {
            "text": "    temp_fasta = temp_dir / f\"sra_extracted_{sample_id}_{os.getpid()}.fasta\"",
            "lineNumber": 189
          },
          {
            "text": "    temp_fasta_processed = temp_dir / f\"sra_processed_{sample_id}_{os.getpid()}.fasta\"",
            "lineNumber": 190
          },
          {
            "lineNumber": 191
          },
          {
            "text": "    temp_files.add(temp_gzip)",
            "lineNumber": 192
          },
          {
            "text": "    temp_files.add(temp_fasta)",
            "lineNumber": 193
          },
          {
            "text": "    temp_files.add(temp_fasta_processed)",
            "lineNumber": 194
          },
          {
            "lineNumber": 195
          },
          {
            "text": "    try:",
            "lineNumber": 196
          },
          {
            "text": "        # Step 1: Download first 0.25MB",
            "lineNumber": 197
          },
          {
            "text": "        if not download_first_mb(url, temp_gzip):",
            "lineNumber": 198
          },
          {
            "text": "            return False",
            "lineNumber": 199
          },
          {
            "lineNumber": 200
          },
          {
            "text": "        # Step 2: Extract incomplete gzip",
            "lineNumber": 201
          },
          {
            "text": "        if not extract_incomplete_gzip(temp_gzip, temp_fasta):",
            "lineNumber": 202
          },
          {
            "text": "            return False",
            "lineNumber": 203
          },
          {
            "lineNumber": 204
          },
          {
            "text": "        # Step 3: Remove last sequence",
            "lineNumber": 205
          },
          {
            "text": "        if not remove_last_sequence(temp_fasta, temp_fasta_processed):",
            "lineNumber": 206
          },
          {
            "text": "            return False",
            "lineNumber": 207
          },
          {
            "lineNumber": 208
          },
          {
            "text": "        # Step 4: Compress and save",
            "lineNumber": 209
          },
          {
            "text": "        if not compress_fasta(temp_fasta_processed, output_file):",
            "lineNumber": 210
          },
          {
            "text": "            return False",
            "lineNumber": 211
          },
          {
            "lineNumber": 212
          },
          {
            "text": "        return True",
            "lineNumber": 213
          },
          {
            "lineNumber": 214
          },
          {
            "text": "    except Exception as e:",
            "lineNumber": 215
          },
          {
            "text": "        print(f\"    Error processing {sample_id}: {e}\")",
            "lineNumber": 216
          },
          {
            "text": "        return False",
            "lineNumber": 217
          },
          {
            "text": "    finally:",
            "lineNumber": 218
          },
          {
            "text": "        # Clean up temporary files",
            "lineNumber": 219
          },
          {
            "text": "        for temp_file in [temp_gzip, temp_fasta, temp_fasta_processed]:",
            "lineNumber": 220
          },
          {
            "text": "            try:",
            "lineNumber": 221
          },
          {
            "text": "                if temp_file.exists():",
            "lineNumber": 222
          },
          {
            "text": "                    temp_file.unlink()",
            "lineNumber": 223
          },
          {
            "text": "                    temp_files.discard(temp_file)",
            "lineNumber": 224
          },
          {
            "text": "            except Exception:",
            "lineNumber": 225
          },
          {
            "text": "                pass",
            "lineNumber": 226,
            "isSignature": true
          },
          {
            "lineNumber": 227
          },
          {
            "lineNumber": 228
          },
          {
            "text": "def main():",
            "lineNumber": 229,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Main function to process all samples from sample_data.csv.\"\"\"",
            "lineNumber": 230
          }
        ]
      },
      "score": 0.23693275451660156
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 278
          },
          "endPosition": {
            "line": 438
          }
        },
        "contents": "def collate_fn(batch: List[Dict]) -&gt; Dict:\n,\n        \"study_name\": study_names,\n        \"sample_id\": sample_ids,\n    }\n\n\ndef load_dataset_from_hf(\n    dataset_repo: str = \"jedick/microbial-DNA-pH\",\n    filter_missing_ph: bool = True,\n) -&gt; List[Dict]:\n    \"\"\"\n    Load dataset from HuggingFace.\n\n    Includes study_name and sample_id for subsetting by shared train/val/test splits.\n    HF dataset contains SRA samples only (SRR*, ERR*, DRR*).\n\n    Args:\n        dataset_repo: HuggingFace dataset repository\n        filter_missing_ph: Whether to filter samples with missing pH\n\n    Returns:\n        List of dicts with 'sequences', 'pH', 'study_name', 'sample_id', 'environment'\n        Sorted by sample_id for consistent ordering across workflows.\n    \"\"\"\n    print(f\"Loading dataset from {dataset_repo}...\")\n    dataset = load_dataset(dataset_repo, split=\"train\")\n\n    data = []\n    for item in dataset:\n        # Filter missing pH if requested\n        if filter_missing_ph and item[\"pH\"] is None:\n            continue\n\n        data.append(\n            {\n                \"sequences\": item[\"sequences\"],\n                \"pH\": item[\"pH\"] if item[\"pH\"] is not None else 0.0,\n                \"study_name\": item[\"study_name\"],\n                \"sample_id\": item[\"sample_id\"],\n                \"environment\": item.get(\"environment\", \"unknown\"),\n            }\n        )\n\n    # Sort by sample_id to ensure consistent ordering across workflows\n    data.sort(key=lambda x: x[\"sample_id\"])\n\n    print(f\"Loaded {len(data)} samples\")\n    return data\n\n\ndef setup_model(\n    model_name: str = \"hyenadna-tiny-1k-seqlen\",\n    checkpoint_dir: str = \"./checkpoints\",\n    head_architecture: str = \"mlp2\",\n    pooling_mode: str = \"pool\",\n    freeze_backbone: bool = False,\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    download: bool = False,\n) -&gt; Tuple[nn.Module, CharacterTokenizer, int]:\n    \"\"\"\n    Load pretrained HyenaDNA model and attach regression head.\n\n    Args:\n        model_name: Model name from MODEL_CONFIGS\n        checkpoint_dir: Directory for model checkpoints\n        head_architecture: Regression head architecture\n        pooling_mode: Pooling mode for regression head\n        freeze_backbone: Whether to freeze backbone weights\n        device: Device to use\n        download: Whether to download model if not present\n\n    Returns:\n        (model, tokenizer, max_length)\n    ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def collate_fn(batch: List[Dict]) -&gt; Dict:",
            "lineNumber": 279,
            "isSignature": true
          },
          {
            "text": ",",
            "lineNumber": 313
          },
          {
            "text": "        \"study_name\": study_names,",
            "lineNumber": 314
          },
          {
            "text": "        \"sample_id\": sample_ids,",
            "lineNumber": 315
          },
          {
            "text": "    }",
            "lineNumber": 316,
            "isSignature": true
          },
          {
            "lineNumber": 317
          },
          {
            "lineNumber": 318
          },
          {
            "text": "def load_dataset_from_hf(",
            "lineNumber": 319,
            "isSignature": true
          },
          {
            "text": "    dataset_repo: str = \"jedick/microbial-DNA-pH\",",
            "lineNumber": 320,
            "isSignature": true
          },
          {
            "text": "    filter_missing_ph: bool = True,",
            "lineNumber": 321,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[Dict]:",
            "lineNumber": 322,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 323
          },
          {
            "text": "    Load dataset from HuggingFace.",
            "lineNumber": 324
          },
          {
            "lineNumber": 325
          },
          {
            "text": "    Includes study_name and sample_id for subsetting by shared train/val/test splits.",
            "lineNumber": 326
          },
          {
            "text": "    HF dataset contains SRA samples only (SRR*, ERR*, DRR*).",
            "lineNumber": 327
          },
          {
            "lineNumber": 328
          },
          {
            "text": "    Args:",
            "lineNumber": 329
          },
          {
            "text": "        dataset_repo: HuggingFace dataset repository",
            "lineNumber": 330
          },
          {
            "text": "        filter_missing_ph: Whether to filter samples with missing pH",
            "lineNumber": 331
          },
          {
            "lineNumber": 332
          },
          {
            "text": "    Returns:",
            "lineNumber": 333
          },
          {
            "text": "        List of dicts with 'sequences', 'pH', 'study_name', 'sample_id', 'environment'",
            "lineNumber": 334
          },
          {
            "text": "        Sorted by sample_id for consistent ordering across workflows.",
            "lineNumber": 335
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 336
          },
          {
            "text": "    print(f\"Loading dataset from {dataset_repo}...\")",
            "lineNumber": 337
          },
          {
            "text": "    dataset = load_dataset(dataset_repo, split=\"train\")",
            "lineNumber": 338
          },
          {
            "lineNumber": 339
          },
          {
            "text": "    data = []",
            "lineNumber": 340
          },
          {
            "text": "    for item in dataset:",
            "lineNumber": 341
          },
          {
            "text": "        # Filter missing pH if requested",
            "lineNumber": 342
          },
          {
            "text": "        if filter_missing_ph and item[\"pH\"] is None:",
            "lineNumber": 343
          },
          {
            "text": "            continue",
            "lineNumber": 344
          },
          {
            "lineNumber": 345
          },
          {
            "text": "        data.append(",
            "lineNumber": 346
          },
          {
            "text": "            {",
            "lineNumber": 347
          },
          {
            "text": "                \"sequences\": item[\"sequences\"],",
            "lineNumber": 348
          },
          {
            "text": "                \"pH\": item[\"pH\"] if item[\"pH\"] is not None else 0.0,",
            "lineNumber": 349
          },
          {
            "text": "                \"study_name\": item[\"study_name\"],",
            "lineNumber": 350
          },
          {
            "text": "                \"sample_id\": item[\"sample_id\"],",
            "lineNumber": 351
          },
          {
            "text": "                \"environment\": item.get(\"environment\", \"unknown\"),",
            "lineNumber": 352
          },
          {
            "text": "            }",
            "lineNumber": 353
          },
          {
            "text": "        )",
            "lineNumber": 354
          },
          {
            "lineNumber": 355
          },
          {
            "text": "    # Sort by sample_id to ensure consistent ordering across workflows",
            "lineNumber": 356
          },
          {
            "text": "    data.sort(key=lambda x: x[\"sample_id\"])",
            "lineNumber": 357
          },
          {
            "lineNumber": 358
          },
          {
            "text": "    print(f\"Loaded {len(data)} samples\")",
            "lineNumber": 359
          },
          {
            "text": "    return data",
            "lineNumber": 360,
            "isSignature": true
          },
          {
            "lineNumber": 361
          },
          {
            "lineNumber": 362
          },
          {
            "text": "def setup_model(",
            "lineNumber": 363,
            "isSignature": true
          },
          {
            "text": "    model_name: str = \"hyenadna-tiny-1k-seqlen\",",
            "lineNumber": 364,
            "isSignature": true
          },
          {
            "text": "    checkpoint_dir: str = \"./checkpoints\",",
            "lineNumber": 365,
            "isSignature": true
          },
          {
            "text": "    head_architecture: str = \"mlp2\",",
            "lineNumber": 366,
            "isSignature": true
          },
          {
            "text": "    pooling_mode: str = \"pool\",",
            "lineNumber": 367,
            "isSignature": true
          },
          {
            "text": "    freeze_backbone: bool = False,",
            "lineNumber": 368,
            "isSignature": true
          },
          {
            "text": "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",",
            "lineNumber": 369,
            "isSignature": true
          },
          {
            "text": "    download: bool = False,",
            "lineNumber": 370,
            "isSignature": true
          },
          {
            "text": ") -&gt; Tuple[nn.Module, CharacterTokenizer, int]:",
            "lineNumber": 371,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 372
          },
          {
            "text": "    Load pretrained HyenaDNA model and attach regression head.",
            "lineNumber": 373
          },
          {
            "lineNumber": 374
          },
          {
            "text": "    Args:",
            "lineNumber": 375
          },
          {
            "text": "        model_name: Model name from MODEL_CONFIGS",
            "lineNumber": 376
          },
          {
            "text": "        checkpoint_dir: Directory for model checkpoints",
            "lineNumber": 377
          },
          {
            "text": "        head_architecture: Regression head architecture",
            "lineNumber": 378
          },
          {
            "text": "        pooling_mode: Pooling mode for regression head",
            "lineNumber": 379
          },
          {
            "text": "        freeze_backbone: Whether to freeze backbone weights",
            "lineNumber": 380
          },
          {
            "text": "        device: Device to use",
            "lineNumber": 381
          },
          {
            "text": "        download: Whether to download model if not present",
            "lineNumber": 382
          },
          {
            "lineNumber": 383
          },
          {
            "text": "    Returns:",
            "lineNumber": 384
          },
          {
            "text": "        (model, tokenizer, max_length)",
            "lineNumber": 385
          },
          {
            "text": "    ",
            "lineNumber": 386
          }
        ]
      },
      "score": 0.2322676032781601
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 359,
            "column": 15
          },
          "endPosition": {
            "line": 438
          }
        },
        "contents": "def setup_model(\n    model_name: str = \"hyenadna-tiny-1k-seqlen\",\n    checkpoint_dir: str = \"./checkpoints\",\n    head_architecture: str = \"mlp2\",\n    pooling_mode: str = \"pool\",\n    freeze_backbone: bool = False,\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    download: bool = False,\n) -&gt; Tuple[nn.Module, CharacterTokenizer, int]:\n    \"\"\"\n    Load pretrained HyenaDNA model and attach regression head.\n\n    Args:\n        model_name: Model name from MODEL_CONFIGS\n        checkpoint_dir: Directory for model checkpoints\n        head_architecture: Regression head architecture\n        pooling_mode: Pooling mode for regression head\n        freeze_backbone: Whether to freeze backbone weights\n        device: Device to use\n        download: Whether to download model if not present\n\n    Returns:\n        (model, tokenizer, max_length)\n    \"\"\"\n    if model_name not in MODEL_CONFIGS:\n        raise ValueError(\n            f\"Unknown model: {model_name}. \"\n            f\"Choose from: {list(MODEL_CONFIGS.keys())}\"\n        )\n\n    max_length = MODEL_CONFIGS[model_name][\"max_length\"]\n    print(f\"Loading model: {model_name} (max_length={max_length})...\")\n\n    # Load pretrained model (embeddings only, no head)",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 317,
                "column": 1
              },
              "endPosition": {
                "line": 326,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def setup_model(",
            "lineNumber": 363,
            "isSignature": true
          },
          {
            "text": "    model_name: str = \"hyenadna-tiny-1k-seqlen\",",
            "lineNumber": 364,
            "isSignature": true
          },
          {
            "text": "    checkpoint_dir: str = \"./checkpoints\",",
            "lineNumber": 365,
            "isSignature": true
          },
          {
            "text": "    head_architecture: str = \"mlp2\",",
            "lineNumber": 366,
            "isSignature": true
          },
          {
            "text": "    pooling_mode: str = \"pool\",",
            "lineNumber": 367,
            "isSignature": true
          },
          {
            "text": "    freeze_backbone: bool = False,",
            "lineNumber": 368,
            "isSignature": true
          },
          {
            "text": "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",",
            "lineNumber": 369,
            "isSignature": true
          },
          {
            "text": "    download: bool = False,",
            "lineNumber": 370,
            "isSignature": true
          },
          {
            "text": ") -&gt; Tuple[nn.Module, CharacterTokenizer, int]:",
            "lineNumber": 371,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 372
          },
          {
            "text": "    Load pretrained HyenaDNA model and attach regression head.",
            "lineNumber": 373
          },
          {
            "lineNumber": 374
          },
          {
            "text": "    Args:",
            "lineNumber": 375
          },
          {
            "text": "        model_name: Model name from MODEL_CONFIGS",
            "lineNumber": 376
          },
          {
            "text": "        checkpoint_dir: Directory for model checkpoints",
            "lineNumber": 377
          },
          {
            "text": "        head_architecture: Regression head architecture",
            "lineNumber": 378
          },
          {
            "text": "        pooling_mode: Pooling mode for regression head",
            "lineNumber": 379
          },
          {
            "text": "        freeze_backbone: Whether to freeze backbone weights",
            "lineNumber": 380
          },
          {
            "text": "        device: Device to use",
            "lineNumber": 381
          },
          {
            "text": "        download: Whether to download model if not present",
            "lineNumber": 382
          },
          {
            "lineNumber": 383
          },
          {
            "text": "    Returns:",
            "lineNumber": 384
          },
          {
            "text": "        (model, tokenizer, max_length)",
            "lineNumber": 385
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 386
          },
          {
            "text": "    if model_name not in MODEL_CONFIGS:",
            "lineNumber": 387
          },
          {
            "text": "        raise ValueError(",
            "lineNumber": 388
          },
          {
            "text": "            f\"Unknown model: {model_name}. \"",
            "lineNumber": 389
          },
          {
            "text": "            f\"Choose from: {list(MODEL_CONFIGS.keys())}\"",
            "lineNumber": 390
          },
          {
            "text": "        )",
            "lineNumber": 391
          },
          {
            "lineNumber": 392
          },
          {
            "text": "    max_length = MODEL_CONFIGS[model_name][\"max_length\"]",
            "lineNumber": 393
          },
          {
            "text": "    print(f\"Loading model: {model_name} (max_length={max_length})...\")",
            "lineNumber": 394
          },
          {
            "lineNumber": 395
          },
          {
            "text": "    # Load pretrained model (embeddings only, no head)",
            "lineNumber": 396
          }
        ]
      },
      "score": 0.22873389720916748
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---



---


Implementing 5 non-overlapping sequence sets per sample. Creating the initial todo list:


---

<tool-use data-tool-type="task" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [-] Create function to generate 5 non-overlapping sequence sets from a list of sequences
- [ ] Modify pHDataset to expand each sample into 5 samples (one per set)
- [ ] Update test prediction saving to aggregate by sample_id with columns for each set (1-5) plus mean

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Implementing the changes: adding a function to generate 5 non-overlapping sequence sets, then updating the dataset and test prediction logic.


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>

</tool-use>

---


Updating the `pHDataset` class to expand each sample into 5 samples (one per set):


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>

</tool-use>

---


Updating the test prediction saving logic to aggregate by sample_id with columns for each set:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---





---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Updating the collate function to handle `set_idx`, then updating the test prediction saving logic:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>

</tool-use>

---


Fixing a bug in `split_sequences_into_sets`: the logic for moving to the next set is incorrect.


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Fixing the logic in `split_sequences_into_sets`:


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** • Edit file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="task" data-tool-name="todo_write">
<details><summary>Tool use: **todo_write** • Todo List</summary>

- [x] Create function to generate 5 non-overlapping sequence sets from a list of sequences
- [x] Modify pHDataset to expand each sample into 5 samples (one per set)
- [x] Update test prediction saving to aggregate by sample_id with columns for each set (1-5) plus mean

</details>
</tool-use>

---


Checking for linter errors:


---

<tool-use data-tool-type="generic" data-tool-name="read_lints">
<details>
          <summary>Tool use: **read_lints** • Read lints for 1 file</summary>
      
Lint paths:

- `train_hyenadna_ph.py`

Lint results

Lint errors for `train_hyenadna_ph.py`


| Message | Range | Severity |
|-------|-------|----------|
| Import "huggingface_wrapper" could not be resolved | 31:6-31:25 | DIAGNOSTIC_SEVERITY_WARNING |
| Import "standalone_hyenadna" could not be resolved | 32:6-32:25 | DIAGNOSTIC_SEVERITY_WARNING |


</details>
</tool-use>

---


The linter warnings are about local imports and can be ignored. Verifying the implementation by checking a few key sections:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Verifying the test prediction aggregation logic:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---


## Summary of Changes

1. **Added `split_sequences_into_sets` function**: Splits sequences into 5 non-overlapping sets, each fitting within `max_length`. Sequences are distributed across sets without overlap.

2. **Modified `pHDataset` class**:
   - Added `num_sets` parameter (default: 5)
   - Expands each original sample into up to 5 samples (one per set)
   - Each expanded sample tracks its `set_idx` (1-5)
   - The dataset is now 5x larger (or less if a sample doesn't have enough sequences)

3. **Updated `collate_fn`**: Handles `set_idx` in batches.

4. **Updated test prediction saving**:
   - Collects predictions with their `set_idx`
   - Aggregates by `sample_id` to create one row per sample
   - Adds columns: `predicted_ph_set1` through `predicted_ph_set5`, `predicted_ph_mean`, and `residual_mean`
   - Missing sets are filled with NaN
   - The output has the same number of rows (one per unique `sample_id`) with additional columns

The training, validation, and test datasets are expanded by up to 5x, and test predictions include per-set predictions plus the mean for each sample. The implementation handles cases where a sample has fewer than 5 sets (e.g., not enough sequences).

---

_**User**_

This is working. Can you make one more change: from 75:5:20 to 70:10:20 for the train:val:test split.

---

