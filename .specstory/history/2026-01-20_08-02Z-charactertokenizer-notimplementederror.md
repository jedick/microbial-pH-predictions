<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 9b99053c-c1a0-4f69-b947-85751edff448 (2026-01-20 08:02Z) -->

# CharacterTokenizer NotImplementedError (2026-01-20 08:02Z)

_**User**_

I'm getting an error when running inference_single in @hyena-dna/inference.py .

In [2]: inference_single()
Using device: cpu
Loaded pretrained weights ok!
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
Cell In[2], line 1
----> 1 inference_single()

File ~/github/ML-predictions-of-pH/hyena-dna/inference.py:79, in inference_single()
     76     model = HyenaDNAModel(**backbone_cfg, use_head=use_head, n_classes=n_classes)
     78 # create tokenizer
---> 79 tokenizer = CharacterTokenizer(
     80     characters=["A", "C", "G", "T", "N"],  # add DNA characters, N is uncertain
     81     model_max_length=max_length + 2,  # to account for special tokens, like EOS
     82     # add_special_tokens argument is incompatible with newer tokenizers:
     83     # AttributeError: add_special_tokens conflicts with the method add_special_tokens in CharacterTokenizer
     84     # add_special_tokens=False,  # we handle special tokens elsewhere
     85     padding_side="left",  # since HyenaDNA is causal, we pad on the left
     86 )
     88 #### Single embedding example ####
     89 
     90 # create a sample 450k long, prepare
     91 sequence = "ACTG" * int(max_length / 4)

File ~/github/ML-predictions-of-pH/hyena-dna/standalone_hyenadna.py:1185, in CharacterTokenizer.__init__(self, characters, model_max_length, padding_side, **kwargs)
   1181 unk_token = AddedToken("[UNK]", lstrip=False, rstrip=False)
   1183 mask_token = AddedToken("[MASK]", lstrip=True, rstrip=False)
-> 1185 super().__init__(
   1186     bos_token=bos_token,
   1187     eos_token=sep_token,
   1188     sep_token=sep_token,
   1189     cls_token=cls_token,
   1190     pad_token=pad_token,
   1191     mask_token=mask_token,
   1192     unk_token=unk_token,
   1193     add_prefix_space=False,
   1194     model_max_length=model_max_length,
   1195     padding_side=padding_side,
   1196     **kwargs,
   1197 )
   1199 self._vocab_str_to_int = {
   1200     "[CLS]": 0,
   1201     "[SEP]": 1,
   (...)   1207     **{ch: i + 7 for i, ch in enumerate(characters)},
   1208 }
   1209 self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}

File ~/.local/lib/python3.12/site-packages/transformers/tokenization_utils.py:438, in PreTrainedTokenizer.__init__(self, **kwargs)
    434 super().__init__(**kwargs)
    436 # 4. If some of the special tokens are not part of the vocab, we add them, at the end.
    437 # the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`
--> 438 self._add_tokens(
    439     [token for token in self.all_special_tokens_extended if token not in self._added_tokens_encoder],
    440     special_tokens=True,
    441 )
    443 self._decode_use_source_tokenizer = False

File ~/.local/lib/python3.12/site-packages/transformers/tokenization_utils.py:546, in PreTrainedTokenizer._add_tokens(self, new_tokens, special_tokens)
    544     return added_tokens
    545 # TODO this is fairly slow to improve!
--> 546 current_vocab = self.get_vocab().copy()
    547 new_idx = len(current_vocab)  # only call this once, len gives the last index + 1
    548 for token in new_tokens:

File ~/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1525, in PreTrainedTokenizerBase.get_vocab(self)
   1515 def get_vocab(self) -> Dict[str, int]:
   1516     """
   1517     Returns the vocabulary as a dictionary of token to index.
   1518 
   (...)   1523         `Dict[str, int]`: The vocabulary.
   1524     """
-> 1525     raise NotImplementedError()

NotImplementedError: 

I think the code needs to be updated for the current version of transformers/tokenizers. Can you debug this problem and suggest a fix?

---

_**Agent (model default, mode Agent)**_


Examining the relevant files to understand the issue.


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/ML-predictions-of-pH/hyena-dna/inference.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** • Read file: /home/jedick/github/ML-predictions-of-pH/hyena-dna/standalone_hyenadna.py</summary>

</details>
</tool-use>

---

