<!-- Generated by SpecStory, Markdown v2.1.0 -->

<!-- cursor Session 735017c2-dd22-43f3-86fd-92116f58f6ac (2026-01-27 13:05Z) -->

# HyenaDNA test prediction script (2026-01-27 13:05Z)

_**User**_

I'd like a new script to run the test predictions for the HyenaDNA model. Can you move the code starting at around @train_hyenadna_ph.py:965 to a function in a separate script? The new script should be able to be run standalone with arguments for the checkpoint file (e.g. best_model.pt) and the output file (e.g. test_predictions.csv). Try to minimize code duplication -  modify @train_hyenadna_ph.py to call the new function rather than duplicate the code.

---

_**Agent (model default, mode Agent)**_


Examining the code around line 965 to identify what to extract.


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Creating a new script for test predictions and updating the training script to use it. Checking what information is saved in the checkpoint:


---

<tool-use data-tool-type="unknown" data-tool-name="ripgrep_raw_search">
<details>
<summary>Tool use: **ripgrep_raw_search**</summary>


Parameters:

```json
{
  "pattern": "checkpoint.*=.*\\{",
  "path": "/home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py",
  "contextAfter": 5,
  "caseInsensitive": false
}
```
Result:

```json
{
  "success": {
    "pattern": "checkpoint.*=.*\\{",
    "path": "/home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py",
    "outputMode": "content",
    "workspaceResults": {
      "/home/jedick/github/microbial-pH-predictions": {
        "content": {
          "matches": [
            {
              "file": "train_hyenadna_ph.py",
              "matches": [
                {
                  "lineNumber": 695,
                  "content": "    checkpoint = {"
                },
                {
                  "lineNumber": 696,
                  "content": "        \"epoch\": epoch,",
                  "isContextLine": true
                },
                {
                  "lineNumber": 697,
                  "content": "        \"model_state_dict\": model.state_dict(),",
                  "isContextLine": true
                },
                {
                  "lineNumber": 698,
                  "content": "        \"metrics\": metrics,",
                  "isContextLine": true
                },
                {
                  "lineNumber": 699,
                  "content": "    }",
                  "isContextLine": true
                },
                {
                  "lineNumber": 700,
                  "isContextLine": true
                },
                {
                  "lineNumber": 702,
                  "content": "    checkpoint_path = output_dir / f\"checkpoint_epoch_{epoch}.pt\""
                },
                {
                  "lineNumber": 703,
                  "content": "    torch.save(checkpoint, checkpoint_path)",
                  "isContextLine": true
                },
                {
                  "lineNumber": 704,
                  "isContextLine": true
                },
                {
                  "lineNumber": 705,
                  "content": "    # Save best model",
                  "isContextLine": true
                },
                {
                  "lineNumber": 706,
                  "content": "    if is_best:",
                  "isContextLine": true
                },
                {
                  "lineNumber": 707,
                  "content": "        best_path = output_dir / \"best_model.pt\"",
                  "isContextLine": true
                }
              ]
            }
          ],
          "totalLines": 12,
          "totalMatchedLines": 2
        }
      }
    }
  }
}
```
Status: **completed**


</details>
</tool-use>

---

<tool-use data-tool-type="unknown" data-tool-name="semantic_search_full">
<details>
<summary>Tool use: **semantic_search_full**</summary>


Parameters:

```json
{
  "query": "How are model checkpoints saved and what information is stored in them?",
  "topK": 10,
  "explanation": ""
}
```
Result:

```json
{
  "codeResults": [
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 495,
            "column": 15
          },
          "endPosition": {
            "line": 574
          }
        },
        "contents": "def setup_model(\n    model_name: str = \"hyenadna-tiny-1k-seqlen\",\n    checkpoint_dir: str = \"./checkpoints\",\n    head_architecture: str = \"mlp2\",\n    pooling_mode: str = \"pool\",\n    freeze_backbone: bool = False,\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    download: bool = False,\n) -&gt; Tuple[nn.Module, CharacterTokenizer, int]:\n    \"\"\"\n    Load pretrained HyenaDNA model and attach regression head.\n\n    Args:\n        model_name: Model name from MODEL_CONFIGS\n        checkpoint_dir: Directory for model checkpoints\n        head_architecture: Regression head architecture\n        pooling_mode: Pooling mode for regression head\n        freeze_backbone: Whether to freeze backbone weights\n        device: Device to use\n        download: Whether to download model if not present\n\n    Returns:\n        (model, tokenizer, max_length)\n    \"\"\"\n    if model_name not in MODEL_CONFIGS:\n        raise ValueError(\n            f\"Unknown model: {model_name}. \"\n            f\"Choose from: {list(MODEL_CONFIGS.keys())}\"\n        )\n\n    max_length = MODEL_CONFIGS[model_name][\"max_length\"]",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def setup_model(",
            "lineNumber": 499,
            "isSignature": true
          },
          {
            "text": "    model_name: str = \"hyenadna-tiny-1k-seqlen\",",
            "lineNumber": 500,
            "isSignature": true
          },
          {
            "text": "    checkpoint_dir: str = \"./checkpoints\",",
            "lineNumber": 501,
            "isSignature": true
          },
          {
            "text": "    head_architecture: str = \"mlp2\",",
            "lineNumber": 502,
            "isSignature": true
          },
          {
            "text": "    pooling_mode: str = \"pool\",",
            "lineNumber": 503,
            "isSignature": true
          },
          {
            "text": "    freeze_backbone: bool = False,",
            "lineNumber": 504,
            "isSignature": true
          },
          {
            "text": "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",",
            "lineNumber": 505,
            "isSignature": true
          },
          {
            "text": "    download: bool = False,",
            "lineNumber": 506,
            "isSignature": true
          },
          {
            "text": ") -&gt; Tuple[nn.Module, CharacterTokenizer, int]:",
            "lineNumber": 507,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 508
          },
          {
            "text": "    Load pretrained HyenaDNA model and attach regression head.",
            "lineNumber": 509
          },
          {
            "lineNumber": 510
          },
          {
            "text": "    Args:",
            "lineNumber": 511
          },
          {
            "text": "        model_name: Model name from MODEL_CONFIGS",
            "lineNumber": 512
          },
          {
            "text": "        checkpoint_dir: Directory for model checkpoints",
            "lineNumber": 513
          },
          {
            "text": "        head_architecture: Regression head architecture",
            "lineNumber": 514
          },
          {
            "text": "        pooling_mode: Pooling mode for regression head",
            "lineNumber": 515
          },
          {
            "text": "        freeze_backbone: Whether to freeze backbone weights",
            "lineNumber": 516
          },
          {
            "text": "        device: Device to use",
            "lineNumber": 517
          },
          {
            "text": "        download: Whether to download model if not present",
            "lineNumber": 518
          },
          {
            "lineNumber": 519
          },
          {
            "text": "    Returns:",
            "lineNumber": 520
          },
          {
            "text": "        (model, tokenizer, max_length)",
            "lineNumber": 521
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 522
          },
          {
            "text": "    if model_name not in MODEL_CONFIGS:",
            "lineNumber": 523
          },
          {
            "text": "        raise ValueError(",
            "lineNumber": 524
          },
          {
            "text": "            f\"Unknown model: {model_name}. \"",
            "lineNumber": 525
          },
          {
            "text": "            f\"Choose from: {list(MODEL_CONFIGS.keys())}\"",
            "lineNumber": 526
          },
          {
            "text": "        )",
            "lineNumber": 527
          },
          {
            "lineNumber": 528
          },
          {
            "text": "    max_length = MODEL_CONFIGS[model_name][\"max_length\"]",
            "lineNumber": 529
          }
        ]
      },
      "score": 0.42732319235801697
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 686
          },
          "endPosition": {
            "line": 1070
          }
        },
        "contents": "def save_checkpoint(\n    model: nn.Module,\n    epoch: int,\n    metrics: Dict[str, float],\n    output_dir: Path,\n    is_best: bool = False,\n):\n\n        print(f\"Saved best model to {best_path}\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Train HyenaDNA for pH prediction\")\n\n    # Model arguments\n    parser.add_argument(\n        \"--model-name\",\n        type=str,\n        default=\"hyenadna-tiny-1k-seqlen\",\n        choices=list(MODEL_CONFIGS.keys()),\n        help=\"HyenaDNA model name\",\n    )\n    parser.add_argument(\n        \"--head-architecture\",\n        type=str,\n        default=\"mlp2\",\n        choices=[\"linear\", \"mlp2\", \"mlp3\", \"mlp4_high\", \"mlp4_low\", \"mlp5\", \"mlp2_ln\"],\n        help=\"Regression head architecture\",\n    )\n    parser.add_argument(\n        \"--pooling-mode\",\n        type=str,\n        default=\"pool\",\n        choices=[\"pool\", \"last\", \"first\", \"sum\"],\n        help=\"Pooling mode for regression head\",\n    )\n    parser.add_argument(\n        \"--freeze-backbone\",\n        action=\"store_true\",\n        help=\"Freeze backbone weights (only train head)\",\n    )\n\n    # Training arguments\n    parser.add_argument(\"--batch-size\", type=int, default=4, help=\"Batch size\")\n    parser.add_argument(\n        \"--learning-rate\", type=float, default=1e-4, help=\"Learning rate\"\n    )\n    parser.add_argument(\"--num-epochs\", type=int, default=10, help=\"Number of epochs\")\n    parser.add_argument(\n        \"--loss-function\",\n        type=str,\n        default=\"smooth_l1\",\n        choices=[\"mse\", \"mae\", \"smooth_l1\", \"huber\"],\n        help=\"Loss function\",\n    )\n    parser.add_argument(\n        \"--weight-decay\", type=float, default=0.01, help=\"Weight decay for AdamW\"\n    )\n\n    # Data arguments\n    parser.add_argument(\n        \"--dataset-repo\",\n        type=str,\n        default=\"jedick/microbial-DNA-pH\",\n        help=\"HuggingFace dataset repository\",\n    )\n    parser.add_argument(\n        \"--random-seed\",\n        type=int,\n        default=42,\n        help=\"Random seed for reproducibility (default: 42)\",\n    )",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 530,
                "column": 1
              },
              "endPosition": {
                "line": 531,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def save_checkpoint(",
            "lineNumber": 687,
            "isSignature": true
          },
          {
            "text": "    model: nn.Module,",
            "lineNumber": 688,
            "isSignature": true
          },
          {
            "text": "    epoch: int,",
            "lineNumber": 689,
            "isSignature": true
          },
          {
            "text": "    metrics: Dict[str, float],",
            "lineNumber": 690,
            "isSignature": true
          },
          {
            "text": "    output_dir: Path,",
            "lineNumber": 691,
            "isSignature": true
          },
          {
            "text": "    is_best: bool = False,",
            "lineNumber": 692,
            "isSignature": true
          },
          {
            "text": "):",
            "lineNumber": 693,
            "isSignature": true
          },
          {
            "lineNumber": 708
          },
          {
            "text": "        print(f\"Saved best model to {best_path}\")",
            "lineNumber": 709,
            "isSignature": true
          },
          {
            "lineNumber": 710
          },
          {
            "lineNumber": 711
          },
          {
            "text": "def main():",
            "lineNumber": 712,
            "isSignature": true
          },
          {
            "text": "    parser = argparse.ArgumentParser(description=\"Train HyenaDNA for pH prediction\")",
            "lineNumber": 713
          },
          {
            "lineNumber": 714
          },
          {
            "text": "    # Model arguments",
            "lineNumber": 715
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 716
          },
          {
            "text": "        \"--model-name\",",
            "lineNumber": 717
          },
          {
            "text": "        type=str,",
            "lineNumber": 718
          },
          {
            "text": "        default=\"hyenadna-tiny-1k-seqlen\",",
            "lineNumber": 719
          },
          {
            "text": "        choices=list(MODEL_CONFIGS.keys()),",
            "lineNumber": 720
          },
          {
            "text": "        help=\"HyenaDNA model name\",",
            "lineNumber": 721
          },
          {
            "text": "    )",
            "lineNumber": 722
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 723
          },
          {
            "text": "        \"--head-architecture\",",
            "lineNumber": 724
          },
          {
            "text": "        type=str,",
            "lineNumber": 725
          },
          {
            "text": "        default=\"mlp2\",",
            "lineNumber": 726
          },
          {
            "text": "        choices=[\"linear\", \"mlp2\", \"mlp3\", \"mlp4_high\", \"mlp4_low\", \"mlp5\", \"mlp2_ln\"],",
            "lineNumber": 727
          },
          {
            "text": "        help=\"Regression head architecture\",",
            "lineNumber": 728
          },
          {
            "text": "    )",
            "lineNumber": 729
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 730
          },
          {
            "text": "        \"--pooling-mode\",",
            "lineNumber": 731
          },
          {
            "text": "        type=str,",
            "lineNumber": 732
          },
          {
            "text": "        default=\"pool\",",
            "lineNumber": 733
          },
          {
            "text": "        choices=[\"pool\", \"last\", \"first\", \"sum\"],",
            "lineNumber": 734
          },
          {
            "text": "        help=\"Pooling mode for regression head\",",
            "lineNumber": 735
          },
          {
            "text": "    )",
            "lineNumber": 736
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 737
          },
          {
            "text": "        \"--freeze-backbone\",",
            "lineNumber": 738
          },
          {
            "text": "        action=\"store_true\",",
            "lineNumber": 739
          },
          {
            "text": "        help=\"Freeze backbone weights (only train head)\",",
            "lineNumber": 740
          },
          {
            "text": "    )",
            "lineNumber": 741
          },
          {
            "lineNumber": 742
          },
          {
            "text": "    # Training arguments",
            "lineNumber": 743
          },
          {
            "text": "    parser.add_argument(\"--batch-size\", type=int, default=4, help=\"Batch size\")",
            "lineNumber": 744
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 745
          },
          {
            "text": "        \"--learning-rate\", type=float, default=1e-4, help=\"Learning rate\"",
            "lineNumber": 746
          },
          {
            "text": "    )",
            "lineNumber": 747
          },
          {
            "text": "    parser.add_argument(\"--num-epochs\", type=int, default=10, help=\"Number of epochs\")",
            "lineNumber": 748
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 749
          },
          {
            "text": "        \"--loss-function\",",
            "lineNumber": 750
          },
          {
            "text": "        type=str,",
            "lineNumber": 751
          },
          {
            "text": "        default=\"smooth_l1\",",
            "lineNumber": 752
          },
          {
            "text": "        choices=[\"mse\", \"mae\", \"smooth_l1\", \"huber\"],",
            "lineNumber": 753
          },
          {
            "text": "        help=\"Loss function\",",
            "lineNumber": 754
          },
          {
            "text": "    )",
            "lineNumber": 755
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 756
          },
          {
            "text": "        \"--weight-decay\", type=float, default=0.01, help=\"Weight decay for AdamW\"",
            "lineNumber": 757
          },
          {
            "text": "    )",
            "lineNumber": 758
          },
          {
            "lineNumber": 759
          },
          {
            "text": "    # Data arguments",
            "lineNumber": 760
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 761
          },
          {
            "text": "        \"--dataset-repo\",",
            "lineNumber": 762
          },
          {
            "text": "        type=str,",
            "lineNumber": 763
          },
          {
            "text": "        default=\"jedick/microbial-DNA-pH\",",
            "lineNumber": 764
          },
          {
            "text": "        help=\"HuggingFace dataset repository\",",
            "lineNumber": 765
          },
          {
            "text": "    )",
            "lineNumber": 766
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 767
          },
          {
            "text": "        \"--random-seed\",",
            "lineNumber": 768
          },
          {
            "text": "        type=int,",
            "lineNumber": 769
          },
          {
            "text": "        default=42,",
            "lineNumber": 770
          },
          {
            "text": "        help=\"Random seed for reproducibility (default: 42)\",",
            "lineNumber": 771
          },
          {
            "text": "    )",
            "lineNumber": 772
          }
        ]
      },
      "score": 0.30739665031433105
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/huggingface_wrapper.py",
        "range": {
          "startPosition": {
            "line": 69
          },
          "endPosition": {
            "line": 143,
            "column": 28
          }
        },
        "contents": "class HyenaDNAPreTrainedModel(PreTrainedModel):\n@classmethod\ndef from_pretrained(\n        cls,\n        path,\n        model_name,\n        download=False,\n        config=None,\n        device=\"cpu\",\n        use_head=False,\n        n_classes=2,\n    ):\n        # first check if it is a local path\n,\n        )\n\n        # need to load weights slightly different if using gradient checkpointing\n        if config.get(\"checkpoint_mixer\", False):\n            checkpointing = (\n                config[\"checkpoint_mixer\"] == True or config[\"checkpoint_mixer\"] == True\n            )\n        else:\n            checkpointing = False\n\n        # grab state dict from both and load weights\n        state_dict = load_weights(\n            scratch_model.state_dict(),\n            loaded_ckpt[\"state_dict\"],\n            checkpointing=checkpointing,\n        )\n\n        # scratch model has now been updated\n        scratch_model.load_state_dict(state_dict)\n        print(\"Loaded pretrained weights ok!\")\n        return scratch_model",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 70,
                "column": 1
              },
              "endPosition": {
                "line": 71,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 84,
                "column": 5
              },
              "endPosition": {
                "line": 85,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 85,
                "column": 5
              },
              "endPosition": {
                "line": 96,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class HyenaDNAPreTrainedModel(PreTrainedModel):",
            "lineNumber": 70,
            "isSignature": true
          },
          {
            "text": "@classmethod",
            "lineNumber": 84
          },
          {
            "text": "def from_pretrained(",
            "lineNumber": 85
          },
          {
            "text": "        cls,",
            "lineNumber": 86
          },
          {
            "text": "        path,",
            "lineNumber": 87
          },
          {
            "text": "        model_name,",
            "lineNumber": 88
          },
          {
            "text": "        download=False,",
            "lineNumber": 89
          },
          {
            "text": "        config=None,",
            "lineNumber": 90
          },
          {
            "text": "        device=\"cpu\",",
            "lineNumber": 91
          },
          {
            "text": "        use_head=False,",
            "lineNumber": 92
          },
          {
            "text": "        n_classes=2,",
            "lineNumber": 93
          },
          {
            "text": "    ):",
            "lineNumber": 94
          },
          {
            "text": "        # first check if it is a local path",
            "lineNumber": 95
          },
          {
            "text": ",",
            "lineNumber": 123
          },
          {
            "text": "        )",
            "lineNumber": 124
          },
          {
            "lineNumber": 125
          },
          {
            "text": "        # need to load weights slightly different if using gradient checkpointing",
            "lineNumber": 126
          },
          {
            "text": "        if config.get(\"checkpoint_mixer\", False):",
            "lineNumber": 127
          },
          {
            "text": "            checkpointing = (",
            "lineNumber": 128
          },
          {
            "text": "                config[\"checkpoint_mixer\"] == True or config[\"checkpoint_mixer\"] == True",
            "lineNumber": 129
          },
          {
            "text": "            )",
            "lineNumber": 130
          },
          {
            "text": "        else:",
            "lineNumber": 131
          },
          {
            "text": "            checkpointing = False",
            "lineNumber": 132
          },
          {
            "lineNumber": 133
          },
          {
            "text": "        # grab state dict from both and load weights",
            "lineNumber": 134
          },
          {
            "text": "        state_dict = load_weights(",
            "lineNumber": 135
          },
          {
            "text": "            scratch_model.state_dict(),",
            "lineNumber": 136
          },
          {
            "text": "            loaded_ckpt[\"state_dict\"],",
            "lineNumber": 137
          },
          {
            "text": "            checkpointing=checkpointing,",
            "lineNumber": 138
          },
          {
            "text": "        )",
            "lineNumber": 139
          },
          {
            "lineNumber": 140
          },
          {
            "text": "        # scratch model has now been updated",
            "lineNumber": 141
          },
          {
            "text": "        scratch_model.load_state_dict(state_dict)",
            "lineNumber": 142
          },
          {
            "text": "        print(\"Loaded pretrained weights ok!\")",
            "lineNumber": 143
          },
          {
            "text": "        return scratch_model",
            "lineNumber": 144,
            "isSignature": true
          }
        ]
      },
      "score": 0.29466015100479126
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/huggingface_wrapper.py",
        "range": {
          "startPosition": {
            "line": 66,
            "column": 23
          },
          "endPosition": {
            "line": 143
          }
        },
        "contents": "class HyenaDNAPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    base_model_prefix = \"hyenadna\"\n\n    def __init__(self, config):\n        pass\n\n    def forward(self, input_ids, **kwargs):\n        return self.model(input_ids, **kwargs)\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        path,\n        model_name,\n        download=False,\n        config=None,\n        device=\"cpu\",\n        use_head=False,\n        n_classes=2,\n    ):\n        # first check if it is a local path\n        pretrained_model_name_or_path = os.path.join(path, model_name)\n        if os.path.isdir(pretrained_model_name_or_path) and download == False:\n            if config is None:\n                config = json.load(\n                    open(os.path.join(pretrained_model_name_or_path, \"config.json\"))\n                )\n        else:\n            hf_url = f\"https://huggingface.co/LongSafari/{model_name}\"\n\n            subprocess.run(f\"rm -rf {pretrained_model_name_or_path}\", shell=True)\n            command = (\n                f\"mkdir -p {path} &amp;&amp; cd {path} &amp;&amp; git lfs install &amp;&amp; git clone {hf_url}\"\n            )\n            subprocess.run(command, shell=True)\n\n            if config is None:\n                config = json.load(\n                    open(os.path.join(pretrained_model_name_or_path, \"config.json\"))\n                )\n\n        scratch_model = HyenaDNAModel(\n            **config, use_head=use_head, n_classes=n_classes\n        )  # the new model format\n        loaded_ckpt = torch.load(\n            os.path.join(pretrained_model_name_or_path, \"weights.ckpt\"),\n            map_location=torch.device(device),\n            # Needed for PyTorch &gt;= 2.6\n            weights_only=False,\n        )\n\n        # need to load weights slightly different if using gradient checkpointing",
        "signatures": {},
        "detailedLines": [
          {
            "text": "class HyenaDNAPreTrainedModel(PreTrainedModel):",
            "lineNumber": 70,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 71
          },
          {
            "text": "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained",
            "lineNumber": 72
          },
          {
            "text": "    models.",
            "lineNumber": 73
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 74
          },
          {
            "lineNumber": 75
          },
          {
            "text": "    base_model_prefix = \"hyenadna\"",
            "lineNumber": 76
          },
          {
            "lineNumber": 77
          },
          {
            "text": "    def __init__(self, config):",
            "lineNumber": 78
          },
          {
            "text": "        pass",
            "lineNumber": 79
          },
          {
            "lineNumber": 80
          },
          {
            "text": "    def forward(self, input_ids, **kwargs):",
            "lineNumber": 81
          },
          {
            "text": "        return self.model(input_ids, **kwargs)",
            "lineNumber": 82
          },
          {
            "lineNumber": 83
          },
          {
            "text": "    @classmethod",
            "lineNumber": 84
          },
          {
            "text": "    def from_pretrained(",
            "lineNumber": 85
          },
          {
            "text": "        cls,",
            "lineNumber": 86
          },
          {
            "text": "        path,",
            "lineNumber": 87
          },
          {
            "text": "        model_name,",
            "lineNumber": 88
          },
          {
            "text": "        download=False,",
            "lineNumber": 89
          },
          {
            "text": "        config=None,",
            "lineNumber": 90
          },
          {
            "text": "        device=\"cpu\",",
            "lineNumber": 91
          },
          {
            "text": "        use_head=False,",
            "lineNumber": 92
          },
          {
            "text": "        n_classes=2,",
            "lineNumber": 93
          },
          {
            "text": "    ):",
            "lineNumber": 94
          },
          {
            "text": "        # first check if it is a local path",
            "lineNumber": 95
          },
          {
            "text": "        pretrained_model_name_or_path = os.path.join(path, model_name)",
            "lineNumber": 96
          },
          {
            "text": "        if os.path.isdir(pretrained_model_name_or_path) and download == False:",
            "lineNumber": 97
          },
          {
            "text": "            if config is None:",
            "lineNumber": 98
          },
          {
            "text": "                config = json.load(",
            "lineNumber": 99
          },
          {
            "text": "                    open(os.path.join(pretrained_model_name_or_path, \"config.json\"))",
            "lineNumber": 100
          },
          {
            "text": "                )",
            "lineNumber": 101
          },
          {
            "text": "        else:",
            "lineNumber": 102
          },
          {
            "text": "            hf_url = f\"https://huggingface.co/LongSafari/{model_name}\"",
            "lineNumber": 103
          },
          {
            "lineNumber": 104
          },
          {
            "text": "            subprocess.run(f\"rm -rf {pretrained_model_name_or_path}\", shell=True)",
            "lineNumber": 105
          },
          {
            "text": "            command = (",
            "lineNumber": 106
          },
          {
            "text": "                f\"mkdir -p {path} &amp;&amp; cd {path} &amp;&amp; git lfs install &amp;&amp; git clone {hf_url}\"",
            "lineNumber": 107
          },
          {
            "text": "            )",
            "lineNumber": 108
          },
          {
            "text": "            subprocess.run(command, shell=True)",
            "lineNumber": 109
          },
          {
            "lineNumber": 110
          },
          {
            "text": "            if config is None:",
            "lineNumber": 111
          },
          {
            "text": "                config = json.load(",
            "lineNumber": 112
          },
          {
            "text": "                    open(os.path.join(pretrained_model_name_or_path, \"config.json\"))",
            "lineNumber": 113
          },
          {
            "text": "                )",
            "lineNumber": 114
          },
          {
            "lineNumber": 115
          },
          {
            "text": "        scratch_model = HyenaDNAModel(",
            "lineNumber": 116
          },
          {
            "text": "            **config, use_head=use_head, n_classes=n_classes",
            "lineNumber": 117
          },
          {
            "text": "        )  # the new model format",
            "lineNumber": 118
          },
          {
            "text": "        loaded_ckpt = torch.load(",
            "lineNumber": 119
          },
          {
            "text": "            os.path.join(pretrained_model_name_or_path, \"weights.ckpt\"),",
            "lineNumber": 120
          },
          {
            "text": "            map_location=torch.device(device),",
            "lineNumber": 121
          },
          {
            "text": "            # Needed for PyTorch &gt;= 2.6",
            "lineNumber": 122
          },
          {
            "text": "            weights_only=False,",
            "lineNumber": 123
          },
          {
            "text": "        )",
            "lineNumber": 124
          },
          {
            "lineNumber": 125
          },
          {
            "text": "        # need to load weights slightly different if using gradient checkpointing",
            "lineNumber": 126
          }
        ]
      },
      "score": 0.28962281346321106
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 272
          },
          "endPosition": {
            "line": 409
          }
        },
        "contents": "def split_sequences_into_sets(\n    sequences: List[str],\n    max_length: int,\n    num_sets: int = 5,\n) -&gt; List[List[str]]:\n\n\n            if cumulative_length + tokens_needed &lt;= max_length:\n                current_set.append(seq)\n                cumulative_length += tokens_needed\n                sequence_idx += 1\n            else:\n                # This sequence would exceed max_length for current set\n                # Move to next set\n                break\n\n        # Move to next set (either we filled this one or can't add more)\n        current_set_idx += 1\n\n        # If all sequences processed, we're done\n        if sequence_idx &gt;= len(valid_sequences):\n            break\n\n    return sets\n\n\ndef concatenate_sequences(\n    sequences: List[str],\n    tokenizer: CharacterTokenizer,\n    max_length: int,\n    sep_token_id: int = SEP_TOKEN_ID,\n) -&gt; List[int]:\n    \"\"\"\n    Concatenate multiple DNA sequences with [SEP] tokens.\n\n    Args:\n        sequences: List of DNA sequence strings\n        tokenizer: CharacterTokenizer instance\n        max_length: Maximum total length (including special tokens)\n        sep_token_id: [SEP] token ID\n\n    Returns:\n        List of token IDs of length &lt;= max_length\n    \"\"\"\n    if not sequences:\n        # Return single [SEP] token if no sequences\n        return [sep_token_id]\n\n    # Filter out empty sequences first\n    valid_sequences = [seq for seq in sequences if seq and len(seq) &gt; 0]\n\n    if not valid_sequences:\n        # No valid sequences, return single [SEP]\n        return [sep_token_id]\n\n    # Precompute how many sequences we need based on character lengths\n    # For character tokenizer, each DNA character maps to one token (after filtering special tokens)\n    # We need to account for: sequence lengths + (n-1) SEP tokens for n sequences\n    cumulative_length = 0\n    sequences_to_use = []\n\n    for seq in valid_sequences:\n        seq_len = len(seq)\n        # Estimate tokens needed: seq_len characters + 1 SEP token (if not first sequence)\n        tokens_needed = seq_len + (1 if sequences_to_use else 0)\n\n        if cumulative_length + tokens_needed &lt;= max_length:",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def split_sequences_into_sets(",
            "lineNumber": 273,
            "isSignature": true
          },
          {
            "text": "    sequences: List[str],",
            "lineNumber": 274,
            "isSignature": true
          },
          {
            "text": "    max_length: int,",
            "lineNumber": 275,
            "isSignature": true
          },
          {
            "text": "    num_sets: int = 5,",
            "lineNumber": 276,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[List[str]]:",
            "lineNumber": 277,
            "isSignature": true
          },
          {
            "lineNumber": 313
          },
          {
            "lineNumber": 314
          },
          {
            "text": "            if cumulative_length + tokens_needed &lt;= max_length:",
            "lineNumber": 315
          },
          {
            "text": "                current_set.append(seq)",
            "lineNumber": 316
          },
          {
            "text": "                cumulative_length += tokens_needed",
            "lineNumber": 317
          },
          {
            "text": "                sequence_idx += 1",
            "lineNumber": 318
          },
          {
            "text": "            else:",
            "lineNumber": 319
          },
          {
            "text": "                # This sequence would exceed max_length for current set",
            "lineNumber": 320
          },
          {
            "text": "                # Move to next set",
            "lineNumber": 321
          },
          {
            "text": "                break",
            "lineNumber": 322
          },
          {
            "lineNumber": 323
          },
          {
            "text": "        # Move to next set (either we filled this one or can't add more)",
            "lineNumber": 324
          },
          {
            "text": "        current_set_idx += 1",
            "lineNumber": 325
          },
          {
            "lineNumber": 326
          },
          {
            "text": "        # If all sequences processed, we're done",
            "lineNumber": 327
          },
          {
            "text": "        if sequence_idx &gt;= len(valid_sequences):",
            "lineNumber": 328
          },
          {
            "text": "            break",
            "lineNumber": 329
          },
          {
            "lineNumber": 330
          },
          {
            "text": "    return sets",
            "lineNumber": 331,
            "isSignature": true
          },
          {
            "lineNumber": 332
          },
          {
            "lineNumber": 333
          },
          {
            "text": "def concatenate_sequences(",
            "lineNumber": 334,
            "isSignature": true
          },
          {
            "text": "    sequences: List[str],",
            "lineNumber": 335,
            "isSignature": true
          },
          {
            "text": "    tokenizer: CharacterTokenizer,",
            "lineNumber": 336,
            "isSignature": true
          },
          {
            "text": "    max_length: int,",
            "lineNumber": 337,
            "isSignature": true
          },
          {
            "text": "    sep_token_id: int = SEP_TOKEN_ID,",
            "lineNumber": 338,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[int]:",
            "lineNumber": 339,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 340
          },
          {
            "text": "    Concatenate multiple DNA sequences with [SEP] tokens.",
            "lineNumber": 341
          },
          {
            "lineNumber": 342
          },
          {
            "text": "    Args:",
            "lineNumber": 343
          },
          {
            "text": "        sequences: List of DNA sequence strings",
            "lineNumber": 344
          },
          {
            "text": "        tokenizer: CharacterTokenizer instance",
            "lineNumber": 345
          },
          {
            "text": "        max_length: Maximum total length (including special tokens)",
            "lineNumber": 346
          },
          {
            "text": "        sep_token_id: [SEP] token ID",
            "lineNumber": 347
          },
          {
            "lineNumber": 348
          },
          {
            "text": "    Returns:",
            "lineNumber": 349
          },
          {
            "text": "        List of token IDs of length &lt;= max_length",
            "lineNumber": 350
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 351
          },
          {
            "text": "    if not sequences:",
            "lineNumber": 352
          },
          {
            "text": "        # Return single [SEP] token if no sequences",
            "lineNumber": 353
          },
          {
            "text": "        return [sep_token_id]",
            "lineNumber": 354
          },
          {
            "lineNumber": 355
          },
          {
            "text": "    # Filter out empty sequences first",
            "lineNumber": 356
          },
          {
            "text": "    valid_sequences = [seq for seq in sequences if seq and len(seq) &gt; 0]",
            "lineNumber": 357
          },
          {
            "lineNumber": 358
          },
          {
            "text": "    if not valid_sequences:",
            "lineNumber": 359
          },
          {
            "text": "        # No valid sequences, return single [SEP]",
            "lineNumber": 360
          },
          {
            "text": "        return [sep_token_id]",
            "lineNumber": 361
          },
          {
            "lineNumber": 362
          },
          {
            "text": "    # Precompute how many sequences we need based on character lengths",
            "lineNumber": 363
          },
          {
            "text": "    # For character tokenizer, each DNA character maps to one token (after filtering special tokens)",
            "lineNumber": 364
          },
          {
            "text": "    # We need to account for: sequence lengths + (n-1) SEP tokens for n sequences",
            "lineNumber": 365
          },
          {
            "text": "    cumulative_length = 0",
            "lineNumber": 366
          },
          {
            "text": "    sequences_to_use = []",
            "lineNumber": 367
          },
          {
            "lineNumber": 368
          },
          {
            "text": "    for seq in valid_sequences:",
            "lineNumber": 369
          },
          {
            "text": "        seq_len = len(seq)",
            "lineNumber": 370
          },
          {
            "text": "        # Estimate tokens needed: seq_len characters + 1 SEP token (if not first sequence)",
            "lineNumber": 371
          },
          {
            "text": "        tokens_needed = seq_len + (1 if sequences_to_use else 0)",
            "lineNumber": 372
          },
          {
            "lineNumber": 373
          },
          {
            "text": "        if cumulative_length + tokens_needed &lt;= max_length:",
            "lineNumber": 374
          }
        ]
      },
      "score": 0.2812095880508423
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/huggingface_wrapper.py",
        "range": {
          "startPosition": {
            "line": 31,
            "column": 26
          },
          "endPosition": {
            "line": 143
          }
        },
        "contents": "def load_weights(scratch_dict, pretrained_dict, checkpointing=False):\n    \"\"\"Loads pretrained (backbone only) weights into the scratch state dict.\n\n    scratch_dict: dict, a state dict from a newly initialized HyenaDNA model\n    pretrained_dict: dict, a state dict from the pretrained ckpt\n    checkpointing: bool, whether the gradient checkpoint flag was used in the\n    pretrained model ckpt. This slightly changes state dict keys, so we patch\n    that if used.\n\n    return:\n    dict, a state dict with the pretrained weights loaded (head is scratch)\n\n    # loop thru state dict of scratch\n    # find the corresponding weights in the loaded model, and set it\n\n    \"\"\"\n\n    # need to do some state dict \"surgery\"\n    for key, value in scratch_dict.items():\n        if \"backbone\" in key:\n            # the state dicts differ by one prefix, '.model', so we add that\n            key_loaded = \"model.\" + key\n            # breakpoint()\n            # need to add an extra \".layer\" in key\n            if checkpointing:\n                key_loaded = inject_substring(key_loaded)\n            try:\n                scratch_dict[key] = pretrained_dict[key_loaded]\n            except:\n                raise Exception(\"key mismatch in the state dicts!\")\n\n    # scratch_dict has been updated\n    return scratch_dict\n\n\nclass HyenaDNAPreTrainedModel(PreTrainedModel):\n    \"\"\"",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def load_weights(scratch_dict, pretrained_dict, checkpointing=False):",
            "lineNumber": 35,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Loads pretrained (backbone only) weights into the scratch state dict.",
            "lineNumber": 36
          },
          {
            "lineNumber": 37
          },
          {
            "text": "    scratch_dict: dict, a state dict from a newly initialized HyenaDNA model",
            "lineNumber": 38
          },
          {
            "text": "    pretrained_dict: dict, a state dict from the pretrained ckpt",
            "lineNumber": 39
          },
          {
            "text": "    checkpointing: bool, whether the gradient checkpoint flag was used in the",
            "lineNumber": 40
          },
          {
            "text": "    pretrained model ckpt. This slightly changes state dict keys, so we patch",
            "lineNumber": 41
          },
          {
            "text": "    that if used.",
            "lineNumber": 42
          },
          {
            "lineNumber": 43
          },
          {
            "text": "    return:",
            "lineNumber": 44
          },
          {
            "text": "    dict, a state dict with the pretrained weights loaded (head is scratch)",
            "lineNumber": 45
          },
          {
            "lineNumber": 46
          },
          {
            "text": "    # loop thru state dict of scratch",
            "lineNumber": 47
          },
          {
            "text": "    # find the corresponding weights in the loaded model, and set it",
            "lineNumber": 48
          },
          {
            "lineNumber": 49
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 50
          },
          {
            "lineNumber": 51
          },
          {
            "text": "    # need to do some state dict \"surgery\"",
            "lineNumber": 52
          },
          {
            "text": "    for key, value in scratch_dict.items():",
            "lineNumber": 53
          },
          {
            "text": "        if \"backbone\" in key:",
            "lineNumber": 54
          },
          {
            "text": "            # the state dicts differ by one prefix, '.model', so we add that",
            "lineNumber": 55
          },
          {
            "text": "            key_loaded = \"model.\" + key",
            "lineNumber": 56
          },
          {
            "text": "            # breakpoint()",
            "lineNumber": 57
          },
          {
            "text": "            # need to add an extra \".layer\" in key",
            "lineNumber": 58
          },
          {
            "text": "            if checkpointing:",
            "lineNumber": 59
          },
          {
            "text": "                key_loaded = inject_substring(key_loaded)",
            "lineNumber": 60
          },
          {
            "text": "            try:",
            "lineNumber": 61
          },
          {
            "text": "                scratch_dict[key] = pretrained_dict[key_loaded]",
            "lineNumber": 62
          },
          {
            "text": "            except:",
            "lineNumber": 63
          },
          {
            "text": "                raise Exception(\"key mismatch in the state dicts!\")",
            "lineNumber": 64
          },
          {
            "lineNumber": 65
          },
          {
            "text": "    # scratch_dict has been updated",
            "lineNumber": 66
          },
          {
            "text": "    return scratch_dict",
            "lineNumber": 67,
            "isSignature": true
          },
          {
            "lineNumber": 68
          },
          {
            "lineNumber": 69
          },
          {
            "text": "class HyenaDNAPreTrainedModel(PreTrainedModel):",
            "lineNumber": 70,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 71
          }
        ]
      },
      "score": 0.2783568501472473
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./README.md",
        "range": {
          "startPosition": {
            "line": 154
          },
          "endPosition": {
            "line": 182
          }
        },
        "contents": "python train_hyenadna_ph.py --freeze-backbone --download-model\n&#96;&#96;&#96;\n\n&lt;details&gt;\n&lt;summary&gt;Details&lt;/summary&gt;\n\nThe script implements a separate &#96;RegressionHead&#96; class that maintains separation from the HyenaDNA codebase. It loads DNA sequences from the HuggingFace dataset (&#96;jedick/microbial-DNA-pH&#96;), concatenates multiple sequences per sample with [SEP] tokens up to the model's maximum length, and fine-tunes pretrained HyenaDNA weights for pH prediction.\n\n- **Model options**: Supports all HyenaDNA model sizes (tiny-1k to large-1m-seqlen), default is tiny-1k-seqlen for testing\n- **Regression head architectures**: Linear, 2-layer MLP (default), 3-layer MLP, or 2-layer MLP with LayerNorm\n- **Pooling modes**: Mean pooling (default), last token, first token, or sum pooling\n- **Sequence handling**: Concatenates multiple DNA sequences per sample with [SEP] tokens, truncates to max_length if needed\n- **Training options**: Can freeze backbone (only train head) or fine-tune entire model, configurable loss functions (MSE, MAE, SmoothL1, Huber)\n- **Evaluation**: Computes MSE, RMSE, MAE, and RÂ² metrics on train/val/test splits\n- **Data split**: Creates a 75:5:20 train-val-test split using the same random seed (42) and test set as the traditional ML workflow\n\n**Output Files:**\n- &#96;best_model.pt&#96;: Best model checkpoint based on validation loss\n- &#96;training_log.json&#96;: Training history with metrics per epoch\n- &#96;test_predictions.csv&#96;: Test set predictions with true and predicted pH values\n- &#96;config.json&#96;: Training configuration used\n\n**Key Hyperparameters:**\n- Default batch size: 4 (adjust based on GPU memory and sequence length)\n- Default learning rate: 1e-4 (typical for fine-tuning)\n- Default loss: SmoothL1Loss (good balance between MSE and MAE)\n- Train/val/test split: 75:5:20 (same test set as traditional ML workflow)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "python train_hyenadna_ph.py --freeze-backbone --download-model",
            "lineNumber": 155
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 156
          },
          {
            "lineNumber": 157
          },
          {
            "text": "&lt;details&gt;",
            "lineNumber": 158
          },
          {
            "text": "&lt;summary&gt;Details&lt;/summary&gt;",
            "lineNumber": 159
          },
          {
            "lineNumber": 160
          },
          {
            "text": "The script implements a separate &#96;RegressionHead&#96; class that maintains separation from the HyenaDNA codebase. It loads DNA sequences from the HuggingFace dataset (&#96;jedick/microbial-DNA-pH&#96;), concatenates multiple sequences per sample with [SEP] tokens up to the model's maximum length, and fine-tunes pretrained HyenaDNA weights for pH prediction.",
            "lineNumber": 161
          },
          {
            "lineNumber": 162
          },
          {
            "text": "- **Model options**: Supports all HyenaDNA model sizes (tiny-1k to large-1m-seqlen), default is tiny-1k-seqlen for testing",
            "lineNumber": 163
          },
          {
            "text": "- **Regression head architectures**: Linear, 2-layer MLP (default), 3-layer MLP, or 2-layer MLP with LayerNorm",
            "lineNumber": 164
          },
          {
            "text": "- **Pooling modes**: Mean pooling (default), last token, first token, or sum pooling",
            "lineNumber": 165
          },
          {
            "text": "- **Sequence handling**: Concatenates multiple DNA sequences per sample with [SEP] tokens, truncates to max_length if needed",
            "lineNumber": 166
          },
          {
            "text": "- **Training options**: Can freeze backbone (only train head) or fine-tune entire model, configurable loss functions (MSE, MAE, SmoothL1, Huber)",
            "lineNumber": 167
          },
          {
            "text": "- **Evaluation**: Computes MSE, RMSE, MAE, and RÂ² metrics on train/val/test splits",
            "lineNumber": 168
          },
          {
            "text": "- **Data split**: Creates a 75:5:20 train-val-test split using the same random seed (42) and test set as the traditional ML workflow",
            "lineNumber": 169
          },
          {
            "lineNumber": 170
          },
          {
            "text": "**Output Files:**",
            "lineNumber": 171
          },
          {
            "text": "- &#96;best_model.pt&#96;: Best model checkpoint based on validation loss",
            "lineNumber": 172
          },
          {
            "text": "- &#96;training_log.json&#96;: Training history with metrics per epoch",
            "lineNumber": 173
          },
          {
            "text": "- &#96;test_predictions.csv&#96;: Test set predictions with true and predicted pH values",
            "lineNumber": 174
          },
          {
            "text": "- &#96;config.json&#96;: Training configuration used",
            "lineNumber": 175
          },
          {
            "lineNumber": 176
          },
          {
            "text": "**Key Hyperparameters:**",
            "lineNumber": 177
          },
          {
            "text": "- Default batch size: 4 (adjust based on GPU memory and sequence length)",
            "lineNumber": 178
          },
          {
            "text": "- Default learning rate: 1e-4 (typical for fine-tuning)",
            "lineNumber": 179
          },
          {
            "text": "- Default loss: SmoothL1Loss (good balance between MSE and MAE)",
            "lineNumber": 180
          },
          {
            "text": "- Train/val/test split: 75:5:20 (same test set as traditional ML workflow)",
            "lineNumber": 181
          }
        ]
      },
      "score": 0.2724061608314514
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/inference.py",
        "range": {
          "startPosition": {
            "line": 13
          },
          "endPosition": {
            "line": 104,
            "column": 47
          }
        },
        "contents": "def inference_single():\n\n    print(\"Using device:\", device)\n\n    # instantiate the model (pretrained here)\n    if pretrained_model_name in [\n        \"hyenadna-tiny-1k-seqlen\",\n        \"hyenadna-small-32k-seqlen\",\n        \"hyenadna-medium-160k-seqlen\",\n        \"hyenadna-medium-450k-seqlen\",\n        \"hyenadna-large-1m-seqlen\",\n    ]:\n        # use the pretrained Huggingface wrapper instead\n        model = HyenaDNAPreTrainedModel.from_pretrained(\n            \"./checkpoints\",\n            pretrained_model_name,\n            # Change to True to download the checkpoint\n            download=False,\n            config=backbone_cfg,\n            device=device,\n            use_head=use_head,\n            n_classes=n_classes,\n        )\n\n    # from scratch\n    elif pretrained_model_name is None:\n        model = HyenaDNAModel(**backbone_cfg, use_head=use_head, n_classes=n_classes)\n\n    # create tokenizer\n    tokenizer = CharacterTokenizer(\n        characters=[\"A\", \"C\", \"G\", \"T\", \"N\"],  # add DNA characters, N is uncertain\n        model_max_length=max_length + 2,  # to account for special tokens, like EOS\n        # add_special_tokens argument is incompatible with newer tokenizers:\n        # AttributeError: add_special_tokens conflicts with the method add_special_tokens in CharacterTokenizer\n        # add_special_tokens=False,  # we handle special tokens elsewhere\n        padding_side=\"left\",  # since HyenaDNA is causal, we pad on the left\n    )\n\n    #### Single embedding example ####\n\n    # create a sample 450k long, prepare\n    sequence = \"ACTG\" * int(max_length / 4)\n    tok_seq = tokenizer(sequence)\n    tok_seq = tok_seq[\"input_ids\"]  # grab ids\n\n    # place on device, convert to tensor\n    tok_seq = torch.LongTensor(tok_seq).unsqueeze(0)  # unsqueeze for batch dim\n    tok_seq = tok_seq.to(device)\n\n    # prep model and forward\n    model.to(device)\n    model.eval()\n    with torch.inference_mode():\n        embeddings = model(tok_seq)\n\n    print(embeddings.shape)  # embeddings here!",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 14,
                "column": 1
              },
              "endPosition": {
                "line": 15,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def inference_single():",
            "lineNumber": 14,
            "isSignature": true
          },
          {
            "lineNumber": 51
          },
          {
            "text": "    print(\"Using device:\", device)",
            "lineNumber": 52
          },
          {
            "lineNumber": 53
          },
          {
            "text": "    # instantiate the model (pretrained here)",
            "lineNumber": 54
          },
          {
            "text": "    if pretrained_model_name in [",
            "lineNumber": 55
          },
          {
            "text": "        \"hyenadna-tiny-1k-seqlen\",",
            "lineNumber": 56
          },
          {
            "text": "        \"hyenadna-small-32k-seqlen\",",
            "lineNumber": 57
          },
          {
            "text": "        \"hyenadna-medium-160k-seqlen\",",
            "lineNumber": 58
          },
          {
            "text": "        \"hyenadna-medium-450k-seqlen\",",
            "lineNumber": 59
          },
          {
            "text": "        \"hyenadna-large-1m-seqlen\",",
            "lineNumber": 60
          },
          {
            "text": "    ]:",
            "lineNumber": 61
          },
          {
            "text": "        # use the pretrained Huggingface wrapper instead",
            "lineNumber": 62
          },
          {
            "text": "        model = HyenaDNAPreTrainedModel.from_pretrained(",
            "lineNumber": 63
          },
          {
            "text": "            \"./checkpoints\",",
            "lineNumber": 64
          },
          {
            "text": "            pretrained_model_name,",
            "lineNumber": 65
          },
          {
            "text": "            # Change to True to download the checkpoint",
            "lineNumber": 66
          },
          {
            "text": "            download=False,",
            "lineNumber": 67
          },
          {
            "text": "            config=backbone_cfg,",
            "lineNumber": 68
          },
          {
            "text": "            device=device,",
            "lineNumber": 69
          },
          {
            "text": "            use_head=use_head,",
            "lineNumber": 70
          },
          {
            "text": "            n_classes=n_classes,",
            "lineNumber": 71
          },
          {
            "text": "        )",
            "lineNumber": 72
          },
          {
            "lineNumber": 73
          },
          {
            "text": "    # from scratch",
            "lineNumber": 74
          },
          {
            "text": "    elif pretrained_model_name is None:",
            "lineNumber": 75
          },
          {
            "text": "        model = HyenaDNAModel(**backbone_cfg, use_head=use_head, n_classes=n_classes)",
            "lineNumber": 76
          },
          {
            "lineNumber": 77
          },
          {
            "text": "    # create tokenizer",
            "lineNumber": 78
          },
          {
            "text": "    tokenizer = CharacterTokenizer(",
            "lineNumber": 79
          },
          {
            "text": "        characters=[\"A\", \"C\", \"G\", \"T\", \"N\"],  # add DNA characters, N is uncertain",
            "lineNumber": 80
          },
          {
            "text": "        model_max_length=max_length + 2,  # to account for special tokens, like EOS",
            "lineNumber": 81
          },
          {
            "text": "        # add_special_tokens argument is incompatible with newer tokenizers:",
            "lineNumber": 82
          },
          {
            "text": "        # AttributeError: add_special_tokens conflicts with the method add_special_tokens in CharacterTokenizer",
            "lineNumber": 83
          },
          {
            "text": "        # add_special_tokens=False,  # we handle special tokens elsewhere",
            "lineNumber": 84
          },
          {
            "text": "        padding_side=\"left\",  # since HyenaDNA is causal, we pad on the left",
            "lineNumber": 85
          },
          {
            "text": "    )",
            "lineNumber": 86
          },
          {
            "lineNumber": 87
          },
          {
            "text": "    #### Single embedding example ####",
            "lineNumber": 88
          },
          {
            "lineNumber": 89
          },
          {
            "text": "    # create a sample 450k long, prepare",
            "lineNumber": 90
          },
          {
            "text": "    sequence = \"ACTG\" * int(max_length / 4)",
            "lineNumber": 91
          },
          {
            "text": "    tok_seq = tokenizer(sequence)",
            "lineNumber": 92
          },
          {
            "text": "    tok_seq = tok_seq[\"input_ids\"]  # grab ids",
            "lineNumber": 93
          },
          {
            "lineNumber": 94
          },
          {
            "text": "    # place on device, convert to tensor",
            "lineNumber": 95
          },
          {
            "text": "    tok_seq = torch.LongTensor(tok_seq).unsqueeze(0)  # unsqueeze for batch dim",
            "lineNumber": 96
          },
          {
            "text": "    tok_seq = tok_seq.to(device)",
            "lineNumber": 97
          },
          {
            "lineNumber": 98
          },
          {
            "text": "    # prep model and forward",
            "lineNumber": 99
          },
          {
            "text": "    model.to(device)",
            "lineNumber": 100
          },
          {
            "text": "    model.eval()",
            "lineNumber": 101
          },
          {
            "text": "    with torch.inference_mode():",
            "lineNumber": 102
          },
          {
            "text": "        embeddings = model(tok_seq)",
            "lineNumber": 103
          },
          {
            "lineNumber": 104
          },
          {
            "text": "    print(embeddings.shape)  # embeddings here!",
            "lineNumber": 105,
            "isSignature": true
          }
        ]
      },
      "score": 0.266998291015625
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./predict_ph.py",
        "range": {
          "startPosition": {
            "line": 291
          },
          "endPosition": {
            "line": 510
          }
        },
        "contents": "def main():\n\n        pipeline.fit(X_train, y_train)\n\n    # Make predictions\n    print(\"Making predictions...\")\n    y_train_pred = pipeline.predict(X_train)\n    y_test_pred = pipeline.predict(X_test)\n\n    # Calculate and print metrics\n    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n    train_r2 = r2_score(y_train, y_train_pred)\n    test_r2 = r2_score(y_test, y_test_pred)\n    train_mae = mean_absolute_error(y_train, y_train_pred)\n    test_mae = mean_absolute_error(y_test, y_test_pred)\n\n    print(f\"\\n{'='*60}\")\n    print(f\"Model: {args.model.upper()}\")\n    print(f\"{'='*60}\")\n    print(f\"Train RMSE: {train_rmse:.4f}\")\n    print(f\"Test RMSE:  {test_rmse:.4f}\")\n    print(f\"Train R-squared:   {train_r2:.4f}\")\n    print(f\"Test R-squared:    {test_r2:.4f}\")\n    print(f\"Train MAE:  {train_mae:.4f}\")\n    print(f\"Test MAE:   {test_mae:.4f}\")\n    print(f\"{'='*60}\\n\")\n\n    # Save test set predictions only\n    test_output_path = output_dir / f\"{args.model}_test_predictions.csv\"\n    save_predictions(y_test, y_test_pred, test_metadata, test_output_path)\n\n    # Save model summary\n    summary_path = output_dir / f\"{args.model}_summary.txt\"\n    with open(summary_path, \"w\") as f:\n        f.write(f\"Model: {args.model.upper()}\\n\")\n        f.write(f\"Random Seed: {args.random_seed}\\n\")\n        f.write(f\"Number of features: {len(feature_names)}\\n\")\n        f.write(f\"Train set size: {X_train.shape[0]}\\n\")\n        f.write(f\"Test set size: {X_test.shape[0]}\\n\")\n        # Grid search information\n        if (\n            args.grid_search != \"none\"\n            and args.model != \"linear\"\n            and best_params is not None\n        ):\n            f.write(f\"\\nGrid Search:\\n\")\n            f.write(f\"  Mode: {args.grid_search}\\n\")\n            f.write(f\"  CV folds: {args.cv_folds}\\n\")\n            f.write(f\"  Number of combinations: {n_combinations}\\n\")",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 263,
                "column": 1
              },
              "endPosition": {
                "line": 264,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def main():",
            "lineNumber": 292,
            "isSignature": true
          },
          {
            "lineNumber": 432
          },
          {
            "text": "        pipeline.fit(X_train, y_train)",
            "lineNumber": 433
          },
          {
            "lineNumber": 434
          },
          {
            "text": "    # Make predictions",
            "lineNumber": 435
          },
          {
            "text": "    print(\"Making predictions...\")",
            "lineNumber": 436
          },
          {
            "text": "    y_train_pred = pipeline.predict(X_train)",
            "lineNumber": 437
          },
          {
            "text": "    y_test_pred = pipeline.predict(X_test)",
            "lineNumber": 438
          },
          {
            "lineNumber": 439
          },
          {
            "text": "    # Calculate and print metrics",
            "lineNumber": 440
          },
          {
            "text": "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error",
            "lineNumber": 441
          },
          {
            "lineNumber": 442
          },
          {
            "text": "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))",
            "lineNumber": 443
          },
          {
            "text": "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))",
            "lineNumber": 444
          },
          {
            "text": "    train_r2 = r2_score(y_train, y_train_pred)",
            "lineNumber": 445
          },
          {
            "text": "    test_r2 = r2_score(y_test, y_test_pred)",
            "lineNumber": 446
          },
          {
            "text": "    train_mae = mean_absolute_error(y_train, y_train_pred)",
            "lineNumber": 447
          },
          {
            "text": "    test_mae = mean_absolute_error(y_test, y_test_pred)",
            "lineNumber": 448
          },
          {
            "lineNumber": 449
          },
          {
            "text": "    print(f\"\\n{'='*60}\")",
            "lineNumber": 450
          },
          {
            "text": "    print(f\"Model: {args.model.upper()}\")",
            "lineNumber": 451
          },
          {
            "text": "    print(f\"{'='*60}\")",
            "lineNumber": 452
          },
          {
            "text": "    print(f\"Train RMSE: {train_rmse:.4f}\")",
            "lineNumber": 453
          },
          {
            "text": "    print(f\"Test RMSE:  {test_rmse:.4f}\")",
            "lineNumber": 454
          },
          {
            "text": "    print(f\"Train R-squared:   {train_r2:.4f}\")",
            "lineNumber": 455
          },
          {
            "text": "    print(f\"Test R-squared:    {test_r2:.4f}\")",
            "lineNumber": 456
          },
          {
            "text": "    print(f\"Train MAE:  {train_mae:.4f}\")",
            "lineNumber": 457
          },
          {
            "text": "    print(f\"Test MAE:   {test_mae:.4f}\")",
            "lineNumber": 458
          },
          {
            "text": "    print(f\"{'='*60}\\n\")",
            "lineNumber": 459
          },
          {
            "lineNumber": 460
          },
          {
            "text": "    # Save test set predictions only",
            "lineNumber": 461
          },
          {
            "text": "    test_output_path = output_dir / f\"{args.model}_test_predictions.csv\"",
            "lineNumber": 462
          },
          {
            "text": "    save_predictions(y_test, y_test_pred, test_metadata, test_output_path)",
            "lineNumber": 463
          },
          {
            "lineNumber": 464
          },
          {
            "text": "    # Save model summary",
            "lineNumber": 465
          },
          {
            "text": "    summary_path = output_dir / f\"{args.model}_summary.txt\"",
            "lineNumber": 466
          },
          {
            "text": "    with open(summary_path, \"w\") as f:",
            "lineNumber": 467
          },
          {
            "text": "        f.write(f\"Model: {args.model.upper()}\\n\")",
            "lineNumber": 468
          },
          {
            "text": "        f.write(f\"Random Seed: {args.random_seed}\\n\")",
            "lineNumber": 469
          },
          {
            "text": "        f.write(f\"Number of features: {len(feature_names)}\\n\")",
            "lineNumber": 470
          },
          {
            "text": "        f.write(f\"Train set size: {X_train.shape[0]}\\n\")",
            "lineNumber": 471
          },
          {
            "text": "        f.write(f\"Test set size: {X_test.shape[0]}\\n\")",
            "lineNumber": 472
          },
          {
            "text": "        # Grid search information",
            "lineNumber": 473
          },
          {
            "text": "        if (",
            "lineNumber": 474
          },
          {
            "text": "            args.grid_search != \"none\"",
            "lineNumber": 475
          },
          {
            "text": "            and args.model != \"linear\"",
            "lineNumber": 476
          },
          {
            "text": "            and best_params is not None",
            "lineNumber": 477
          },
          {
            "text": "        ):",
            "lineNumber": 478
          },
          {
            "text": "            f.write(f\"\\nGrid Search:\\n\")",
            "lineNumber": 479
          },
          {
            "text": "            f.write(f\"  Mode: {args.grid_search}\\n\")",
            "lineNumber": 480
          },
          {
            "text": "            f.write(f\"  CV folds: {args.cv_folds}\\n\")",
            "lineNumber": 481
          },
          {
            "text": "            f.write(f\"  Number of combinations: {n_combinations}\\n\")",
            "lineNumber": 482
          }
        ]
      },
      "score": 0.2629550099372864
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./results/hyenadna_ph/config.json",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 15,
            "column": 1
          }
        },
        "contents": "{\n  \"model_name\": \"hyenadna-small-32k-seqlen\",\n  \"head_architecture\": \"mlp4_low\",\n  \"pooling_mode\": \"pool\",\n  \"freeze_backbone\": false,\n  \"batch_size\": 2,\n  \"learning_rate\": 0.0001,\n  \"num_epochs\": 20,\n  \"loss_function\": \"smooth_l1\",\n  \"weight_decay\": 0.01,\n  \"dataset_repo\": \"jedick/microbial-DNA-pH\",\n  \"random_seed\": 42,\n  \"output_dir\": \"results/hyenadna_ph\",\n  \"checkpoint_dir\": \"./checkpoints\",\n  \"download_model\": false\n}",
        "signatures": {},
        "detailedLines": [
          {
            "text": "{",
            "lineNumber": 1
          },
          {
            "text": "  \"model_name\": \"hyenadna-small-32k-seqlen\",",
            "lineNumber": 2
          },
          {
            "text": "  \"head_architecture\": \"mlp4_low\",",
            "lineNumber": 3
          },
          {
            "text": "  \"pooling_mode\": \"pool\",",
            "lineNumber": 4
          },
          {
            "text": "  \"freeze_backbone\": false,",
            "lineNumber": 5
          },
          {
            "text": "  \"batch_size\": 2,",
            "lineNumber": 6
          },
          {
            "text": "  \"learning_rate\": 0.0001,",
            "lineNumber": 7
          },
          {
            "text": "  \"num_epochs\": 20,",
            "lineNumber": 8
          },
          {
            "text": "  \"loss_function\": \"smooth_l1\",",
            "lineNumber": 9
          },
          {
            "text": "  \"weight_decay\": 0.01,",
            "lineNumber": 10
          },
          {
            "text": "  \"dataset_repo\": \"jedick/microbial-DNA-pH\",",
            "lineNumber": 11
          },
          {
            "text": "  \"random_seed\": 42,",
            "lineNumber": 12
          },
          {
            "text": "  \"output_dir\": \"results/hyenadna_ph\",",
            "lineNumber": 13
          },
          {
            "text": "  \"checkpoint_dir\": \"./checkpoints\",",
            "lineNumber": 14
          },
          {
            "text": "  \"download_model\": false",
            "lineNumber": 15
          },
          {
            "text": "}",
            "lineNumber": 16
          }
        ]
      },
      "score": 0.26020848751068115
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./.gitignore",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 6
          }
        },
        "contents": "RDP_classifier\ncheckpoints\nhyenadna_ph\ncontent\nfasta\n.env",
        "signatures": {},
        "detailedLines": [
          {
            "text": "RDP_classifier",
            "lineNumber": 1
          },
          {
            "text": "checkpoints",
            "lineNumber": 2
          },
          {
            "text": "hyenadna_ph",
            "lineNumber": 3
          },
          {
            "text": "content",
            "lineNumber": 4
          },
          {
            "text": "fasta",
            "lineNumber": 5
          },
          {
            "text": ".env",
            "lineNumber": 6
          }
        ]
      },
      "score": 0.2403009533882141
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 577
          },
          "endPosition": {
            "line": 683
          }
        },
        "contents": "class HyenaDNAForRegression(nn.Module):\ndef forward(self, input_ids: torch.Tensor) -&gt; torch.Tensor:\n\n        Forward pass.\n\n        Args:\n            input_ids: (batch, seq_len) token IDs\n\n        Returns:\n            (batch,) pH predictions\n        \"\"\"\n        # Get embeddings from HyenaDNA model (when use_head=False, returns hidden_states)\n        hidden_states = self.hyenadna_model(input_ids)  # (batch, seq_len, d_model)\n\n        # Apply regression head\n        ph_pred = self.regression_head(hidden_states)  # (batch,)\n\n        return ph_pred\n\n\ndef train_epoch(\n    model: nn.Module,\n    dataloader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    criterion: nn.Module,\n    device: str,\n) -&gt; float:\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    total_loss = 0.0\n    num_batches = 0\n\n    for batch in tqdm(dataloader, desc=\"Training\"):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        ph_true = batch[\"ph\"].to(device)\n\n        # Forward pass\n        ph_pred = model(input_ids)\n\n        # Compute loss (only on non-padding tokens if using attention mask)\n        loss = criterion(ph_pred, ph_true)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        num_batches += 1\n\n    return total_loss / num_batches if num_batches &gt; 0 else 0.0\n\n\ndef evaluate(\n    model: nn.Module,\n    dataloader: DataLoader,\n    criterion: nn.Module,\n    device: str,\n) -&gt; Dict[str, float]:\n    \"\"\"Evaluate model.\"\"\"\n    model.eval()\n    all_preds = []\n    all_true = []\n    total_loss = 0.0\n    num_batches = 0\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            ph_true = batch[\"ph\"].to(device)\n\n            # Forward pass\n            ph_pred = model(input_ids)",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 530,
                "column": 1
              },
              "endPosition": {
                "line": 531,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class HyenaDNAForRegression(nn.Module):",
            "lineNumber": 578,
            "isSignature": true
          },
          {
            "text": "def forward(self, input_ids: torch.Tensor) -&gt; torch.Tensor:",
            "lineNumber": 586
          },
          {
            "lineNumber": 587
          },
          {
            "text": "        Forward pass.",
            "lineNumber": 588
          },
          {
            "lineNumber": 589
          },
          {
            "text": "        Args:",
            "lineNumber": 590
          },
          {
            "text": "            input_ids: (batch, seq_len) token IDs",
            "lineNumber": 591
          },
          {
            "lineNumber": 592
          },
          {
            "text": "        Returns:",
            "lineNumber": 593
          },
          {
            "text": "            (batch,) pH predictions",
            "lineNumber": 594
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 595
          },
          {
            "text": "        # Get embeddings from HyenaDNA model (when use_head=False, returns hidden_states)",
            "lineNumber": 596
          },
          {
            "text": "        hidden_states = self.hyenadna_model(input_ids)  # (batch, seq_len, d_model)",
            "lineNumber": 597
          },
          {
            "lineNumber": 598
          },
          {
            "text": "        # Apply regression head",
            "lineNumber": 599
          },
          {
            "text": "        ph_pred = self.regression_head(hidden_states)  # (batch,)",
            "lineNumber": 600
          },
          {
            "lineNumber": 601
          },
          {
            "text": "        return ph_pred",
            "lineNumber": 602,
            "isSignature": true
          },
          {
            "lineNumber": 603
          },
          {
            "lineNumber": 604
          },
          {
            "text": "def train_epoch(",
            "lineNumber": 605,
            "isSignature": true
          },
          {
            "text": "    model: nn.Module,",
            "lineNumber": 606,
            "isSignature": true
          },
          {
            "text": "    dataloader: DataLoader,",
            "lineNumber": 607,
            "isSignature": true
          },
          {
            "text": "    optimizer: torch.optim.Optimizer,",
            "lineNumber": 608,
            "isSignature": true
          },
          {
            "text": "    criterion: nn.Module,",
            "lineNumber": 609,
            "isSignature": true
          },
          {
            "text": "    device: str,",
            "lineNumber": 610,
            "isSignature": true
          },
          {
            "text": ") -&gt; float:",
            "lineNumber": 611,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Train for one epoch.\"\"\"",
            "lineNumber": 612
          },
          {
            "text": "    model.train()",
            "lineNumber": 613
          },
          {
            "text": "    total_loss = 0.0",
            "lineNumber": 614
          },
          {
            "text": "    num_batches = 0",
            "lineNumber": 615
          },
          {
            "lineNumber": 616
          },
          {
            "text": "    for batch in tqdm(dataloader, desc=\"Training\"):",
            "lineNumber": 617
          },
          {
            "text": "        input_ids = batch[\"input_ids\"].to(device)",
            "lineNumber": 618
          },
          {
            "text": "        attention_mask = batch[\"attention_mask\"].to(device)",
            "lineNumber": 619
          },
          {
            "text": "        ph_true = batch[\"ph\"].to(device)",
            "lineNumber": 620
          },
          {
            "lineNumber": 621
          },
          {
            "text": "        # Forward pass",
            "lineNumber": 622
          },
          {
            "text": "        ph_pred = model(input_ids)",
            "lineNumber": 623
          },
          {
            "lineNumber": 624
          },
          {
            "text": "        # Compute loss (only on non-padding tokens if using attention mask)",
            "lineNumber": 625
          },
          {
            "text": "        loss = criterion(ph_pred, ph_true)",
            "lineNumber": 626
          },
          {
            "lineNumber": 627
          },
          {
            "text": "        # Backward pass",
            "lineNumber": 628
          },
          {
            "text": "        optimizer.zero_grad()",
            "lineNumber": 629
          },
          {
            "text": "        loss.backward()",
            "lineNumber": 630
          },
          {
            "text": "        optimizer.step()",
            "lineNumber": 631
          },
          {
            "lineNumber": 632
          },
          {
            "text": "        total_loss += loss.item()",
            "lineNumber": 633
          },
          {
            "text": "        num_batches += 1",
            "lineNumber": 634
          },
          {
            "lineNumber": 635
          },
          {
            "text": "    return total_loss / num_batches if num_batches &gt; 0 else 0.0",
            "lineNumber": 636,
            "isSignature": true
          },
          {
            "lineNumber": 637
          },
          {
            "lineNumber": 638
          },
          {
            "text": "def evaluate(",
            "lineNumber": 639,
            "isSignature": true
          },
          {
            "text": "    model: nn.Module,",
            "lineNumber": 640,
            "isSignature": true
          },
          {
            "text": "    dataloader: DataLoader,",
            "lineNumber": 641,
            "isSignature": true
          },
          {
            "text": "    criterion: nn.Module,",
            "lineNumber": 642,
            "isSignature": true
          },
          {
            "text": "    device: str,",
            "lineNumber": 643,
            "isSignature": true
          },
          {
            "text": ") -&gt; Dict[str, float]:",
            "lineNumber": 644,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Evaluate model.\"\"\"",
            "lineNumber": 645
          },
          {
            "text": "    model.eval()",
            "lineNumber": 646
          },
          {
            "text": "    all_preds = []",
            "lineNumber": 647
          },
          {
            "text": "    all_true = []",
            "lineNumber": 648
          },
          {
            "text": "    total_loss = 0.0",
            "lineNumber": 649
          },
          {
            "text": "    num_batches = 0",
            "lineNumber": 650
          },
          {
            "lineNumber": 651
          },
          {
            "text": "    with torch.no_grad():",
            "lineNumber": 652
          },
          {
            "text": "        for batch in tqdm(dataloader, desc=\"Evaluating\"):",
            "lineNumber": 653
          },
          {
            "text": "            input_ids = batch[\"input_ids\"].to(device)",
            "lineNumber": 654
          },
          {
            "text": "            ph_true = batch[\"ph\"].to(device)",
            "lineNumber": 655
          },
          {
            "lineNumber": 656
          },
          {
            "text": "            # Forward pass",
            "lineNumber": 657
          },
          {
            "text": "            ph_pred = model(input_ids)",
            "lineNumber": 658
          }
        ]
      },
      "score": 0.22874459624290466
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 333
          },
          "endPosition": {
            "line": 409
          }
        },
        "contents": "def concatenate_sequences(\n    sequences: List[str],\n    tokenizer: CharacterTokenizer,\n    max_length: int,\n    sep_token_id: int = SEP_TOKEN_ID,\n) -&gt; List[int]:\n\n        tokens_needed = seq_len + (1 if sequences_to_use else 0)\n\n        if cumulative_length + tokens_needed &lt;= max_length:\n            sequences_to_use.append(seq)\n            cumulative_length += tokens_needed\n        else:\n            # This sequence would exceed max_length, stop here\n            break\n\n    if not sequences_to_use:\n        # Even the first sequence is too long, we'll truncate it\n        sequences_to_use = [valid_sequences[0]]\n\n    # Now tokenize only the sequences we'll use\n    tokenized_seqs = []\n    for seq in sequences_to_use:\n        # Tokenize and extract input_ids\n        tokens = tokenizer(seq)[\"input_ids\"]\n        # Remove special tokens (keep only DNA character tokens, which have IDs &gt;= 7)\n        dna_tokens = [t for t in tokens if t &gt;= 7]\n        if dna_tokens:\n            tokenized_seqs.append(dna_tokens)\n\n    if not tokenized_seqs:\n        # No valid tokens after filtering, return single [SEP]",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 317,
                "column": 1
              },
              "endPosition": {
                "line": 326,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def concatenate_sequences(",
            "lineNumber": 334,
            "isSignature": true
          },
          {
            "text": "    sequences: List[str],",
            "lineNumber": 335,
            "isSignature": true
          },
          {
            "text": "    tokenizer: CharacterTokenizer,",
            "lineNumber": 336,
            "isSignature": true
          },
          {
            "text": "    max_length: int,",
            "lineNumber": 337,
            "isSignature": true
          },
          {
            "text": "    sep_token_id: int = SEP_TOKEN_ID,",
            "lineNumber": 338,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[int]:",
            "lineNumber": 339,
            "isSignature": true
          },
          {
            "lineNumber": 371
          },
          {
            "text": "        tokens_needed = seq_len + (1 if sequences_to_use else 0)",
            "lineNumber": 372
          },
          {
            "lineNumber": 373
          },
          {
            "text": "        if cumulative_length + tokens_needed &lt;= max_length:",
            "lineNumber": 374
          },
          {
            "text": "            sequences_to_use.append(seq)",
            "lineNumber": 375
          },
          {
            "text": "            cumulative_length += tokens_needed",
            "lineNumber": 376
          },
          {
            "text": "        else:",
            "lineNumber": 377
          },
          {
            "text": "            # This sequence would exceed max_length, stop here",
            "lineNumber": 378
          },
          {
            "text": "            break",
            "lineNumber": 379
          },
          {
            "lineNumber": 380
          },
          {
            "text": "    if not sequences_to_use:",
            "lineNumber": 381
          },
          {
            "text": "        # Even the first sequence is too long, we'll truncate it",
            "lineNumber": 382
          },
          {
            "text": "        sequences_to_use = [valid_sequences[0]]",
            "lineNumber": 383
          },
          {
            "lineNumber": 384
          },
          {
            "text": "    # Now tokenize only the sequences we'll use",
            "lineNumber": 385
          },
          {
            "text": "    tokenized_seqs = []",
            "lineNumber": 386
          },
          {
            "text": "    for seq in sequences_to_use:",
            "lineNumber": 387
          },
          {
            "text": "        # Tokenize and extract input_ids",
            "lineNumber": 388
          },
          {
            "text": "        tokens = tokenizer(seq)[\"input_ids\"]",
            "lineNumber": 389
          },
          {
            "text": "        # Remove special tokens (keep only DNA character tokens, which have IDs &gt;= 7)",
            "lineNumber": 390
          },
          {
            "text": "        dna_tokens = [t for t in tokens if t &gt;= 7]",
            "lineNumber": 391
          },
          {
            "text": "        if dna_tokens:",
            "lineNumber": 392
          },
          {
            "text": "            tokenized_seqs.append(dna_tokens)",
            "lineNumber": 393
          },
          {
            "lineNumber": 394
          },
          {
            "text": "    if not tokenized_seqs:",
            "lineNumber": 395
          },
          {
            "text": "        # No valid tokens after filtering, return single [SEP]",
            "lineNumber": 396
          }
        ]
      },
      "score": 0.22524340450763702
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/standalone_hyenadna.py",
        "range": {
          "startPosition": {
            "line": 1027,
            "column": 39
          },
          "endPosition": {
            "line": 1129
          }
        },
        "contents": "# @title Model (backbone + head)\n\n\"\"\"\nPutting it all together, the model consists of a backbone model\nand a decoder head (you can turn off head for embeddings only too).\n\nHere we use a simple head to do multi-classification, but\ncan also swap the head to do next token prediction too.  We defer to the main\nHyenaDNA for that code, since pretraining with next token prediction isn't quite\nfeasible on colab.\n\n\"\"\"\n\n\nclass HyenaDNAModel(nn.Module):\n\n    def __init__(\n        self,\n        d_model: int,\n        n_layer: int,\n        d_inner: int,\n        vocab_size: int,\n        layer=None,\n        attn_layer_idx=None,\n        attn_cfg=None,\n        max_position_embeddings=0,\n        resid_dropout: float = 0.0,\n        embed_dropout: float = 0.1,\n        layer_norm_epsilon: float = 1e-5,\n        initializer_cfg=None,\n        residual_in_fp32=False,\n        pad_vocab_size_multiple: int = 1,\n        use_head=False,\n        n_classes: int = 2,\n        device=None,\n        dtype=None,\n        **kwargs,\n    ) -&gt; None:\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        if vocab_size % pad_vocab_size_multiple != 0:\n            vocab_size += pad_vocab_size_multiple - (\n                vocab_size % pad_vocab_size_multiple\n            )\n\n        self.use_head = use_head\n\n        # check if layer (config) has d_model (HF code differs from main Safari code)\n        if \"d_model\" not in layer:\n            layer[\"d_model\"] = d_model\n\n        self.backbone = LMBackbone(\n            d_model=d_model,",
        "signatures": {},
        "detailedLines": [
          {
            "text": "# @title Model (backbone + head)",
            "lineNumber": 1031
          },
          {
            "lineNumber": 1032
          },
          {
            "text": "\"\"\"",
            "lineNumber": 1033
          },
          {
            "text": "Putting it all together, the model consists of a backbone model",
            "lineNumber": 1034
          },
          {
            "text": "and a decoder head (you can turn off head for embeddings only too).",
            "lineNumber": 1035
          },
          {
            "lineNumber": 1036
          },
          {
            "text": "Here we use a simple head to do multi-classification, but",
            "lineNumber": 1037
          },
          {
            "text": "can also swap the head to do next token prediction too.  We defer to the main",
            "lineNumber": 1038
          },
          {
            "text": "HyenaDNA for that code, since pretraining with next token prediction isn't quite",
            "lineNumber": 1039
          },
          {
            "text": "feasible on colab.",
            "lineNumber": 1040
          },
          {
            "lineNumber": 1041
          },
          {
            "text": "\"\"\"",
            "lineNumber": 1042
          },
          {
            "lineNumber": 1043
          },
          {
            "lineNumber": 1044
          },
          {
            "text": "class HyenaDNAModel(nn.Module):",
            "lineNumber": 1045,
            "isSignature": true
          },
          {
            "lineNumber": 1046
          },
          {
            "text": "    def __init__(",
            "lineNumber": 1047
          },
          {
            "text": "        self,",
            "lineNumber": 1048
          },
          {
            "text": "        d_model: int,",
            "lineNumber": 1049
          },
          {
            "text": "        n_layer: int,",
            "lineNumber": 1050
          },
          {
            "text": "        d_inner: int,",
            "lineNumber": 1051
          },
          {
            "text": "        vocab_size: int,",
            "lineNumber": 1052
          },
          {
            "text": "        layer=None,",
            "lineNumber": 1053
          },
          {
            "text": "        attn_layer_idx=None,",
            "lineNumber": 1054
          },
          {
            "text": "        attn_cfg=None,",
            "lineNumber": 1055
          },
          {
            "text": "        max_position_embeddings=0,",
            "lineNumber": 1056
          },
          {
            "text": "        resid_dropout: float = 0.0,",
            "lineNumber": 1057
          },
          {
            "text": "        embed_dropout: float = 0.1,",
            "lineNumber": 1058
          },
          {
            "text": "        layer_norm_epsilon: float = 1e-5,",
            "lineNumber": 1059
          },
          {
            "text": "        initializer_cfg=None,",
            "lineNumber": 1060
          },
          {
            "text": "        residual_in_fp32=False,",
            "lineNumber": 1061
          },
          {
            "text": "        pad_vocab_size_multiple: int = 1,",
            "lineNumber": 1062
          },
          {
            "text": "        use_head=False,",
            "lineNumber": 1063
          },
          {
            "text": "        n_classes: int = 2,",
            "lineNumber": 1064
          },
          {
            "text": "        device=None,",
            "lineNumber": 1065
          },
          {
            "text": "        dtype=None,",
            "lineNumber": 1066
          },
          {
            "text": "        **kwargs,",
            "lineNumber": 1067
          },
          {
            "text": "    ) -&gt; None:",
            "lineNumber": 1068
          },
          {
            "text": "        factory_kwargs = {\"device\": device, \"dtype\": dtype}",
            "lineNumber": 1069
          },
          {
            "text": "        super().__init__()",
            "lineNumber": 1070
          },
          {
            "text": "        if vocab_size % pad_vocab_size_multiple != 0:",
            "lineNumber": 1071
          },
          {
            "text": "            vocab_size += pad_vocab_size_multiple - (",
            "lineNumber": 1072
          },
          {
            "text": "                vocab_size % pad_vocab_size_multiple",
            "lineNumber": 1073
          },
          {
            "text": "            )",
            "lineNumber": 1074
          },
          {
            "lineNumber": 1075
          },
          {
            "text": "        self.use_head = use_head",
            "lineNumber": 1076
          },
          {
            "lineNumber": 1077
          },
          {
            "text": "        # check if layer (config) has d_model (HF code differs from main Safari code)",
            "lineNumber": 1078
          },
          {
            "text": "        if \"d_model\" not in layer:",
            "lineNumber": 1079
          },
          {
            "text": "            layer[\"d_model\"] = d_model",
            "lineNumber": 1080
          },
          {
            "lineNumber": 1081
          },
          {
            "text": "        self.backbone = LMBackbone(",
            "lineNumber": 1082
          },
          {
            "text": "            d_model=d_model,",
            "lineNumber": 1083
          }
        ]
      },
      "score": 0.2209703028202057
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/standalone_hyenadna.py",
        "range": {
          "startPosition": {
            "line": 1044
          },
          "endPosition": {
            "line": 1139,
            "column": 3
          }
        },
        "contents": "class HyenaDNAModel(nn.Module):\ndef __init__(\n        self,\n        d_model: int,\n        n_layer: int,\n        d_inner: int,\n        vocab_size: int,\n        layer=None,\n        attn_layer_idx=None,\n        attn_cfg=None,\n        max_position_embeddings=0,\n        resid_dropout: float = 0.0,\n        embed_dropout: float = 0.1,\n        layer_norm_epsilon: float = 1e-5,\n        initializer_cfg=None,\n        residual_in_fp32=False,\n        pad_vocab_size_multiple: int = 1,\n        use_head=False,\n        n_classes: int = 2,\n        device=None,\n        dtype=None,\n        **kwargs,\n    ) -&gt; None:\n\n\n        self.backbone = LMBackbone(\n            d_model=d_model,\n            n_layer=n_layer,\n            d_inner=d_inner,\n            vocab_size=vocab_size,\n            layer=layer,\n            attn_layer_idx=attn_layer_idx,\n            attn_cfg=attn_cfg,\n            max_position_embeddings=max_position_embeddings,\n            resid_dropout=resid_dropout,\n            embed_dropout=embed_dropout,\n            layer_norm_epsilon=layer_norm_epsilon,\n            initializer_cfg=initializer_cfg,\n            residual_in_fp32=residual_in_fp32,\n            **factory_kwargs,\n            **kwargs,\n        )\n\n        # we only need a head if doing classification, otherwise we'll use the\n        # hidden states as embeddings\n        if self.use_head:\n            self.head = SequenceDecoder(\n                d_model=d_model, d_output=n_classes, l_output=0, mode=\"pool\"\n            )\n\n        # Initialize weights and apply final processing\n        self.apply(\n            partial(\n                _init_weights,\n                n_layer=n_layer,\n                **(initializer_cfg if initializer_cfg is not None else {}),\n            )\n        )\n\n        # if self.use_head:\n        #     self.tie_weights()\n\n    # def tie_weights(self):\n    #     self.head.weight = self.backbone.embeddings.word_embeddings.weight\n\n    def forward(\n        self, input_ids, position_ids=None, state=None\n    ):  # state for the repo interface\n        hidden_states = self.backbone(input_ids, position_ids=position_ids)\n\n        if self.use_head:\n            return self.head(hidden_states)\n        else:\n            return hidden_states\n\n\n\"\"\"# Data pipeline\n\n\n\"\"\"\n\n# @title Tokenizer\n\n\"\"\"",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 1045,
                "column": 1
              },
              "endPosition": {
                "line": 1047,
                "column": 5
              }
            },
            {
              "startPosition": {
                "line": 1047,
                "column": 5
              },
              "endPosition": {
                "line": 1069,
                "column": 9
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class HyenaDNAModel(nn.Module):",
            "lineNumber": 1045,
            "isSignature": true
          },
          {
            "text": "def __init__(",
            "lineNumber": 1047
          },
          {
            "text": "        self,",
            "lineNumber": 1048
          },
          {
            "text": "        d_model: int,",
            "lineNumber": 1049
          },
          {
            "text": "        n_layer: int,",
            "lineNumber": 1050
          },
          {
            "text": "        d_inner: int,",
            "lineNumber": 1051
          },
          {
            "text": "        vocab_size: int,",
            "lineNumber": 1052
          },
          {
            "text": "        layer=None,",
            "lineNumber": 1053
          },
          {
            "text": "        attn_layer_idx=None,",
            "lineNumber": 1054
          },
          {
            "text": "        attn_cfg=None,",
            "lineNumber": 1055
          },
          {
            "text": "        max_position_embeddings=0,",
            "lineNumber": 1056
          },
          {
            "text": "        resid_dropout: float = 0.0,",
            "lineNumber": 1057
          },
          {
            "text": "        embed_dropout: float = 0.1,",
            "lineNumber": 1058
          },
          {
            "text": "        layer_norm_epsilon: float = 1e-5,",
            "lineNumber": 1059
          },
          {
            "text": "        initializer_cfg=None,",
            "lineNumber": 1060
          },
          {
            "text": "        residual_in_fp32=False,",
            "lineNumber": 1061
          },
          {
            "text": "        pad_vocab_size_multiple: int = 1,",
            "lineNumber": 1062
          },
          {
            "text": "        use_head=False,",
            "lineNumber": 1063
          },
          {
            "text": "        n_classes: int = 2,",
            "lineNumber": 1064
          },
          {
            "text": "        device=None,",
            "lineNumber": 1065
          },
          {
            "text": "        dtype=None,",
            "lineNumber": 1066
          },
          {
            "text": "        **kwargs,",
            "lineNumber": 1067
          },
          {
            "text": "    ) -&gt; None:",
            "lineNumber": 1068
          },
          {
            "lineNumber": 1080
          },
          {
            "lineNumber": 1081
          },
          {
            "text": "        self.backbone = LMBackbone(",
            "lineNumber": 1082
          },
          {
            "text": "            d_model=d_model,",
            "lineNumber": 1083
          },
          {
            "text": "            n_layer=n_layer,",
            "lineNumber": 1084
          },
          {
            "text": "            d_inner=d_inner,",
            "lineNumber": 1085
          },
          {
            "text": "            vocab_size=vocab_size,",
            "lineNumber": 1086
          },
          {
            "text": "            layer=layer,",
            "lineNumber": 1087
          },
          {
            "text": "            attn_layer_idx=attn_layer_idx,",
            "lineNumber": 1088
          },
          {
            "text": "            attn_cfg=attn_cfg,",
            "lineNumber": 1089
          },
          {
            "text": "            max_position_embeddings=max_position_embeddings,",
            "lineNumber": 1090
          },
          {
            "text": "            resid_dropout=resid_dropout,",
            "lineNumber": 1091
          },
          {
            "text": "            embed_dropout=embed_dropout,",
            "lineNumber": 1092
          },
          {
            "text": "            layer_norm_epsilon=layer_norm_epsilon,",
            "lineNumber": 1093
          },
          {
            "text": "            initializer_cfg=initializer_cfg,",
            "lineNumber": 1094
          },
          {
            "text": "            residual_in_fp32=residual_in_fp32,",
            "lineNumber": 1095
          },
          {
            "text": "            **factory_kwargs,",
            "lineNumber": 1096
          },
          {
            "text": "            **kwargs,",
            "lineNumber": 1097
          },
          {
            "text": "        )",
            "lineNumber": 1098
          },
          {
            "lineNumber": 1099
          },
          {
            "text": "        # we only need a head if doing classification, otherwise we'll use the",
            "lineNumber": 1100
          },
          {
            "text": "        # hidden states as embeddings",
            "lineNumber": 1101
          },
          {
            "text": "        if self.use_head:",
            "lineNumber": 1102
          },
          {
            "text": "            self.head = SequenceDecoder(",
            "lineNumber": 1103
          },
          {
            "text": "                d_model=d_model, d_output=n_classes, l_output=0, mode=\"pool\"",
            "lineNumber": 1104
          },
          {
            "text": "            )",
            "lineNumber": 1105
          },
          {
            "lineNumber": 1106
          },
          {
            "text": "        # Initialize weights and apply final processing",
            "lineNumber": 1107
          },
          {
            "text": "        self.apply(",
            "lineNumber": 1108
          },
          {
            "text": "            partial(",
            "lineNumber": 1109
          },
          {
            "text": "                _init_weights,",
            "lineNumber": 1110
          },
          {
            "text": "                n_layer=n_layer,",
            "lineNumber": 1111
          },
          {
            "text": "                **(initializer_cfg if initializer_cfg is not None else {}),",
            "lineNumber": 1112
          },
          {
            "text": "            )",
            "lineNumber": 1113
          },
          {
            "text": "        )",
            "lineNumber": 1114
          },
          {
            "lineNumber": 1115
          },
          {
            "text": "        # if self.use_head:",
            "lineNumber": 1116
          },
          {
            "text": "        #     self.tie_weights()",
            "lineNumber": 1117
          },
          {
            "lineNumber": 1118
          },
          {
            "text": "    # def tie_weights(self):",
            "lineNumber": 1119
          },
          {
            "text": "    #     self.head.weight = self.backbone.embeddings.word_embeddings.weight",
            "lineNumber": 1120
          },
          {
            "lineNumber": 1121
          },
          {
            "text": "    def forward(",
            "lineNumber": 1122
          },
          {
            "text": "        self, input_ids, position_ids=None, state=None",
            "lineNumber": 1123
          },
          {
            "text": "    ):  # state for the repo interface",
            "lineNumber": 1124
          },
          {
            "text": "        hidden_states = self.backbone(input_ids, position_ids=position_ids)",
            "lineNumber": 1125
          },
          {
            "lineNumber": 1126
          },
          {
            "text": "        if self.use_head:",
            "lineNumber": 1127
          },
          {
            "text": "            return self.head(hidden_states)",
            "lineNumber": 1128
          },
          {
            "text": "        else:",
            "lineNumber": 1129
          },
          {
            "text": "            return hidden_states",
            "lineNumber": 1130,
            "isSignature": true
          },
          {
            "lineNumber": 1131
          },
          {
            "lineNumber": 1132
          },
          {
            "text": "\"\"\"# Data pipeline",
            "lineNumber": 1133
          },
          {
            "lineNumber": 1134
          },
          {
            "lineNumber": 1135
          },
          {
            "text": "\"\"\"",
            "lineNumber": 1136
          },
          {
            "lineNumber": 1137
          },
          {
            "text": "# @title Tokenizer",
            "lineNumber": 1138
          },
          {
            "lineNumber": 1139
          },
          {
            "text": "\"\"\"",
            "lineNumber": 1140
          }
        ]
      },
      "score": 0.21776129305362701
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 638
          },
          "endPosition": {
            "line": 1070
          }
        },
        "contents": "def evaluate(\n    model: nn.Module,\n    dataloader: DataLoader,\n    criterion: nn.Module,\n    device: str,\n) -&gt; Dict[str, float]:\n\n            ph_true = batch[\"ph\"].to(device)\n\n            # Forward pass\n            ph_pred = model(input_ids)\n\n            # Compute loss\n            loss = criterion(ph_pred, ph_true)\n\n            # Store predictions\n            all_preds.extend(ph_pred.cpu().numpy())\n            all_true.extend(ph_true.cpu().numpy())\n\n            total_loss += loss.item()\n            num_batches += 1\n\n    # Compute metrics\n    all_preds = np.array(all_preds)\n    all_true = np.array(all_true)\n\n    mse = mean_squared_error(all_true, all_preds)\n    mae = mean_absolute_error(all_true, all_preds)\n    r2 = r2_score(all_true, all_preds)\n\n    return {\n        \"loss\": total_loss / num_batches if num_batches &gt; 0 else 0.0,\n        \"mse\": mse,\n        \"rmse\": np.sqrt(mse),\n        \"mae\": mae,\n        \"r2\": r2,\n    }\n\n\ndef save_checkpoint(\n    model: nn.Module,\n    epoch: int,\n    metrics: Dict[str, float],\n    output_dir: Path,\n    is_best: bool = False,\n):\n    \"\"\"Save model checkpoint.\"\"\"\n    checkpoint = {\n        \"epoch\": epoch,\n        \"model_state_dict\": model.state_dict(),\n        \"metrics\": metrics,\n    }\n\n    # Save regular checkpoint\n    checkpoint_path = output_dir / f\"checkpoint_epoch_{epoch}.pt\"\n    torch.save(checkpoint, checkpoint_path)\n\n    # Save best model\n    if is_best:\n        best_path = output_dir / \"best_model.pt\"\n        torch.save(checkpoint, best_path)\n        print(f\"Saved best model to {best_path}\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Train HyenaDNA for pH prediction\")",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 530,
                "column": 1
              },
              "endPosition": {
                "line": 531,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def evaluate(",
            "lineNumber": 639,
            "isSignature": true
          },
          {
            "text": "    model: nn.Module,",
            "lineNumber": 640,
            "isSignature": true
          },
          {
            "text": "    dataloader: DataLoader,",
            "lineNumber": 641,
            "isSignature": true
          },
          {
            "text": "    criterion: nn.Module,",
            "lineNumber": 642,
            "isSignature": true
          },
          {
            "text": "    device: str,",
            "lineNumber": 643,
            "isSignature": true
          },
          {
            "text": ") -&gt; Dict[str, float]:",
            "lineNumber": 644,
            "isSignature": true
          },
          {
            "lineNumber": 654
          },
          {
            "text": "            ph_true = batch[\"ph\"].to(device)",
            "lineNumber": 655
          },
          {
            "lineNumber": 656
          },
          {
            "text": "            # Forward pass",
            "lineNumber": 657
          },
          {
            "text": "            ph_pred = model(input_ids)",
            "lineNumber": 658
          },
          {
            "lineNumber": 659
          },
          {
            "text": "            # Compute loss",
            "lineNumber": 660
          },
          {
            "text": "            loss = criterion(ph_pred, ph_true)",
            "lineNumber": 661
          },
          {
            "lineNumber": 662
          },
          {
            "text": "            # Store predictions",
            "lineNumber": 663
          },
          {
            "text": "            all_preds.extend(ph_pred.cpu().numpy())",
            "lineNumber": 664
          },
          {
            "text": "            all_true.extend(ph_true.cpu().numpy())",
            "lineNumber": 665
          },
          {
            "lineNumber": 666
          },
          {
            "text": "            total_loss += loss.item()",
            "lineNumber": 667
          },
          {
            "text": "            num_batches += 1",
            "lineNumber": 668
          },
          {
            "lineNumber": 669
          },
          {
            "text": "    # Compute metrics",
            "lineNumber": 670
          },
          {
            "text": "    all_preds = np.array(all_preds)",
            "lineNumber": 671
          },
          {
            "text": "    all_true = np.array(all_true)",
            "lineNumber": 672
          },
          {
            "lineNumber": 673
          },
          {
            "text": "    mse = mean_squared_error(all_true, all_preds)",
            "lineNumber": 674
          },
          {
            "text": "    mae = mean_absolute_error(all_true, all_preds)",
            "lineNumber": 675
          },
          {
            "text": "    r2 = r2_score(all_true, all_preds)",
            "lineNumber": 676
          },
          {
            "lineNumber": 677
          },
          {
            "text": "    return {",
            "lineNumber": 678
          },
          {
            "text": "        \"loss\": total_loss / num_batches if num_batches &gt; 0 else 0.0,",
            "lineNumber": 679
          },
          {
            "text": "        \"mse\": mse,",
            "lineNumber": 680
          },
          {
            "text": "        \"rmse\": np.sqrt(mse),",
            "lineNumber": 681
          },
          {
            "text": "        \"mae\": mae,",
            "lineNumber": 682
          },
          {
            "text": "        \"r2\": r2,",
            "lineNumber": 683
          },
          {
            "text": "    }",
            "lineNumber": 684,
            "isSignature": true
          },
          {
            "lineNumber": 685
          },
          {
            "lineNumber": 686
          },
          {
            "text": "def save_checkpoint(",
            "lineNumber": 687,
            "isSignature": true
          },
          {
            "text": "    model: nn.Module,",
            "lineNumber": 688,
            "isSignature": true
          },
          {
            "text": "    epoch: int,",
            "lineNumber": 689,
            "isSignature": true
          },
          {
            "text": "    metrics: Dict[str, float],",
            "lineNumber": 690,
            "isSignature": true
          },
          {
            "text": "    output_dir: Path,",
            "lineNumber": 691,
            "isSignature": true
          },
          {
            "text": "    is_best: bool = False,",
            "lineNumber": 692,
            "isSignature": true
          },
          {
            "text": "):",
            "lineNumber": 693,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Save model checkpoint.\"\"\"",
            "lineNumber": 694
          },
          {
            "text": "    checkpoint = {",
            "lineNumber": 695
          },
          {
            "text": "        \"epoch\": epoch,",
            "lineNumber": 696
          },
          {
            "text": "        \"model_state_dict\": model.state_dict(),",
            "lineNumber": 697
          },
          {
            "text": "        \"metrics\": metrics,",
            "lineNumber": 698
          },
          {
            "text": "    }",
            "lineNumber": 699
          },
          {
            "lineNumber": 700
          },
          {
            "text": "    # Save regular checkpoint",
            "lineNumber": 701
          },
          {
            "text": "    checkpoint_path = output_dir / f\"checkpoint_epoch_{epoch}.pt\"",
            "lineNumber": 702
          },
          {
            "text": "    torch.save(checkpoint, checkpoint_path)",
            "lineNumber": 703
          },
          {
            "lineNumber": 704
          },
          {
            "text": "    # Save best model",
            "lineNumber": 705
          },
          {
            "text": "    if is_best:",
            "lineNumber": 706
          },
          {
            "text": "        best_path = output_dir / \"best_model.pt\"",
            "lineNumber": 707
          },
          {
            "text": "        torch.save(checkpoint, best_path)",
            "lineNumber": 708
          },
          {
            "text": "        print(f\"Saved best model to {best_path}\")",
            "lineNumber": 709,
            "isSignature": true
          },
          {
            "lineNumber": 710
          },
          {
            "lineNumber": 711
          },
          {
            "text": "def main():",
            "lineNumber": 712,
            "isSignature": true
          },
          {
            "text": "    parser = argparse.ArgumentParser(description=\"Train HyenaDNA for pH prediction\")",
            "lineNumber": 713
          }
        ]
      },
      "score": 0.2171112298965454
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./README.md",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 34
          }
        },
        "contents": "# Microbial pH predictions\n\n## Project overview\n\nThis project aims to develop machine learning models for predicting pH from microbial abundances and 16S rRNA gene sequences. The approach incorporates both traditional machine learning methods (e.g., Random Forests and KNN regression) and deep learning techniques, with particular emphasis on language models for predictions from gene sequences.\n\n## Methods\n\n- **Traditional ML**: Random Forests, KNN regression, and other classical approaches\n- **Deep Learning**: Language models and other neural network architectures, especially for gene sequence-based predictions\n\n## SRA data download\n\nThe &#96;download_sra_data.py&#96; script downloads DNA sequence data from the NCBI Sequence Read Archive (SRA) for samples listed in &#96;sample_data.csv&#96;. The script downloads only the first 0.25MB of each sample to manage storage requirements while preserving sequence data for analysis.\n\n&#96;&#96;&#96;bash\npython download_sra_data.py\n&#96;&#96;&#96;\n\n&lt;details&gt;\n&lt;summary&gt;Details&lt;/summary&gt;\n\nThe script processes samples from &#96;sample_data.csv&#96; that have sample_ids matching the &#96;SRR*&#96;, &#96;ERR*&#96;, or &#96;DRR*&#96; pattern (NCBI SRA identifiers). For each sample, it downloads the first 0.25MB from the NCBI SRA fasta endpoint, processes the incomplete gzip file, and saves the result to &#96;data/fasta/{study_name}/{sample_id}.fasta.gz&#96;.\n\n- **Partial downloads**: Uses HTTP Range headers to download only the first 0.25MB, reducing storage requirements\n- **Incomplete gzip handling**: Extracts truncated gzip files using &#96;zcat&#96;, which can handle incomplete archives\n- **Sequence truncation protection**: Removes the last sequence from each file since it may be truncated due to the partial download\n- **Duplicate handling**: Automatically skips samples that have already been downloaded or appear multiple times in the CSV\n- **Pattern filtering**: Only processes sample_ids matching &#96;SRR*&#96;, &#96;ERR*&#96;, or &#96;DRR*&#96; patterns; other identifiers are skipped\n- **Progress tracking**: Shows current study_name and sample_id during processing with a summary at completion\n- **Cleanup on interruption**: Removes temporary files if the script is interrupted (Ctrl+C), preventing leftover files\n\n**Output Structure:**\n- Files are saved to &#96;data/fasta/{study_name}/{sample_id}.fasta.gz&#96;",
        "signatures": {},
        "detailedLines": [
          {
            "text": "# Microbial pH predictions",
            "lineNumber": 1
          },
          {
            "lineNumber": 2
          },
          {
            "text": "## Project overview",
            "lineNumber": 3
          },
          {
            "lineNumber": 4
          },
          {
            "text": "This project aims to develop machine learning models for predicting pH from microbial abundances and 16S rRNA gene sequences. The approach incorporates both traditional machine learning methods (e.g., Random Forests and KNN regression) and deep learning techniques, with particular emphasis on language models for predictions from gene sequences.",
            "lineNumber": 5
          },
          {
            "lineNumber": 6
          },
          {
            "text": "## Methods",
            "lineNumber": 7
          },
          {
            "lineNumber": 8
          },
          {
            "text": "- **Traditional ML**: Random Forests, KNN regression, and other classical approaches",
            "lineNumber": 9
          },
          {
            "text": "- **Deep Learning**: Language models and other neural network architectures, especially for gene sequence-based predictions",
            "lineNumber": 10
          },
          {
            "lineNumber": 11
          },
          {
            "text": "## SRA data download",
            "lineNumber": 12
          },
          {
            "lineNumber": 13
          },
          {
            "text": "The &#96;download_sra_data.py&#96; script downloads DNA sequence data from the NCBI Sequence Read Archive (SRA) for samples listed in &#96;sample_data.csv&#96;. The script downloads only the first 0.25MB of each sample to manage storage requirements while preserving sequence data for analysis.",
            "lineNumber": 14
          },
          {
            "lineNumber": 15
          },
          {
            "text": "&#96;&#96;&#96;bash",
            "lineNumber": 16
          },
          {
            "text": "python download_sra_data.py",
            "lineNumber": 17
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 18
          },
          {
            "lineNumber": 19
          },
          {
            "text": "&lt;details&gt;",
            "lineNumber": 20
          },
          {
            "text": "&lt;summary&gt;Details&lt;/summary&gt;",
            "lineNumber": 21
          },
          {
            "lineNumber": 22
          },
          {
            "text": "The script processes samples from &#96;sample_data.csv&#96; that have sample_ids matching the &#96;SRR*&#96;, &#96;ERR*&#96;, or &#96;DRR*&#96; pattern (NCBI SRA identifiers). For each sample, it downloads the first 0.25MB from the NCBI SRA fasta endpoint, processes the incomplete gzip file, and saves the result to &#96;data/fasta/{study_name}/{sample_id}.fasta.gz&#96;.",
            "lineNumber": 23
          },
          {
            "lineNumber": 24
          },
          {
            "text": "- **Partial downloads**: Uses HTTP Range headers to download only the first 0.25MB, reducing storage requirements",
            "lineNumber": 25
          },
          {
            "text": "- **Incomplete gzip handling**: Extracts truncated gzip files using &#96;zcat&#96;, which can handle incomplete archives",
            "lineNumber": 26
          },
          {
            "text": "- **Sequence truncation protection**: Removes the last sequence from each file since it may be truncated due to the partial download",
            "lineNumber": 27
          },
          {
            "text": "- **Duplicate handling**: Automatically skips samples that have already been downloaded or appear multiple times in the CSV",
            "lineNumber": 28
          },
          {
            "text": "- **Pattern filtering**: Only processes sample_ids matching &#96;SRR*&#96;, &#96;ERR*&#96;, or &#96;DRR*&#96; patterns; other identifiers are skipped",
            "lineNumber": 29
          },
          {
            "text": "- **Progress tracking**: Shows current study_name and sample_id during processing with a summary at completion",
            "lineNumber": 30
          },
          {
            "text": "- **Cleanup on interruption**: Removes temporary files if the script is interrupted (Ctrl+C), preventing leftover files",
            "lineNumber": 31
          },
          {
            "lineNumber": 32
          },
          {
            "text": "**Output Structure:**",
            "lineNumber": 33
          },
          {
            "text": "- Files are saved to &#96;data/fasta/{study_name}/{sample_id}.fasta.gz&#96;",
            "lineNumber": 34
          }
        ]
      },
      "score": 0.21702979505062103
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./.cursor/plans/hyenadna_ph_regression_head_0c97fe04.plan.md",
        "range": {
          "startPosition": {
            "line": 227
          },
          "endPosition": {
            "line": 296
          }
        },
        "contents": "**Data**:\n\n- Train/Val/Test split: &#96;0.8/0.1/0.1&#96;\n- Filter samples with missing pH values\n- Random seed: &#96;42&#96; (for reproducibility)\n\n### Sequence Concatenation Details\n\n**Confirmed Feasibility**: [SEP] token (ID=1) exists in &#96;CharacterTokenizer&#96; and can be used for concatenation.\n\n**Implementation Strategy**:\n\n1. For each sample with multiple sequences:\n\n   - Tokenize each sequence: &#96;tokenizer(seq)[\"input_ids\"]&#96;\n   - Remove special tokens from individual sequences (keep only DNA tokens)\n   - Join sequences with [SEP] token: &#96;seq1_tokens + [1] + seq2_tokens + [1] + ...&#96;\n   - Truncate if total length &gt; &#96;max_length&#96;: keep last &#96;max_length&#96; tokens\n   - Add [CLS] at start if desired (though causal models may not need it)\n\n2. Edge cases:\n\n   - Empty sequence list: return empty or single [SEP] token\n   - Single sequence: no [SEP] needed, just tokenize and truncate\n   - All sequences too long: take last sequence and truncate to max_length\n\n3. Padding:\n\n   - Pad to &#96;max_length&#96; with [PAD] token (ID=4) on the left\n   - Create attention mask to ignore padding tokens in loss\n\n### File Structure\n\n&#96;&#96;&#96;\ntrain_hyenadna_ph.py\nâ”œâ”€â”€ Imports (torch, datasets, transformers, etc.)\nâ”œâ”€â”€ RegressionHead class\nâ”œâ”€â”€ Data loading functions\nâ”‚   â”œâ”€â”€ load_dataset_from_hf()\nâ”‚   â”œâ”€â”€ concatenate_sequences()\nâ”‚   â”œâ”€â”€ create_dataloader()\nâ”‚   â””â”€â”€ collate_fn() for batching\nâ”œâ”€â”€ Model setup functions\nâ”‚   â”œâ”€â”€ setup_model()\nâ”‚   â””â”€â”€ get_model_config()\nâ”œâ”€â”€ Training functions\nâ”‚   â”œâ”€â”€ train_epoch()\nâ”‚   â”œâ”€â”€ evaluate()\nâ”‚   â””â”€â”€ save_checkpoint()\nâ””â”€â”€ main() with argument parsing\n&#96;&#96;&#96;\n\n### Dependencies\n\nAdd to project requirements (if not already present):\n\n- &#96;torch&#96; (PyTorch)\n- &#96;datasets&#96; (HuggingFace datasets)\n- &#96;transformers&#96; (for tokenizer compatibility)\n- &#96;numpy&#96;\n- &#96;scikit-learn&#96; (for metrics: RÂ², MSE, MAE)\n- &#96;tqdm&#96; (optional, for progress bars)\n\n### Output Files\n\nThe script should save:\n\n- &#96;best_model.pt&#96;: Best model checkpoint (based on validation loss)\n- &#96;training_log.txt&#96;: Training history (loss, metrics per epoch)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "**Data**:",
            "lineNumber": 228
          },
          {
            "lineNumber": 229
          },
          {
            "text": "- Train/Val/Test split: &#96;0.8/0.1/0.1&#96;",
            "lineNumber": 230
          },
          {
            "text": "- Filter samples with missing pH values",
            "lineNumber": 231
          },
          {
            "text": "- Random seed: &#96;42&#96; (for reproducibility)",
            "lineNumber": 232
          },
          {
            "lineNumber": 233
          },
          {
            "text": "### Sequence Concatenation Details",
            "lineNumber": 234
          },
          {
            "lineNumber": 235
          },
          {
            "text": "**Confirmed Feasibility**: [SEP] token (ID=1) exists in &#96;CharacterTokenizer&#96; and can be used for concatenation.",
            "lineNumber": 236
          },
          {
            "lineNumber": 237
          },
          {
            "text": "**Implementation Strategy**:",
            "lineNumber": 238
          },
          {
            "lineNumber": 239
          },
          {
            "text": "1. For each sample with multiple sequences:",
            "lineNumber": 240
          },
          {
            "lineNumber": 241
          },
          {
            "text": "   - Tokenize each sequence: &#96;tokenizer(seq)[\"input_ids\"]&#96;",
            "lineNumber": 242
          },
          {
            "text": "   - Remove special tokens from individual sequences (keep only DNA tokens)",
            "lineNumber": 243
          },
          {
            "text": "   - Join sequences with [SEP] token: &#96;seq1_tokens + [1] + seq2_tokens + [1] + ...&#96;",
            "lineNumber": 244
          },
          {
            "text": "   - Truncate if total length &gt; &#96;max_length&#96;: keep last &#96;max_length&#96; tokens",
            "lineNumber": 245
          },
          {
            "text": "   - Add [CLS] at start if desired (though causal models may not need it)",
            "lineNumber": 246
          },
          {
            "lineNumber": 247
          },
          {
            "text": "2. Edge cases:",
            "lineNumber": 248
          },
          {
            "lineNumber": 249
          },
          {
            "text": "   - Empty sequence list: return empty or single [SEP] token",
            "lineNumber": 250
          },
          {
            "text": "   - Single sequence: no [SEP] needed, just tokenize and truncate",
            "lineNumber": 251
          },
          {
            "text": "   - All sequences too long: take last sequence and truncate to max_length",
            "lineNumber": 252
          },
          {
            "lineNumber": 253
          },
          {
            "text": "3. Padding:",
            "lineNumber": 254
          },
          {
            "lineNumber": 255
          },
          {
            "text": "   - Pad to &#96;max_length&#96; with [PAD] token (ID=4) on the left",
            "lineNumber": 256
          },
          {
            "text": "   - Create attention mask to ignore padding tokens in loss",
            "lineNumber": 257
          },
          {
            "lineNumber": 258
          },
          {
            "text": "### File Structure",
            "lineNumber": 259
          },
          {
            "lineNumber": 260
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 261
          },
          {
            "text": "train_hyenadna_ph.py",
            "lineNumber": 262
          },
          {
            "text": "â”œâ”€â”€ Imports (torch, datasets, transformers, etc.)",
            "lineNumber": 263
          },
          {
            "text": "â”œâ”€â”€ RegressionHead class",
            "lineNumber": 264
          },
          {
            "text": "â”œâ”€â”€ Data loading functions",
            "lineNumber": 265
          },
          {
            "text": "â”‚   â”œâ”€â”€ load_dataset_from_hf()",
            "lineNumber": 266
          },
          {
            "text": "â”‚   â”œâ”€â”€ concatenate_sequences()",
            "lineNumber": 267
          },
          {
            "text": "â”‚   â”œâ”€â”€ create_dataloader()",
            "lineNumber": 268
          },
          {
            "text": "â”‚   â””â”€â”€ collate_fn() for batching",
            "lineNumber": 269
          },
          {
            "text": "â”œâ”€â”€ Model setup functions",
            "lineNumber": 270
          },
          {
            "text": "â”‚   â”œâ”€â”€ setup_model()",
            "lineNumber": 271
          },
          {
            "text": "â”‚   â””â”€â”€ get_model_config()",
            "lineNumber": 272
          },
          {
            "text": "â”œâ”€â”€ Training functions",
            "lineNumber": 273
          },
          {
            "text": "â”‚   â”œâ”€â”€ train_epoch()",
            "lineNumber": 274
          },
          {
            "text": "â”‚   â”œâ”€â”€ evaluate()",
            "lineNumber": 275
          },
          {
            "text": "â”‚   â””â”€â”€ save_checkpoint()",
            "lineNumber": 276
          },
          {
            "text": "â””â”€â”€ main() with argument parsing",
            "lineNumber": 277
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 278
          },
          {
            "lineNumber": 279
          },
          {
            "text": "### Dependencies",
            "lineNumber": 280
          },
          {
            "lineNumber": 281
          },
          {
            "text": "Add to project requirements (if not already present):",
            "lineNumber": 282
          },
          {
            "lineNumber": 283
          },
          {
            "text": "- &#96;torch&#96; (PyTorch)",
            "lineNumber": 284
          },
          {
            "text": "- &#96;datasets&#96; (HuggingFace datasets)",
            "lineNumber": 285
          },
          {
            "text": "- &#96;transformers&#96; (for tokenizer compatibility)",
            "lineNumber": 286
          },
          {
            "text": "- &#96;numpy&#96;",
            "lineNumber": 287
          },
          {
            "text": "- &#96;scikit-learn&#96; (for metrics: RÂ², MSE, MAE)",
            "lineNumber": 288
          },
          {
            "text": "- &#96;tqdm&#96; (optional, for progress bars)",
            "lineNumber": 289
          },
          {
            "lineNumber": 290
          },
          {
            "text": "### Output Files",
            "lineNumber": 291
          },
          {
            "lineNumber": 292
          },
          {
            "text": "The script should save:",
            "lineNumber": 293
          },
          {
            "lineNumber": 294
          },
          {
            "text": "- &#96;best_model.pt&#96;: Best model checkpoint (based on validation loss)",
            "lineNumber": 295
          },
          {
            "text": "- &#96;training_log.txt&#96;: Training history (loss, metrics per epoch)",
            "lineNumber": 296
          }
        ]
      },
      "score": 0.2148706316947937
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/standalone_hyenadna.py",
        "range": {
          "startPosition": {
            "line": 1149
          },
          "endPosition": {
            "line": 1295,
            "column": 35
          }
        },
        "contents": "class CharacterTokenizer(PreTrainedTokenizer):\ndef build_inputs_with_special_tokens(\n        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n    ) -&gt; List[int]:\n\n        return result\n\n    def get_special_tokens_mask(\n        self,\n        token_ids_0: List[int],\n        token_ids_1: Optional[List[int]] = None,\n        already_has_special_tokens: bool = False,\n    ) -&gt; List[int]:\n        if already_has_special_tokens:\n            return super().get_special_tokens_mask(\n                token_ids_0=token_ids_0,\n                token_ids_1=token_ids_1,\n                already_has_special_tokens=True,\n            )\n\n        result = [1] + ([0] * len(token_ids_0)) + [1]\n        if token_ids_1 is not None:\n            result += ([0] * len(token_ids_1)) + [1]\n        return result\n\n    def create_token_type_ids_from_sequences(\n        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n    ) -&gt; List[int]:\n        sep = [self.sep_token_id]\n        cls = [self.cls_token_id]\n\n        result = len(cls + token_ids_0 + sep) * [0]\n        if token_ids_1 is not None:\n            result += len(token_ids_1 + sep) * [1]\n        return result\n\n    def get_config(self) -&gt; Dict:\n        return {\n            \"char_ords\": [ord(ch) for ch in self.characters],\n            \"model_max_length\": self.model_max_length,\n        }\n\n    @classmethod\n    def from_config(cls, config: Dict) -&gt; \"CharacterTokenizer\":\n        cfg = {}\n        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n        return cls(**cfg)\n\n    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n        cfg = self.get_config()\n        with open(cfg_file, \"w\") as f:\n            json.dump(cfg, f, indent=4)\n\n    @classmethod\n    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n        with open(cfg_file) as f:\n            cfg = json.load(f)\n        return cls.from_config(cfg)",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 1150,
                "column": 1
              },
              "endPosition": {
                "line": 1151,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class CharacterTokenizer(PreTrainedTokenizer):",
            "lineNumber": 1150,
            "isSignature": true
          },
          {
            "text": "def build_inputs_with_special_tokens(",
            "lineNumber": 1233
          },
          {
            "text": "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None",
            "lineNumber": 1234
          },
          {
            "text": "    ) -&gt; List[int]:",
            "lineNumber": 1235
          },
          {
            "lineNumber": 1240
          },
          {
            "text": "        return result",
            "lineNumber": 1241
          },
          {
            "lineNumber": 1242
          },
          {
            "text": "    def get_special_tokens_mask(",
            "lineNumber": 1243
          },
          {
            "text": "        self,",
            "lineNumber": 1244
          },
          {
            "text": "        token_ids_0: List[int],",
            "lineNumber": 1245
          },
          {
            "text": "        token_ids_1: Optional[List[int]] = None,",
            "lineNumber": 1246
          },
          {
            "text": "        already_has_special_tokens: bool = False,",
            "lineNumber": 1247
          },
          {
            "text": "    ) -&gt; List[int]:",
            "lineNumber": 1248
          },
          {
            "text": "        if already_has_special_tokens:",
            "lineNumber": 1249
          },
          {
            "text": "            return super().get_special_tokens_mask(",
            "lineNumber": 1250
          },
          {
            "text": "                token_ids_0=token_ids_0,",
            "lineNumber": 1251
          },
          {
            "text": "                token_ids_1=token_ids_1,",
            "lineNumber": 1252
          },
          {
            "text": "                already_has_special_tokens=True,",
            "lineNumber": 1253
          },
          {
            "text": "            )",
            "lineNumber": 1254
          },
          {
            "lineNumber": 1255
          },
          {
            "text": "        result = [1] + ([0] * len(token_ids_0)) + [1]",
            "lineNumber": 1256
          },
          {
            "text": "        if token_ids_1 is not None:",
            "lineNumber": 1257
          },
          {
            "text": "            result += ([0] * len(token_ids_1)) + [1]",
            "lineNumber": 1258
          },
          {
            "text": "        return result",
            "lineNumber": 1259
          },
          {
            "lineNumber": 1260
          },
          {
            "text": "    def create_token_type_ids_from_sequences(",
            "lineNumber": 1261
          },
          {
            "text": "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None",
            "lineNumber": 1262
          },
          {
            "text": "    ) -&gt; List[int]:",
            "lineNumber": 1263
          },
          {
            "text": "        sep = [self.sep_token_id]",
            "lineNumber": 1264
          },
          {
            "text": "        cls = [self.cls_token_id]",
            "lineNumber": 1265
          },
          {
            "lineNumber": 1266
          },
          {
            "text": "        result = len(cls + token_ids_0 + sep) * [0]",
            "lineNumber": 1267
          },
          {
            "text": "        if token_ids_1 is not None:",
            "lineNumber": 1268
          },
          {
            "text": "            result += len(token_ids_1 + sep) * [1]",
            "lineNumber": 1269
          },
          {
            "text": "        return result",
            "lineNumber": 1270
          },
          {
            "lineNumber": 1271
          },
          {
            "text": "    def get_config(self) -&gt; Dict:",
            "lineNumber": 1272
          },
          {
            "text": "        return {",
            "lineNumber": 1273
          },
          {
            "text": "            \"char_ords\": [ord(ch) for ch in self.characters],",
            "lineNumber": 1274
          },
          {
            "text": "            \"model_max_length\": self.model_max_length,",
            "lineNumber": 1275
          },
          {
            "text": "        }",
            "lineNumber": 1276
          },
          {
            "lineNumber": 1277
          },
          {
            "text": "    @classmethod",
            "lineNumber": 1278
          },
          {
            "text": "    def from_config(cls, config: Dict) -&gt; \"CharacterTokenizer\":",
            "lineNumber": 1279
          },
          {
            "text": "        cfg = {}",
            "lineNumber": 1280
          },
          {
            "text": "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]",
            "lineNumber": 1281
          },
          {
            "text": "        cfg[\"model_max_length\"] = config[\"model_max_length\"]",
            "lineNumber": 1282
          },
          {
            "text": "        return cls(**cfg)",
            "lineNumber": 1283
          },
          {
            "lineNumber": 1284
          },
          {
            "text": "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):",
            "lineNumber": 1285
          },
          {
            "text": "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"",
            "lineNumber": 1286
          },
          {
            "text": "        cfg = self.get_config()",
            "lineNumber": 1287
          },
          {
            "text": "        with open(cfg_file, \"w\") as f:",
            "lineNumber": 1288
          },
          {
            "text": "            json.dump(cfg, f, indent=4)",
            "lineNumber": 1289
          },
          {
            "lineNumber": 1290
          },
          {
            "text": "    @classmethod",
            "lineNumber": 1291
          },
          {
            "text": "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):",
            "lineNumber": 1292
          },
          {
            "text": "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"",
            "lineNumber": 1293
          },
          {
            "text": "        with open(cfg_file) as f:",
            "lineNumber": 1294
          },
          {
            "text": "            cfg = json.load(f)",
            "lineNumber": 1295
          },
          {
            "text": "        return cls.from_config(cfg)",
            "lineNumber": 1296,
            "isSignature": true
          }
        ]
      },
      "score": 0.21222719550132751
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 45,
            "column": 16
          }
        },
        "contents": "#!/usr/bin/env python3\n\"\"\"\nTrain HyenaDNA model for pH prediction from DNA sequences.\n\nThis script implements a regression head for HyenaDNA to predict continuous pH values.\nIt loads data from HuggingFace, concatenates multiple sequences with [SEP] tokens,\nand trains using pretrained HyenaDNA weights.\n\"\"\"\n\nimport argparse\nimport json\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_dataset\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\n\n# Add hyena-dna to path\nsys.path.insert(0, str(Path(__file__).parent / \"hyena-dna\"))\n\nfrom huggingface_wrapper import HyenaDNAPreTrainedModel\nfrom standalone_hyenadna import CharacterTokenizer, HyenaDNAModel\n\n# Model size configurations\nMODEL_CONFIGS = {\n    \"hyenadna-tiny-1k-seqlen\": {\"max_length\": 1024},\n    \"hyenadna-small-32k-seqlen\": {\"max_length\": 32768},\n    \"hyenadna-medium-160k-seqlen\": {\"max_length\": 160000},\n    \"hyenadna-medium-450k-seqlen\": {\"max_length\": 450000},\n    \"hyenadna-large-1m-seqlen\": {\"max_length\": 1_000_000},\n}\n\n# Special token IDs\nSEP_TOKEN_ID = 1\nPAD_TOKEN_ID = 4\nCLS_TOKEN_ID = 0",
        "signatures": {},
        "detailedLines": [
          {
            "text": "#!/usr/bin/env python3",
            "lineNumber": 1
          },
          {
            "text": "\"\"\"",
            "lineNumber": 2
          },
          {
            "text": "Train HyenaDNA model for pH prediction from DNA sequences.",
            "lineNumber": 3
          },
          {
            "lineNumber": 4
          },
          {
            "text": "This script implements a regression head for HyenaDNA to predict continuous pH values.",
            "lineNumber": 5
          },
          {
            "text": "It loads data from HuggingFace, concatenates multiple sequences with [SEP] tokens,",
            "lineNumber": 6
          },
          {
            "text": "and trains using pretrained HyenaDNA weights.",
            "lineNumber": 7
          },
          {
            "text": "\"\"\"",
            "lineNumber": 8
          },
          {
            "lineNumber": 9
          },
          {
            "text": "import argparse",
            "lineNumber": 10
          },
          {
            "text": "import json",
            "lineNumber": 11
          },
          {
            "text": "import os",
            "lineNumber": 12
          },
          {
            "text": "import sys",
            "lineNumber": 13
          },
          {
            "text": "from pathlib import Path",
            "lineNumber": 14
          },
          {
            "text": "from typing import Dict, List, Optional, Tuple",
            "lineNumber": 15
          },
          {
            "lineNumber": 16
          },
          {
            "text": "import numpy as np",
            "lineNumber": 17
          },
          {
            "text": "import torch",
            "lineNumber": 18
          },
          {
            "text": "import torch.nn as nn",
            "lineNumber": 19
          },
          {
            "text": "import torch.nn.functional as F",
            "lineNumber": 20
          },
          {
            "text": "from datasets import load_dataset",
            "lineNumber": 21
          },
          {
            "text": "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score",
            "lineNumber": 22
          },
          {
            "text": "from sklearn.model_selection import train_test_split",
            "lineNumber": 23
          },
          {
            "lineNumber": 24
          },
          {
            "text": "from torch.utils.data import DataLoader, Dataset",
            "lineNumber": 25
          },
          {
            "text": "from tqdm import tqdm",
            "lineNumber": 26
          },
          {
            "lineNumber": 27
          },
          {
            "text": "# Add hyena-dna to path",
            "lineNumber": 28
          },
          {
            "text": "sys.path.insert(0, str(Path(__file__).parent / \"hyena-dna\"))",
            "lineNumber": 29
          },
          {
            "lineNumber": 30
          },
          {
            "text": "from huggingface_wrapper import HyenaDNAPreTrainedModel",
            "lineNumber": 31
          },
          {
            "text": "from standalone_hyenadna import CharacterTokenizer, HyenaDNAModel",
            "lineNumber": 32
          },
          {
            "lineNumber": 33
          },
          {
            "text": "# Model size configurations",
            "lineNumber": 34
          },
          {
            "text": "MODEL_CONFIGS = {",
            "lineNumber": 35
          },
          {
            "text": "    \"hyenadna-tiny-1k-seqlen\": {\"max_length\": 1024},",
            "lineNumber": 36
          },
          {
            "text": "    \"hyenadna-small-32k-seqlen\": {\"max_length\": 32768},",
            "lineNumber": 37
          },
          {
            "text": "    \"hyenadna-medium-160k-seqlen\": {\"max_length\": 160000},",
            "lineNumber": 38
          },
          {
            "text": "    \"hyenadna-medium-450k-seqlen\": {\"max_length\": 450000},",
            "lineNumber": 39
          },
          {
            "text": "    \"hyenadna-large-1m-seqlen\": {\"max_length\": 1_000_000},",
            "lineNumber": 40
          },
          {
            "text": "}",
            "lineNumber": 41
          },
          {
            "lineNumber": 42
          },
          {
            "text": "# Special token IDs",
            "lineNumber": 43
          },
          {
            "text": "SEP_TOKEN_ID = 1",
            "lineNumber": 44
          },
          {
            "text": "PAD_TOKEN_ID = 4",
            "lineNumber": 45
          },
          {
            "text": "CLS_TOKEN_ID = 0",
            "lineNumber": 46
          }
        ]
      },
      "score": 0.2105472981929779
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./README.md",
        "range": {
          "startPosition": {
            "line": 109
          },
          "endPosition": {
            "line": 161
          }
        },
        "contents": "3. **Model Training**:\n   - Optional grid search with progress bar for hyperparameter tuning\n   - Uses scikit-learn Pipeline for preprocessing and model fitting\n\n4. **Output**:\n   - Saves test set predictions to CSV files with study names and sample IDs\n   - Generates summary files with model metrics (RMSE, R-squared, MAE) and grid search results\n\n**Available Models:**\n- &#96;linear&#96;: Linear Regression\n- &#96;knn&#96;: K-Nearest Neighbors Regression\n- &#96;rf&#96;: Random Forest Regression\n- &#96;hgb&#96;: HistGradientBoosting Regression\n\n**Grid Search Modes:**\n- &#96;test&#96;: Small grids (2-4 combinations) for quick testing\n- &#96;full&#96;: Comprehensive grids (hundreds of combinations) for thorough tuning\n- &#96;none&#96;: Skip grid search and use default parameters\n\n**Output Files:**\n- &#96;{model}_test_predictions.csv&#96;: Test set predictions with study_name, sample_id, true_pH, predicted_pH, and residual\n- &#96;{model}_summary.txt&#96;: Model summary with metrics and grid search results (if performed)\n\n&lt;/details&gt;\n\n## pH prediction from HyenaDNA sequence model\n\nThe &#96;train_hyenadna_ph.py&#96; script trains a HyenaDNA model with a regression head to predict continuous pH values from DNA sequences. The script loads data from the HuggingFace dataset, uses pretrained HyenaDNA weights, concatenates multiple sequences per sample with [SEP] tokens, and performs a 75:5:20 train-val-test split (same test set as the traditional ML workflow).\n\n&#96;&#96;&#96;bash\n# Basic training with defaults (tiny model, downloads from HuggingFace)\npython train_hyenadna_ph.py --download-model\n\n# Custom configuration with larger model\npython train_hyenadna_ph.py \\\n    --model-name hyenadna-small-32k-seqlen \\\n    --head-architecture mlp3 \\\n    --pooling-mode last \\\n    --batch-size 2 \\\n    --learning-rate 5e-5 \\\n    --num-epochs 20 \\\n    --freeze-backbone \\\n    --download-model\n\n# Train with frozen backbone (only regression head)\npython train_hyenadna_ph.py --freeze-backbone --download-model\n&#96;&#96;&#96;\n\n&lt;details&gt;\n&lt;summary&gt;Details&lt;/summary&gt;\n\nThe script implements a separate &#96;RegressionHead&#96; class that maintains separation from the HyenaDNA codebase. It loads DNA sequences from the HuggingFace dataset (&#96;jedick/microbial-DNA-pH&#96;), concatenates multiple sequences per sample with [SEP] tokens up to the model's maximum length, and fine-tunes pretrained HyenaDNA weights for pH prediction.",
        "signatures": {},
        "detailedLines": [
          {
            "text": "3. **Model Training**:",
            "lineNumber": 110
          },
          {
            "text": "   - Optional grid search with progress bar for hyperparameter tuning",
            "lineNumber": 111
          },
          {
            "text": "   - Uses scikit-learn Pipeline for preprocessing and model fitting",
            "lineNumber": 112
          },
          {
            "lineNumber": 113
          },
          {
            "text": "4. **Output**:",
            "lineNumber": 114
          },
          {
            "text": "   - Saves test set predictions to CSV files with study names and sample IDs",
            "lineNumber": 115
          },
          {
            "text": "   - Generates summary files with model metrics (RMSE, R-squared, MAE) and grid search results",
            "lineNumber": 116
          },
          {
            "lineNumber": 117
          },
          {
            "text": "**Available Models:**",
            "lineNumber": 118
          },
          {
            "text": "- &#96;linear&#96;: Linear Regression",
            "lineNumber": 119
          },
          {
            "text": "- &#96;knn&#96;: K-Nearest Neighbors Regression",
            "lineNumber": 120
          },
          {
            "text": "- &#96;rf&#96;: Random Forest Regression",
            "lineNumber": 121
          },
          {
            "text": "- &#96;hgb&#96;: HistGradientBoosting Regression",
            "lineNumber": 122
          },
          {
            "lineNumber": 123
          },
          {
            "text": "**Grid Search Modes:**",
            "lineNumber": 124
          },
          {
            "text": "- &#96;test&#96;: Small grids (2-4 combinations) for quick testing",
            "lineNumber": 125
          },
          {
            "text": "- &#96;full&#96;: Comprehensive grids (hundreds of combinations) for thorough tuning",
            "lineNumber": 126
          },
          {
            "text": "- &#96;none&#96;: Skip grid search and use default parameters",
            "lineNumber": 127
          },
          {
            "lineNumber": 128
          },
          {
            "text": "**Output Files:**",
            "lineNumber": 129
          },
          {
            "text": "- &#96;{model}_test_predictions.csv&#96;: Test set predictions with study_name, sample_id, true_pH, predicted_pH, and residual",
            "lineNumber": 130
          },
          {
            "text": "- &#96;{model}_summary.txt&#96;: Model summary with metrics and grid search results (if performed)",
            "lineNumber": 131
          },
          {
            "lineNumber": 132
          },
          {
            "text": "&lt;/details&gt;",
            "lineNumber": 133
          },
          {
            "lineNumber": 134
          },
          {
            "text": "## pH prediction from HyenaDNA sequence model",
            "lineNumber": 135
          },
          {
            "lineNumber": 136
          },
          {
            "text": "The &#96;train_hyenadna_ph.py&#96; script trains a HyenaDNA model with a regression head to predict continuous pH values from DNA sequences. The script loads data from the HuggingFace dataset, uses pretrained HyenaDNA weights, concatenates multiple sequences per sample with [SEP] tokens, and performs a 75:5:20 train-val-test split (same test set as the traditional ML workflow).",
            "lineNumber": 137
          },
          {
            "lineNumber": 138
          },
          {
            "text": "&#96;&#96;&#96;bash",
            "lineNumber": 139
          },
          {
            "text": "# Basic training with defaults (tiny model, downloads from HuggingFace)",
            "lineNumber": 140
          },
          {
            "text": "python train_hyenadna_ph.py --download-model",
            "lineNumber": 141
          },
          {
            "lineNumber": 142
          },
          {
            "text": "# Custom configuration with larger model",
            "lineNumber": 143
          },
          {
            "text": "python train_hyenadna_ph.py \\",
            "lineNumber": 144
          },
          {
            "text": "    --model-name hyenadna-small-32k-seqlen \\",
            "lineNumber": 145
          },
          {
            "text": "    --head-architecture mlp3 \\",
            "lineNumber": 146
          },
          {
            "text": "    --pooling-mode last \\",
            "lineNumber": 147
          },
          {
            "text": "    --batch-size 2 \\",
            "lineNumber": 148
          },
          {
            "text": "    --learning-rate 5e-5 \\",
            "lineNumber": 149
          },
          {
            "text": "    --num-epochs 20 \\",
            "lineNumber": 150
          },
          {
            "text": "    --freeze-backbone \\",
            "lineNumber": 151
          },
          {
            "text": "    --download-model",
            "lineNumber": 152
          },
          {
            "lineNumber": 153
          },
          {
            "text": "# Train with frozen backbone (only regression head)",
            "lineNumber": 154
          },
          {
            "text": "python train_hyenadna_ph.py --freeze-backbone --download-model",
            "lineNumber": 155
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 156
          },
          {
            "lineNumber": 157
          },
          {
            "text": "&lt;details&gt;",
            "lineNumber": 158
          },
          {
            "text": "&lt;summary&gt;Details&lt;/summary&gt;",
            "lineNumber": 159
          },
          {
            "lineNumber": 160
          },
          {
            "text": "The script implements a separate &#96;RegressionHead&#96; class that maintains separation from the HyenaDNA codebase. It loads DNA sequences from the HuggingFace dataset (&#96;jedick/microbial-DNA-pH&#96;), concatenates multiple sequences per sample with [SEP] tokens up to the model's maximum length, and fine-tunes pretrained HyenaDNA weights for pH prediction.",
            "lineNumber": 161
          }
        ]
      },
      "score": 0.20887404680252075
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./.cursor/plans/hyenadna_ph_regression_head_0c97fe04.plan.md",
        "range": {
          "startPosition": {
            "line": 164
          },
          "endPosition": {
            "line": 232
          }
        },
        "contents": "2. **HyenaDNAModel modifications**:\n   &#96;&#96;&#96;python\n   def __init__(\n       ...,\n       use_head=False,\n       task_type=\"classification\",      # NEW\n       n_classes: int = 2,              # For classification\n       head_architecture=\"linear\",     # NEW\n       ...\n   ):\n   &#96;&#96;&#96;\n\n\n   - Determine &#96;d_output&#96; based on &#96;task_type&#96;\n   - Pass &#96;task_type&#96; and &#96;head_architecture&#96; to &#96;SequenceDecoder&#96;\n\n3. **Output shape**:\n\n   - Classification: &#96;(batch, n_classes)&#96; \n   - Regression: &#96;(batch, 1)&#96; or &#96;(batch,)&#96; after squeeze\n\n### HyenaDNA Model Configuration\n\nBased on the [HyenaDNA repository](https://github.com/HazyResearch/hyena-dna), needed configurations:\n\n**Model Size Options** (with max_lengths):\n\n- &#96;hyenadna-tiny-1k-seqlen&#96;: max_length=1024 (default for testing)\n- &#96;hyenadna-small-32k-seqlen&#96;: max_length=32768\n- &#96;hyenadna-medium-160k-seqlen&#96;: max_length=160000\n- &#96;hyenadna-medium-450k-seqlen&#96;: max_length=450000\n- &#96;hyenadna-large-1m-seqlen&#96;: max_length=1_000_000\n\n**Tokenizer Configuration**:\n\n- Characters: &#96;[\"A\", \"C\", \"G\", \"T\", \"N\"]&#96; (N for uncertain nucleotides)\n- &#96;model_max_length = max_length + 2&#96; (account for special tokens)\n- &#96;padding_side = \"left\"&#96; (causal model)\n\n**Model Loading**:\n\n- Use &#96;HyenaDNAPreTrainedModel.from_pretrained()&#96; from &#96;hyena-dna/huggingface_wrapper.py&#96;\n- Set &#96;use_head=False&#96; to get embeddings only\n- Download weights if not present locally (configurable)\n\n### Default Hyperparameters\n\n**Model**:\n\n- Model size: &#96;hyenadna-tiny-1k-seqlen&#96; (default, configurable)\n- Head architecture: &#96;mlp2&#96; (2-layer MLP)\n- Pooling mode: &#96;pool&#96; (mean pooling)\n- Freeze backbone: &#96;False&#96; (fine-tune entire model)\n\n**Training**:\n\n- Batch size: &#96;4&#96; (small for long sequences, adjust based on GPU memory)\n- Learning rate: &#96;1e-4&#96; (typical for fine-tuning)\n- Num epochs: &#96;10&#96; (configurable)\n- Loss function: &#96;SmoothL1Loss&#96; (beta=1.0)\n- Optimizer: &#96;AdamW&#96; with weight decay &#96;0.01&#96;\n- Learning rate scheduler: Optional &#96;ReduceLROnPlateau&#96; (patience=3)\n\n**Data**:\n\n- Train/Val/Test split: &#96;0.8/0.1/0.1&#96;\n- Filter samples with missing pH values\n- Random seed: &#96;42&#96; (for reproducibility)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "2. **HyenaDNAModel modifications**:",
            "lineNumber": 165
          },
          {
            "text": "   &#96;&#96;&#96;python",
            "lineNumber": 166
          },
          {
            "text": "   def __init__(",
            "lineNumber": 167
          },
          {
            "text": "       ...,",
            "lineNumber": 168
          },
          {
            "text": "       use_head=False,",
            "lineNumber": 169
          },
          {
            "text": "       task_type=\"classification\",      # NEW",
            "lineNumber": 170
          },
          {
            "text": "       n_classes: int = 2,              # For classification",
            "lineNumber": 171
          },
          {
            "text": "       head_architecture=\"linear\",     # NEW",
            "lineNumber": 172
          },
          {
            "text": "       ...",
            "lineNumber": 173
          },
          {
            "text": "   ):",
            "lineNumber": 174
          },
          {
            "text": "   &#96;&#96;&#96;",
            "lineNumber": 175
          },
          {
            "lineNumber": 176
          },
          {
            "lineNumber": 177
          },
          {
            "text": "   - Determine &#96;d_output&#96; based on &#96;task_type&#96;",
            "lineNumber": 178
          },
          {
            "text": "   - Pass &#96;task_type&#96; and &#96;head_architecture&#96; to &#96;SequenceDecoder&#96;",
            "lineNumber": 179
          },
          {
            "lineNumber": 180
          },
          {
            "text": "3. **Output shape**:",
            "lineNumber": 181
          },
          {
            "lineNumber": 182
          },
          {
            "text": "   - Classification: &#96;(batch, n_classes)&#96; ",
            "lineNumber": 183
          },
          {
            "text": "   - Regression: &#96;(batch, 1)&#96; or &#96;(batch,)&#96; after squeeze",
            "lineNumber": 184
          },
          {
            "lineNumber": 185
          },
          {
            "text": "### HyenaDNA Model Configuration",
            "lineNumber": 186
          },
          {
            "lineNumber": 187
          },
          {
            "text": "Based on the [HyenaDNA repository](https://github.com/HazyResearch/hyena-dna), needed configurations:",
            "lineNumber": 188
          },
          {
            "lineNumber": 189
          },
          {
            "text": "**Model Size Options** (with max_lengths):",
            "lineNumber": 190
          },
          {
            "lineNumber": 191
          },
          {
            "text": "- &#96;hyenadna-tiny-1k-seqlen&#96;: max_length=1024 (default for testing)",
            "lineNumber": 192
          },
          {
            "text": "- &#96;hyenadna-small-32k-seqlen&#96;: max_length=32768",
            "lineNumber": 193
          },
          {
            "text": "- &#96;hyenadna-medium-160k-seqlen&#96;: max_length=160000",
            "lineNumber": 194
          },
          {
            "text": "- &#96;hyenadna-medium-450k-seqlen&#96;: max_length=450000",
            "lineNumber": 195
          },
          {
            "text": "- &#96;hyenadna-large-1m-seqlen&#96;: max_length=1_000_000",
            "lineNumber": 196
          },
          {
            "lineNumber": 197
          },
          {
            "text": "**Tokenizer Configuration**:",
            "lineNumber": 198
          },
          {
            "lineNumber": 199
          },
          {
            "text": "- Characters: &#96;[\"A\", \"C\", \"G\", \"T\", \"N\"]&#96; (N for uncertain nucleotides)",
            "lineNumber": 200
          },
          {
            "text": "- &#96;model_max_length = max_length + 2&#96; (account for special tokens)",
            "lineNumber": 201
          },
          {
            "text": "- &#96;padding_side = \"left\"&#96; (causal model)",
            "lineNumber": 202
          },
          {
            "lineNumber": 203
          },
          {
            "text": "**Model Loading**:",
            "lineNumber": 204
          },
          {
            "lineNumber": 205
          },
          {
            "text": "- Use &#96;HyenaDNAPreTrainedModel.from_pretrained()&#96; from &#96;hyena-dna/huggingface_wrapper.py&#96;",
            "lineNumber": 206
          },
          {
            "text": "- Set &#96;use_head=False&#96; to get embeddings only",
            "lineNumber": 207
          },
          {
            "text": "- Download weights if not present locally (configurable)",
            "lineNumber": 208
          },
          {
            "lineNumber": 209
          },
          {
            "text": "### Default Hyperparameters",
            "lineNumber": 210
          },
          {
            "lineNumber": 211
          },
          {
            "text": "**Model**:",
            "lineNumber": 212
          },
          {
            "lineNumber": 213
          },
          {
            "text": "- Model size: &#96;hyenadna-tiny-1k-seqlen&#96; (default, configurable)",
            "lineNumber": 214
          },
          {
            "text": "- Head architecture: &#96;mlp2&#96; (2-layer MLP)",
            "lineNumber": 215
          },
          {
            "text": "- Pooling mode: &#96;pool&#96; (mean pooling)",
            "lineNumber": 216
          },
          {
            "text": "- Freeze backbone: &#96;False&#96; (fine-tune entire model)",
            "lineNumber": 217
          },
          {
            "lineNumber": 218
          },
          {
            "text": "**Training**:",
            "lineNumber": 219
          },
          {
            "lineNumber": 220
          },
          {
            "text": "- Batch size: &#96;4&#96; (small for long sequences, adjust based on GPU memory)",
            "lineNumber": 221
          },
          {
            "text": "- Learning rate: &#96;1e-4&#96; (typical for fine-tuning)",
            "lineNumber": 222
          },
          {
            "text": "- Num epochs: &#96;10&#96; (configurable)",
            "lineNumber": 223
          },
          {
            "text": "- Loss function: &#96;SmoothL1Loss&#96; (beta=1.0)",
            "lineNumber": 224
          },
          {
            "text": "- Optimizer: &#96;AdamW&#96; with weight decay &#96;0.01&#96;",
            "lineNumber": 225
          },
          {
            "text": "- Learning rate scheduler: Optional &#96;ReduceLROnPlateau&#96; (patience=3)",
            "lineNumber": 226
          },
          {
            "lineNumber": 227
          },
          {
            "text": "**Data**:",
            "lineNumber": 228
          },
          {
            "lineNumber": 229
          },
          {
            "text": "- Train/Val/Test split: &#96;0.8/0.1/0.1&#96;",
            "lineNumber": 230
          },
          {
            "text": "- Filter samples with missing pH values",
            "lineNumber": 231
          },
          {
            "text": "- Random seed: &#96;42&#96; (for reproducibility)",
            "lineNumber": 232
          }
        ]
      },
      "score": 0.20669186115264893
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./.cursor/plans/hyenadna_ph_regression_head_0c97fe04.plan.md",
        "range": {
          "startPosition": {
            "line": 41
          },
          "endPosition": {
            "line": 119
          }
        },
        "contents": "- Uses &#96;SequenceDecoder&#96; for classification (we will NOT modify this)\n- &#96;CharacterTokenizer&#96; supports [SEP] token (ID=1) - **confirmed feasible for concatenation**\n- Model uses causal attention with left padding\n- Pretrained weights available on HuggingFace\n\nThe dataset structure (from [&#96;create_hf_dataset.py&#96;](create_hf_dataset.py)):\n\n- DNA sequences stored as lists of strings per sample\n- pH values are continuous floats (can be &#96;None&#96; for missing data)\n- Each sample has multiple sequences (up to 1MB total DNA length)\n- Available on HuggingFace: &#96;jedick/microbial-DNA-pH&#96;\n\n## Architecture Design\n\n### RegressionHead Class\n\nCreate a new &#96;RegressionHead&#96; class that:\n\n- Takes hidden states from HyenaDNA backbone (shape: &#96;(batch, seq_len, d_model)&#96;)\n- Applies pooling to get sequence-level representation (similar to &#96;SequenceDecoder&#96;)\n- Maps pooled features to single pH value via MLP\n- Supports multiple architecture options\n\n**Architecture Options**:\n\n1. **&#96;linear&#96;**: Simple linear layer\n   &#96;&#96;&#96;python\n   nn.Linear(d_model, 1)\n   &#96;&#96;&#96;\n\n2. **&#96;mlp2&#96;** (Default): 2-layer MLP\n   &#96;&#96;&#96;python\n   nn.Sequential(\n       nn.Linear(d_model, d_model // 2),\n       nn.ReLU(),\n       nn.Dropout(0.1),\n       nn.Linear(d_model // 2, 1)\n   )\n   &#96;&#96;&#96;\n\n3. **&#96;mlp3&#96;**: 3-layer MLP\n   &#96;&#96;&#96;python\n   nn.Sequential(\n       nn.Linear(d_model, d_model),\n       nn.ReLU(),\n       nn.Dropout(0.1),\n       nn.Linear(d_model, d_model // 2),\n       nn.ReLU(),\n       nn.Dropout(0.1),\n       nn.Linear(d_model // 2, 1)\n   )\n   &#96;&#96;&#96;\n\n4. **&#96;mlp2_ln&#96;**: 2-layer MLP with LayerNorm\n   &#96;&#96;&#96;python\n   nn.Sequential(\n       nn.LayerNorm(d_model),\n       nn.Linear(d_model, d_model // 2),\n       nn.ReLU(),\n       nn.Dropout(0.1),\n       nn.Linear(d_model // 2, 1)\n   )\n   &#96;&#96;&#96;\n\n\n**Pooling Modes** (reuse logic from &#96;SequenceDecoder&#96;):\n\n- &#96;pool&#96;: Mean pooling (default) - works well for variable-length sequences\n- &#96;last&#96;: Use last token representation\n- &#96;first&#96;: Use first token representation\n- &#96;sum&#96;: Sum all tokens\n\n**Output**: Single scalar pH value per sample &#96;(batch,)&#96; or &#96;(batch, 1)&#96;\n\n### Model Integration Strategy\n\nSince we're keeping separation from HyenaDNA codebase:",
        "signatures": {},
        "detailedLines": [
          {
            "text": "- Uses &#96;SequenceDecoder&#96; for classification (we will NOT modify this)",
            "lineNumber": 43
          },
          {
            "text": "- &#96;CharacterTokenizer&#96; supports [SEP] token (ID=1) - **confirmed feasible for concatenation**",
            "lineNumber": 44
          },
          {
            "text": "- Model uses causal attention with left padding",
            "lineNumber": 45
          },
          {
            "text": "- Pretrained weights available on HuggingFace",
            "lineNumber": 46
          },
          {
            "lineNumber": 47
          },
          {
            "text": "The dataset structure (from [&#96;create_hf_dataset.py&#96;](create_hf_dataset.py)):",
            "lineNumber": 48
          },
          {
            "lineNumber": 49
          },
          {
            "text": "- DNA sequences stored as lists of strings per sample",
            "lineNumber": 50
          },
          {
            "text": "- pH values are continuous floats (can be &#96;None&#96; for missing data)",
            "lineNumber": 51
          },
          {
            "text": "- Each sample has multiple sequences (up to 1MB total DNA length)",
            "lineNumber": 52
          },
          {
            "text": "- Available on HuggingFace: &#96;jedick/microbial-DNA-pH&#96;",
            "lineNumber": 53
          },
          {
            "lineNumber": 54
          },
          {
            "text": "## Architecture Design",
            "lineNumber": 55
          },
          {
            "lineNumber": 56
          },
          {
            "text": "### RegressionHead Class",
            "lineNumber": 57
          },
          {
            "lineNumber": 58
          },
          {
            "text": "Create a new &#96;RegressionHead&#96; class that:",
            "lineNumber": 59
          },
          {
            "lineNumber": 60
          },
          {
            "text": "- Takes hidden states from HyenaDNA backbone (shape: &#96;(batch, seq_len, d_model)&#96;)",
            "lineNumber": 61
          },
          {
            "text": "- Applies pooling to get sequence-level representation (similar to &#96;SequenceDecoder&#96;)",
            "lineNumber": 62
          },
          {
            "text": "- Maps pooled features to single pH value via MLP",
            "lineNumber": 63
          },
          {
            "text": "- Supports multiple architecture options",
            "lineNumber": 64
          },
          {
            "lineNumber": 65
          },
          {
            "text": "**Architecture Options**:",
            "lineNumber": 66
          },
          {
            "lineNumber": 67
          },
          {
            "text": "1. **&#96;linear&#96;**: Simple linear layer",
            "lineNumber": 68
          },
          {
            "text": "   &#96;&#96;&#96;python",
            "lineNumber": 69
          },
          {
            "text": "   nn.Linear(d_model, 1)",
            "lineNumber": 70
          },
          {
            "text": "   &#96;&#96;&#96;",
            "lineNumber": 71
          },
          {
            "lineNumber": 72
          },
          {
            "text": "2. **&#96;mlp2&#96;** (Default): 2-layer MLP",
            "lineNumber": 73
          },
          {
            "text": "   &#96;&#96;&#96;python",
            "lineNumber": 74
          },
          {
            "text": "   nn.Sequential(",
            "lineNumber": 75
          },
          {
            "text": "       nn.Linear(d_model, d_model // 2),",
            "lineNumber": 76
          },
          {
            "text": "       nn.ReLU(),",
            "lineNumber": 77
          },
          {
            "text": "       nn.Dropout(0.1),",
            "lineNumber": 78
          },
          {
            "text": "       nn.Linear(d_model // 2, 1)",
            "lineNumber": 79
          },
          {
            "text": "   )",
            "lineNumber": 80
          },
          {
            "text": "   &#96;&#96;&#96;",
            "lineNumber": 81
          },
          {
            "lineNumber": 82
          },
          {
            "text": "3. **&#96;mlp3&#96;**: 3-layer MLP",
            "lineNumber": 83
          },
          {
            "text": "   &#96;&#96;&#96;python",
            "lineNumber": 84
          },
          {
            "text": "   nn.Sequential(",
            "lineNumber": 85
          },
          {
            "text": "       nn.Linear(d_model, d_model),",
            "lineNumber": 86
          },
          {
            "text": "       nn.ReLU(),",
            "lineNumber": 87
          },
          {
            "text": "       nn.Dropout(0.1),",
            "lineNumber": 88
          },
          {
            "text": "       nn.Linear(d_model, d_model // 2),",
            "lineNumber": 89
          },
          {
            "text": "       nn.ReLU(),",
            "lineNumber": 90
          },
          {
            "text": "       nn.Dropout(0.1),",
            "lineNumber": 91
          },
          {
            "text": "       nn.Linear(d_model // 2, 1)",
            "lineNumber": 92
          },
          {
            "text": "   )",
            "lineNumber": 93
          },
          {
            "text": "   &#96;&#96;&#96;",
            "lineNumber": 94
          },
          {
            "lineNumber": 95
          },
          {
            "text": "4. **&#96;mlp2_ln&#96;**: 2-layer MLP with LayerNorm",
            "lineNumber": 96
          },
          {
            "text": "   &#96;&#96;&#96;python",
            "lineNumber": 97
          },
          {
            "text": "   nn.Sequential(",
            "lineNumber": 98
          },
          {
            "text": "       nn.LayerNorm(d_model),",
            "lineNumber": 99
          },
          {
            "text": "       nn.Linear(d_model, d_model // 2),",
            "lineNumber": 100
          },
          {
            "text": "       nn.ReLU(),",
            "lineNumber": 101
          },
          {
            "text": "       nn.Dropout(0.1),",
            "lineNumber": 102
          },
          {
            "text": "       nn.Linear(d_model // 2, 1)",
            "lineNumber": 103
          },
          {
            "text": "   )",
            "lineNumber": 104
          },
          {
            "text": "   &#96;&#96;&#96;",
            "lineNumber": 105
          },
          {
            "lineNumber": 106
          },
          {
            "lineNumber": 107
          },
          {
            "text": "**Pooling Modes** (reuse logic from &#96;SequenceDecoder&#96;):",
            "lineNumber": 108
          },
          {
            "lineNumber": 109
          },
          {
            "text": "- &#96;pool&#96;: Mean pooling (default) - works well for variable-length sequences",
            "lineNumber": 110
          },
          {
            "text": "- &#96;last&#96;: Use last token representation",
            "lineNumber": 111
          },
          {
            "text": "- &#96;first&#96;: Use first token representation",
            "lineNumber": 112
          },
          {
            "text": "- &#96;sum&#96;: Sum all tokens",
            "lineNumber": 113
          },
          {
            "lineNumber": 114
          },
          {
            "text": "**Output**: Single scalar pH value per sample &#96;(batch,)&#96; or &#96;(batch, 1)&#96;",
            "lineNumber": 115
          },
          {
            "lineNumber": 116
          },
          {
            "text": "### Model Integration Strategy",
            "lineNumber": 117
          },
          {
            "lineNumber": 118
          },
          {
            "text": "Since we're keeping separation from HyenaDNA codebase:",
            "lineNumber": 119
          }
        ]
      },
      "score": 0.1918455958366394
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 498
          },
          "endPosition": {
            "line": 601
          }
        },
        "contents": "def setup_model(\n    model_name: str = \"hyenadna-tiny-1k-seqlen\",\n    checkpoint_dir: str = \"./checkpoints\",\n    head_architecture: str = \"mlp2\",\n    pooling_mode: str = \"pool\",\n    freeze_backbone: bool = False,\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    download: bool = False,\n) -&gt; Tuple[nn.Module, CharacterTokenizer, int]:\n\n\n    max_length = MODEL_CONFIGS[model_name][\"max_length\"]\n    print(f\"Loading model: {model_name} (max_length={max_length})...\")\n\n    # Load pretrained model (embeddings only, no head)\n    model = HyenaDNAPreTrainedModel.from_pretrained(\n        checkpoint_dir,\n        model_name,\n        download=download,\n        config=None,\n        device=device,\n        use_head=False,\n        n_classes=2,  # Not used when use_head=False\n    )\n\n    # Get d_model from model config\n    # The model is a HyenaDNAModel, access d_model from backbone\n    d_model = model.backbone.embeddings.word_embeddings.embedding_dim\n\n    # Create and attach regression head\n    regression_head = RegressionHead(\n        d_model=d_model,\n        architecture=head_architecture,\n        pooling_mode=pooling_mode,\n    )\n    model.regression_head = regression_head\n\n    # Freeze backbone if requested\n    if freeze_backbone:\n        print(\"Freezing backbone weights...\")\n        for param in model.backbone.parameters():\n            param.requires_grad = False\n        # Only regression head will be trained\n        for param in model.regression_head.parameters():\n            param.requires_grad = True\n\n    # Move to device\n    model = model.to(device)\n\n    # Create tokenizer\n    tokenizer = CharacterTokenizer(\n        characters=[\"A\", \"C\", \"G\", \"T\", \"N\"],\n        model_max_length=max_length + 2,  # Account for special tokens\n        padding_side=\"left\",  # Causal model uses left padding\n    )\n\n    print(f\"Model setup complete. d_model={d_model}, device={device}\")\n    return model, tokenizer, max_length\n\n\nclass HyenaDNAForRegression(nn.Module):\n    \"\"\"Wrapper model combining HyenaDNA backbone and regression head.\"\"\"\n\n    def __init__(self, hyenadna_model: nn.Module, regression_head: nn.Module):\n        super().__init__()\n        self.hyenadna_model = hyenadna_model\n        self.regression_head = regression_head\n\n    def forward(self, input_ids: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Args:\n            input_ids: (batch, seq_len) token IDs\n\n        Returns:\n            (batch,) pH predictions\n        ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def setup_model(",
            "lineNumber": 499,
            "isSignature": true
          },
          {
            "text": "    model_name: str = \"hyenadna-tiny-1k-seqlen\",",
            "lineNumber": 500,
            "isSignature": true
          },
          {
            "text": "    checkpoint_dir: str = \"./checkpoints\",",
            "lineNumber": 501,
            "isSignature": true
          },
          {
            "text": "    head_architecture: str = \"mlp2\",",
            "lineNumber": 502,
            "isSignature": true
          },
          {
            "text": "    pooling_mode: str = \"pool\",",
            "lineNumber": 503,
            "isSignature": true
          },
          {
            "text": "    freeze_backbone: bool = False,",
            "lineNumber": 504,
            "isSignature": true
          },
          {
            "text": "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",",
            "lineNumber": 505,
            "isSignature": true
          },
          {
            "text": "    download: bool = False,",
            "lineNumber": 506,
            "isSignature": true
          },
          {
            "text": ") -&gt; Tuple[nn.Module, CharacterTokenizer, int]:",
            "lineNumber": 507,
            "isSignature": true
          },
          {
            "lineNumber": 527
          },
          {
            "lineNumber": 528
          },
          {
            "text": "    max_length = MODEL_CONFIGS[model_name][\"max_length\"]",
            "lineNumber": 529
          },
          {
            "text": "    print(f\"Loading model: {model_name} (max_length={max_length})...\")",
            "lineNumber": 530
          },
          {
            "lineNumber": 531
          },
          {
            "text": "    # Load pretrained model (embeddings only, no head)",
            "lineNumber": 532
          },
          {
            "text": "    model = HyenaDNAPreTrainedModel.from_pretrained(",
            "lineNumber": 533
          },
          {
            "text": "        checkpoint_dir,",
            "lineNumber": 534
          },
          {
            "text": "        model_name,",
            "lineNumber": 535
          },
          {
            "text": "        download=download,",
            "lineNumber": 536
          },
          {
            "text": "        config=None,",
            "lineNumber": 537
          },
          {
            "text": "        device=device,",
            "lineNumber": 538
          },
          {
            "text": "        use_head=False,",
            "lineNumber": 539
          },
          {
            "text": "        n_classes=2,  # Not used when use_head=False",
            "lineNumber": 540
          },
          {
            "text": "    )",
            "lineNumber": 541
          },
          {
            "lineNumber": 542
          },
          {
            "text": "    # Get d_model from model config",
            "lineNumber": 543
          },
          {
            "text": "    # The model is a HyenaDNAModel, access d_model from backbone",
            "lineNumber": 544
          },
          {
            "text": "    d_model = model.backbone.embeddings.word_embeddings.embedding_dim",
            "lineNumber": 545
          },
          {
            "lineNumber": 546
          },
          {
            "text": "    # Create and attach regression head",
            "lineNumber": 547
          },
          {
            "text": "    regression_head = RegressionHead(",
            "lineNumber": 548
          },
          {
            "text": "        d_model=d_model,",
            "lineNumber": 549
          },
          {
            "text": "        architecture=head_architecture,",
            "lineNumber": 550
          },
          {
            "text": "        pooling_mode=pooling_mode,",
            "lineNumber": 551
          },
          {
            "text": "    )",
            "lineNumber": 552
          },
          {
            "text": "    model.regression_head = regression_head",
            "lineNumber": 553
          },
          {
            "lineNumber": 554
          },
          {
            "text": "    # Freeze backbone if requested",
            "lineNumber": 555
          },
          {
            "text": "    if freeze_backbone:",
            "lineNumber": 556
          },
          {
            "text": "        print(\"Freezing backbone weights...\")",
            "lineNumber": 557
          },
          {
            "text": "        for param in model.backbone.parameters():",
            "lineNumber": 558
          },
          {
            "text": "            param.requires_grad = False",
            "lineNumber": 559
          },
          {
            "text": "        # Only regression head will be trained",
            "lineNumber": 560
          },
          {
            "text": "        for param in model.regression_head.parameters():",
            "lineNumber": 561
          },
          {
            "text": "            param.requires_grad = True",
            "lineNumber": 562
          },
          {
            "lineNumber": 563
          },
          {
            "text": "    # Move to device",
            "lineNumber": 564
          },
          {
            "text": "    model = model.to(device)",
            "lineNumber": 565
          },
          {
            "lineNumber": 566
          },
          {
            "text": "    # Create tokenizer",
            "lineNumber": 567
          },
          {
            "text": "    tokenizer = CharacterTokenizer(",
            "lineNumber": 568
          },
          {
            "text": "        characters=[\"A\", \"C\", \"G\", \"T\", \"N\"],",
            "lineNumber": 569
          },
          {
            "text": "        model_max_length=max_length + 2,  # Account for special tokens",
            "lineNumber": 570
          },
          {
            "text": "        padding_side=\"left\",  # Causal model uses left padding",
            "lineNumber": 571
          },
          {
            "text": "    )",
            "lineNumber": 572
          },
          {
            "lineNumber": 573
          },
          {
            "text": "    print(f\"Model setup complete. d_model={d_model}, device={device}\")",
            "lineNumber": 574
          },
          {
            "text": "    return model, tokenizer, max_length",
            "lineNumber": 575,
            "isSignature": true
          },
          {
            "lineNumber": 576
          },
          {
            "lineNumber": 577
          },
          {
            "text": "class HyenaDNAForRegression(nn.Module):",
            "lineNumber": 578,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Wrapper model combining HyenaDNA backbone and regression head.\"\"\"",
            "lineNumber": 579
          },
          {
            "lineNumber": 580
          },
          {
            "text": "    def __init__(self, hyenadna_model: nn.Module, regression_head: nn.Module):",
            "lineNumber": 581
          },
          {
            "text": "        super().__init__()",
            "lineNumber": 582
          },
          {
            "text": "        self.hyenadna_model = hyenadna_model",
            "lineNumber": 583
          },
          {
            "text": "        self.regression_head = regression_head",
            "lineNumber": 584
          },
          {
            "lineNumber": 585
          },
          {
            "text": "    def forward(self, input_ids: torch.Tensor) -&gt; torch.Tensor:",
            "lineNumber": 586
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 587
          },
          {
            "text": "        Forward pass.",
            "lineNumber": 588
          },
          {
            "lineNumber": 589
          },
          {
            "text": "        Args:",
            "lineNumber": 590
          },
          {
            "text": "            input_ids: (batch, seq_len) token IDs",
            "lineNumber": 591
          },
          {
            "lineNumber": 592
          },
          {
            "text": "        Returns:",
            "lineNumber": 593
          },
          {
            "text": "            (batch,) pH predictions",
            "lineNumber": 594
          },
          {
            "text": "        ",
            "lineNumber": 595
          }
        ]
      },
      "score": 0.19156159460544586
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./predict_ph.py",
        "range": {
          "startPosition": {
            "line": 291
          },
          "endPosition": {
            "line": 510
          }
        },
        "contents": "def main():\n\n                results.append((params, mean_score))\n\n                # Print progress if not using tqdm\n                if not HAS_TQDM and (i + 1) % max(1, n_combinations // 10) == 0:\n                    print(f\"  Completed {i + 1}/{n_combinations} combinations...\")\n\n            grid_search_time = time.time() - start_time\n\n            # Find best parameters (highest score = lowest MSE)\n            best_params, best_cv_score = max(results, key=lambda x: x[1])\n            best_cv_score = -best_cv_score  # Convert back to positive MSE\n\n            # Set pipeline to best parameters and fit on full training set\n            pipeline.set_params(**best_params)\n            pipeline.fit(X_train, y_train)\n\n            print(f\"\\nGrid search completed in {grid_search_time:.2f} seconds\")\n            print(f\"Best parameters: {best_params}\")\n            print(f\"Best CV score (RMSE): {np.sqrt(best_cv_score):.4f}\")\n        else:\n            print(\"No parameter grid available for this model.\")\n    else:\n        if args.model == \"linear\":\n            print(\"Linear regression doesn't require hyperparameter tuning.\")\n        print(f\"Fitting {args.model} model with default parameters...\")\n        pipeline.fit(X_train, y_train)\n\n    # Make predictions\n    print(\"Making predictions...\")\n    y_train_pred = pipeline.predict(X_train)",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 263,
                "column": 1
              },
              "endPosition": {
                "line": 264,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def main():",
            "lineNumber": 292,
            "isSignature": true
          },
          {
            "lineNumber": 407
          },
          {
            "text": "                results.append((params, mean_score))",
            "lineNumber": 408
          },
          {
            "lineNumber": 409
          },
          {
            "text": "                # Print progress if not using tqdm",
            "lineNumber": 410
          },
          {
            "text": "                if not HAS_TQDM and (i + 1) % max(1, n_combinations // 10) == 0:",
            "lineNumber": 411
          },
          {
            "text": "                    print(f\"  Completed {i + 1}/{n_combinations} combinations...\")",
            "lineNumber": 412
          },
          {
            "lineNumber": 413
          },
          {
            "text": "            grid_search_time = time.time() - start_time",
            "lineNumber": 414
          },
          {
            "lineNumber": 415
          },
          {
            "text": "            # Find best parameters (highest score = lowest MSE)",
            "lineNumber": 416
          },
          {
            "text": "            best_params, best_cv_score = max(results, key=lambda x: x[1])",
            "lineNumber": 417
          },
          {
            "text": "            best_cv_score = -best_cv_score  # Convert back to positive MSE",
            "lineNumber": 418
          },
          {
            "lineNumber": 419
          },
          {
            "text": "            # Set pipeline to best parameters and fit on full training set",
            "lineNumber": 420
          },
          {
            "text": "            pipeline.set_params(**best_params)",
            "lineNumber": 421
          },
          {
            "text": "            pipeline.fit(X_train, y_train)",
            "lineNumber": 422
          },
          {
            "lineNumber": 423
          },
          {
            "text": "            print(f\"\\nGrid search completed in {grid_search_time:.2f} seconds\")",
            "lineNumber": 424
          },
          {
            "text": "            print(f\"Best parameters: {best_params}\")",
            "lineNumber": 425
          },
          {
            "text": "            print(f\"Best CV score (RMSE): {np.sqrt(best_cv_score):.4f}\")",
            "lineNumber": 426
          },
          {
            "text": "        else:",
            "lineNumber": 427
          },
          {
            "text": "            print(\"No parameter grid available for this model.\")",
            "lineNumber": 428
          },
          {
            "text": "    else:",
            "lineNumber": 429
          },
          {
            "text": "        if args.model == \"linear\":",
            "lineNumber": 430
          },
          {
            "text": "            print(\"Linear regression doesn't require hyperparameter tuning.\")",
            "lineNumber": 431
          },
          {
            "text": "        print(f\"Fitting {args.model} model with default parameters...\")",
            "lineNumber": 432
          },
          {
            "text": "        pipeline.fit(X_train, y_train)",
            "lineNumber": 433
          },
          {
            "lineNumber": 434
          },
          {
            "text": "    # Make predictions",
            "lineNumber": 435
          },
          {
            "text": "    print(\"Making predictions...\")",
            "lineNumber": 436
          },
          {
            "text": "    y_train_pred = pipeline.predict(X_train)",
            "lineNumber": 437
          }
        ]
      },
      "score": 0.19049380719661713
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---


Checking if config.json is saved alongside checkpoints:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Creating a new test script and updating the training script. The checkpoint is saved in the output directory, and config.json is saved there too, so we can load the config from the checkpoint directory.


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Creating the new test script and updating the training script to use it:


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>

</tool-use>

---


Updating the training script to use the new function instead of duplicating code:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>

</tool-use>

---


Checking for syntax errors and verifying the imports work correctly:


---

<tool-use data-tool-type="unknown" data-tool-name="read_lints">
<details>
<summary>Tool use: **read_lints**</summary>


Parameters:

```json
{
  "paths": [
    "test_hyenadna_ph.py",
    "train_hyenadna_ph.py"
  ]
}
```
Result:

```json
{
  "linterErrorsByFile": [
    {
      "relativeWorkspacePath": "test_hyenadna_ph.py"
    },
    {
      "relativeWorkspacePath": "train_hyenadna_ph.py",
      "errors": [
        {
          "message": "Import \"huggingface_wrapper\" could not be resolved",
          "range": {
            "startPosition": {
              "line": 31,
              "column": 6
            },
            "endPosition": {
              "line": 31,
              "column": 25
            }
          },
          "source": "basedpyright",
          "severity": "DIAGNOSTIC_SEVERITY_WARNING"
        },
        {
          "message": "Import \"standalone_hyenadna\" could not be resolved",
          "range": {
            "startPosition": {
              "line": 32,
              "column": 6
            },
            "endPosition": {
              "line": 32,
              "column": 25
            }
          },
          "source": "basedpyright",
          "severity": "DIAGNOSTIC_SEVERITY_WARNING"
        }
      ]
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---


The linter warnings are about imports in a subdirectory and are expected. Verifying the modified section:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Making the script executable and checking for any obvious issues:


---

<tool-use data-tool-type="shell" data-tool-name="run_terminal_command_v2">
<details><summary>Tool use: **run_terminal_command_v2** â€¢ Run command: chmod +x /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

```bash
chmod +x /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py
```
</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Removing an unused import:


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>

</tool-use>

---


Checking the function return values:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Fixing the return type annotation:


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>

</tool-use>

---


Verifying the code works correctly. Checking the final state of both files:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="unknown" data-tool-name="ripgrep_raw_search">
<details>
<summary>Tool use: **ripgrep_raw_search**</summary>


Parameters:

```json
{
  "pattern": "^if __name__",
  "path": "/home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py",
  "caseInsensitive": false
}
```
Result:

```json
{
  "success": {
    "pattern": "^if __name__",
    "path": "/home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py",
    "outputMode": "content",
    "workspaceResults": {
      "/home/jedick/github/microbial-pH-predictions": {
        "content": {
          "matches": [
            {
              "file": "test_hyenadna_ph.py",
              "matches": [
                {
                  "lineNumber": 410,
                  "content": "if __name__ == \"__main__\":"
                }
              ]
            }
          ],
          "totalLines": 1,
          "totalMatchedLines": 1
        }
      }
    }
  }
}
```
Status: **completed**


</details>
</tool-use>

---


## Summary

1. **Created `test_hyenadna_ph.py`** â€” a standalone script with:
   - `run_test_predictions()` function that loads a checkpoint, runs test predictions, and saves to CSV
   - Command-line interface with required arguments:
     - `--checkpoint`: path to checkpoint file (e.g., `best_model.pt`)
     - `--output`: path to output CSV file (e.g., `test_predictions.csv`)
   - Optional model configuration arguments (`--model-name`, `--head-architecture`, `--pooling-mode`) that auto-load from `config.json` if it exists in the checkpoint directory
   - All the test evaluation and prediction aggregation logic from the original code

2. **Modified `train_hyenadna_ph.py`** â€” replaced the duplicated test evaluation code (starting at line 965) with a call to `run_test_predictions()`, avoiding code duplication.

The new script can be run standalone like this:
```bash
python test_hyenadna_ph.py --checkpoint results/hyenadna_ph/best_model.pt --output test_predictions.csv
```

If `config.json` exists in the checkpoint directory, it will automatically load the model configuration. Otherwise, you can provide the model configuration arguments explicitly.

The code is ready to use. The training script now calls the shared function, and the new script can be used independently for running test predictions on trained models.

---

_**User**_

Please change the script to remove all arguments that can be read from the config.json. That's what config.json is there for - to store the hyperparameter settings used for training the model. Also add this as a project rule: Don't use arguments to duplicate logic for hyperparameters in config.json (or any other type of configuration setting). We don't have to reinvent the wheel to pass argument values that are already stored in a configuration file. The main exception is if the purpose of the script is specifically to modify these argument values - which is not the case here.

---

_**Agent (model default, mode Agent)**_


Updating the test script to read from `config.json` and avoid duplicating arguments. Creating a project rule for this.


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Checking what's saved in config.json:


---

<tool-use data-tool-type="unknown" data-tool-name="semantic_search_full">
<details>
<summary>Tool use: **semantic_search_full**</summary>


Parameters:

```json
{
  "query": "What arguments are saved to config.json in the training script?",
  "topK": 10,
  "explanation": ""
}
```
Result:

```json
{
  "codeResults": [
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 498
          },
          "endPosition": {
            "line": 601
          }
        },
        "contents": "def setup_model(\n    model_name: str = \"hyenadna-tiny-1k-seqlen\",\n    checkpoint_dir: str = \"./checkpoints\",\n    head_architecture: str = \"mlp2\",\n    pooling_mode: str = \"pool\",\n    freeze_backbone: bool = False,\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    download: bool = False,\n) -&gt; Tuple[nn.Module, CharacterTokenizer, int]:\n\n\n    max_length = MODEL_CONFIGS[model_name][\"max_length\"]\n    print(f\"Loading model: {model_name} (max_length={max_length})...\")\n\n    # Load pretrained model (embeddings only, no head)\n    model = HyenaDNAPreTrainedModel.from_pretrained(\n        checkpoint_dir,\n        model_name,\n        download=download,\n        config=None,\n        device=device,\n        use_head=False,\n        n_classes=2,  # Not used when use_head=False\n    )\n\n    # Get d_model from model config\n    # The model is a HyenaDNAModel, access d_model from backbone\n    d_model = model.backbone.embeddings.word_embeddings.embedding_dim\n\n    # Create and attach regression head\n    regression_head = RegressionHead(\n        d_model=d_model,\n        architecture=head_architecture,\n        pooling_mode=pooling_mode,\n    )\n    model.regression_head = regression_head\n\n    # Freeze backbone if requested\n    if freeze_backbone:\n        print(\"Freezing backbone weights...\")\n        for param in model.backbone.parameters():\n            param.requires_grad = False\n        # Only regression head will be trained\n        for param in model.regression_head.parameters():\n            param.requires_grad = True\n\n    # Move to device\n    model = model.to(device)\n\n    # Create tokenizer\n    tokenizer = CharacterTokenizer(\n        characters=[\"A\", \"C\", \"G\", \"T\", \"N\"],\n        model_max_length=max_length + 2,  # Account for special tokens\n        padding_side=\"left\",  # Causal model uses left padding\n    )\n\n    print(f\"Model setup complete. d_model={d_model}, device={device}\")\n    return model, tokenizer, max_length\n\n\nclass HyenaDNAForRegression(nn.Module):\n    \"\"\"Wrapper model combining HyenaDNA backbone and regression head.\"\"\"\n\n    def __init__(self, hyenadna_model: nn.Module, regression_head: nn.Module):\n        super().__init__()\n        self.hyenadna_model = hyenadna_model\n        self.regression_head = regression_head\n\n    def forward(self, input_ids: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass.\n\n        Args:\n            input_ids: (batch, seq_len) token IDs\n\n        Returns:\n            (batch,) pH predictions\n        ",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def setup_model(",
            "lineNumber": 499,
            "isSignature": true
          },
          {
            "text": "    model_name: str = \"hyenadna-tiny-1k-seqlen\",",
            "lineNumber": 500,
            "isSignature": true
          },
          {
            "text": "    checkpoint_dir: str = \"./checkpoints\",",
            "lineNumber": 501,
            "isSignature": true
          },
          {
            "text": "    head_architecture: str = \"mlp2\",",
            "lineNumber": 502,
            "isSignature": true
          },
          {
            "text": "    pooling_mode: str = \"pool\",",
            "lineNumber": 503,
            "isSignature": true
          },
          {
            "text": "    freeze_backbone: bool = False,",
            "lineNumber": 504,
            "isSignature": true
          },
          {
            "text": "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",",
            "lineNumber": 505,
            "isSignature": true
          },
          {
            "text": "    download: bool = False,",
            "lineNumber": 506,
            "isSignature": true
          },
          {
            "text": ") -&gt; Tuple[nn.Module, CharacterTokenizer, int]:",
            "lineNumber": 507,
            "isSignature": true
          },
          {
            "lineNumber": 527
          },
          {
            "lineNumber": 528
          },
          {
            "text": "    max_length = MODEL_CONFIGS[model_name][\"max_length\"]",
            "lineNumber": 529
          },
          {
            "text": "    print(f\"Loading model: {model_name} (max_length={max_length})...\")",
            "lineNumber": 530
          },
          {
            "lineNumber": 531
          },
          {
            "text": "    # Load pretrained model (embeddings only, no head)",
            "lineNumber": 532
          },
          {
            "text": "    model = HyenaDNAPreTrainedModel.from_pretrained(",
            "lineNumber": 533
          },
          {
            "text": "        checkpoint_dir,",
            "lineNumber": 534
          },
          {
            "text": "        model_name,",
            "lineNumber": 535
          },
          {
            "text": "        download=download,",
            "lineNumber": 536
          },
          {
            "text": "        config=None,",
            "lineNumber": 537
          },
          {
            "text": "        device=device,",
            "lineNumber": 538
          },
          {
            "text": "        use_head=False,",
            "lineNumber": 539
          },
          {
            "text": "        n_classes=2,  # Not used when use_head=False",
            "lineNumber": 540
          },
          {
            "text": "    )",
            "lineNumber": 541
          },
          {
            "lineNumber": 542
          },
          {
            "text": "    # Get d_model from model config",
            "lineNumber": 543
          },
          {
            "text": "    # The model is a HyenaDNAModel, access d_model from backbone",
            "lineNumber": 544
          },
          {
            "text": "    d_model = model.backbone.embeddings.word_embeddings.embedding_dim",
            "lineNumber": 545
          },
          {
            "lineNumber": 546
          },
          {
            "text": "    # Create and attach regression head",
            "lineNumber": 547
          },
          {
            "text": "    regression_head = RegressionHead(",
            "lineNumber": 548
          },
          {
            "text": "        d_model=d_model,",
            "lineNumber": 549
          },
          {
            "text": "        architecture=head_architecture,",
            "lineNumber": 550
          },
          {
            "text": "        pooling_mode=pooling_mode,",
            "lineNumber": 551
          },
          {
            "text": "    )",
            "lineNumber": 552
          },
          {
            "text": "    model.regression_head = regression_head",
            "lineNumber": 553
          },
          {
            "lineNumber": 554
          },
          {
            "text": "    # Freeze backbone if requested",
            "lineNumber": 555
          },
          {
            "text": "    if freeze_backbone:",
            "lineNumber": 556
          },
          {
            "text": "        print(\"Freezing backbone weights...\")",
            "lineNumber": 557
          },
          {
            "text": "        for param in model.backbone.parameters():",
            "lineNumber": 558
          },
          {
            "text": "            param.requires_grad = False",
            "lineNumber": 559
          },
          {
            "text": "        # Only regression head will be trained",
            "lineNumber": 560
          },
          {
            "text": "        for param in model.regression_head.parameters():",
            "lineNumber": 561
          },
          {
            "text": "            param.requires_grad = True",
            "lineNumber": 562
          },
          {
            "lineNumber": 563
          },
          {
            "text": "    # Move to device",
            "lineNumber": 564
          },
          {
            "text": "    model = model.to(device)",
            "lineNumber": 565
          },
          {
            "lineNumber": 566
          },
          {
            "text": "    # Create tokenizer",
            "lineNumber": 567
          },
          {
            "text": "    tokenizer = CharacterTokenizer(",
            "lineNumber": 568
          },
          {
            "text": "        characters=[\"A\", \"C\", \"G\", \"T\", \"N\"],",
            "lineNumber": 569
          },
          {
            "text": "        model_max_length=max_length + 2,  # Account for special tokens",
            "lineNumber": 570
          },
          {
            "text": "        padding_side=\"left\",  # Causal model uses left padding",
            "lineNumber": 571
          },
          {
            "text": "    )",
            "lineNumber": 572
          },
          {
            "lineNumber": 573
          },
          {
            "text": "    print(f\"Model setup complete. d_model={d_model}, device={device}\")",
            "lineNumber": 574
          },
          {
            "text": "    return model, tokenizer, max_length",
            "lineNumber": 575,
            "isSignature": true
          },
          {
            "lineNumber": 576
          },
          {
            "lineNumber": 577
          },
          {
            "text": "class HyenaDNAForRegression(nn.Module):",
            "lineNumber": 578,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Wrapper model combining HyenaDNA backbone and regression head.\"\"\"",
            "lineNumber": 579
          },
          {
            "lineNumber": 580
          },
          {
            "text": "    def __init__(self, hyenadna_model: nn.Module, regression_head: nn.Module):",
            "lineNumber": 581
          },
          {
            "text": "        super().__init__()",
            "lineNumber": 582
          },
          {
            "text": "        self.hyenadna_model = hyenadna_model",
            "lineNumber": 583
          },
          {
            "text": "        self.regression_head = regression_head",
            "lineNumber": 584
          },
          {
            "lineNumber": 585
          },
          {
            "text": "    def forward(self, input_ids: torch.Tensor) -&gt; torch.Tensor:",
            "lineNumber": 586
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 587
          },
          {
            "text": "        Forward pass.",
            "lineNumber": 588
          },
          {
            "lineNumber": 589
          },
          {
            "text": "        Args:",
            "lineNumber": 590
          },
          {
            "text": "            input_ids: (batch, seq_len) token IDs",
            "lineNumber": 591
          },
          {
            "lineNumber": 592
          },
          {
            "text": "        Returns:",
            "lineNumber": 593
          },
          {
            "text": "            (batch,) pH predictions",
            "lineNumber": 594
          },
          {
            "text": "        ",
            "lineNumber": 595
          }
        ]
      },
      "score": 0.41041630506515503
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 577
          },
          "endPosition": {
            "line": 683
          }
        },
        "contents": "class HyenaDNAForRegression(nn.Module):\ndef forward(self, input_ids: torch.Tensor) -&gt; torch.Tensor:\n\n        Forward pass.\n\n        Args:\n            input_ids: (batch, seq_len) token IDs\n\n        Returns:\n            (batch,) pH predictions\n        \"\"\"\n        # Get embeddings from HyenaDNA model (when use_head=False, returns hidden_states)\n        hidden_states = self.hyenadna_model(input_ids)  # (batch, seq_len, d_model)\n\n        # Apply regression head\n        ph_pred = self.regression_head(hidden_states)  # (batch,)\n\n        return ph_pred\n\n\ndef train_epoch(\n    model: nn.Module,\n    dataloader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    criterion: nn.Module,\n    device: str,\n) -&gt; float:\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    total_loss = 0.0\n    num_batches = 0\n\n    for batch in tqdm(dataloader, desc=\"Training\"):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        ph_true = batch[\"ph\"].to(device)\n\n        # Forward pass\n        ph_pred = model(input_ids)\n\n        # Compute loss (only on non-padding tokens if using attention mask)\n        loss = criterion(ph_pred, ph_true)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        num_batches += 1\n\n    return total_loss / num_batches if num_batches &gt; 0 else 0.0\n\n\ndef evaluate(\n    model: nn.Module,\n    dataloader: DataLoader,\n    criterion: nn.Module,\n    device: str,\n) -&gt; Dict[str, float]:\n    \"\"\"Evaluate model.\"\"\"\n    model.eval()\n    all_preds = []\n    all_true = []\n    total_loss = 0.0\n    num_batches = 0\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            input_ids = batch[\"input_ids\"].to(device)\n            ph_true = batch[\"ph\"].to(device)\n\n            # Forward pass\n            ph_pred = model(input_ids)",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 530,
                "column": 1
              },
              "endPosition": {
                "line": 531,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "class HyenaDNAForRegression(nn.Module):",
            "lineNumber": 578,
            "isSignature": true
          },
          {
            "text": "def forward(self, input_ids: torch.Tensor) -&gt; torch.Tensor:",
            "lineNumber": 586
          },
          {
            "lineNumber": 587
          },
          {
            "text": "        Forward pass.",
            "lineNumber": 588
          },
          {
            "lineNumber": 589
          },
          {
            "text": "        Args:",
            "lineNumber": 590
          },
          {
            "text": "            input_ids: (batch, seq_len) token IDs",
            "lineNumber": 591
          },
          {
            "lineNumber": 592
          },
          {
            "text": "        Returns:",
            "lineNumber": 593
          },
          {
            "text": "            (batch,) pH predictions",
            "lineNumber": 594
          },
          {
            "text": "        \"\"\"",
            "lineNumber": 595
          },
          {
            "text": "        # Get embeddings from HyenaDNA model (when use_head=False, returns hidden_states)",
            "lineNumber": 596
          },
          {
            "text": "        hidden_states = self.hyenadna_model(input_ids)  # (batch, seq_len, d_model)",
            "lineNumber": 597
          },
          {
            "lineNumber": 598
          },
          {
            "text": "        # Apply regression head",
            "lineNumber": 599
          },
          {
            "text": "        ph_pred = self.regression_head(hidden_states)  # (batch,)",
            "lineNumber": 600
          },
          {
            "lineNumber": 601
          },
          {
            "text": "        return ph_pred",
            "lineNumber": 602,
            "isSignature": true
          },
          {
            "lineNumber": 603
          },
          {
            "lineNumber": 604
          },
          {
            "text": "def train_epoch(",
            "lineNumber": 605,
            "isSignature": true
          },
          {
            "text": "    model: nn.Module,",
            "lineNumber": 606,
            "isSignature": true
          },
          {
            "text": "    dataloader: DataLoader,",
            "lineNumber": 607,
            "isSignature": true
          },
          {
            "text": "    optimizer: torch.optim.Optimizer,",
            "lineNumber": 608,
            "isSignature": true
          },
          {
            "text": "    criterion: nn.Module,",
            "lineNumber": 609,
            "isSignature": true
          },
          {
            "text": "    device: str,",
            "lineNumber": 610,
            "isSignature": true
          },
          {
            "text": ") -&gt; float:",
            "lineNumber": 611,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Train for one epoch.\"\"\"",
            "lineNumber": 612
          },
          {
            "text": "    model.train()",
            "lineNumber": 613
          },
          {
            "text": "    total_loss = 0.0",
            "lineNumber": 614
          },
          {
            "text": "    num_batches = 0",
            "lineNumber": 615
          },
          {
            "lineNumber": 616
          },
          {
            "text": "    for batch in tqdm(dataloader, desc=\"Training\"):",
            "lineNumber": 617
          },
          {
            "text": "        input_ids = batch[\"input_ids\"].to(device)",
            "lineNumber": 618
          },
          {
            "text": "        attention_mask = batch[\"attention_mask\"].to(device)",
            "lineNumber": 619
          },
          {
            "text": "        ph_true = batch[\"ph\"].to(device)",
            "lineNumber": 620
          },
          {
            "lineNumber": 621
          },
          {
            "text": "        # Forward pass",
            "lineNumber": 622
          },
          {
            "text": "        ph_pred = model(input_ids)",
            "lineNumber": 623
          },
          {
            "lineNumber": 624
          },
          {
            "text": "        # Compute loss (only on non-padding tokens if using attention mask)",
            "lineNumber": 625
          },
          {
            "text": "        loss = criterion(ph_pred, ph_true)",
            "lineNumber": 626
          },
          {
            "lineNumber": 627
          },
          {
            "text": "        # Backward pass",
            "lineNumber": 628
          },
          {
            "text": "        optimizer.zero_grad()",
            "lineNumber": 629
          },
          {
            "text": "        loss.backward()",
            "lineNumber": 630
          },
          {
            "text": "        optimizer.step()",
            "lineNumber": 631
          },
          {
            "lineNumber": 632
          },
          {
            "text": "        total_loss += loss.item()",
            "lineNumber": 633
          },
          {
            "text": "        num_batches += 1",
            "lineNumber": 634
          },
          {
            "lineNumber": 635
          },
          {
            "text": "    return total_loss / num_batches if num_batches &gt; 0 else 0.0",
            "lineNumber": 636,
            "isSignature": true
          },
          {
            "lineNumber": 637
          },
          {
            "lineNumber": 638
          },
          {
            "text": "def evaluate(",
            "lineNumber": 639,
            "isSignature": true
          },
          {
            "text": "    model: nn.Module,",
            "lineNumber": 640,
            "isSignature": true
          },
          {
            "text": "    dataloader: DataLoader,",
            "lineNumber": 641,
            "isSignature": true
          },
          {
            "text": "    criterion: nn.Module,",
            "lineNumber": 642,
            "isSignature": true
          },
          {
            "text": "    device: str,",
            "lineNumber": 643,
            "isSignature": true
          },
          {
            "text": ") -&gt; Dict[str, float]:",
            "lineNumber": 644,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Evaluate model.\"\"\"",
            "lineNumber": 645
          },
          {
            "text": "    model.eval()",
            "lineNumber": 646
          },
          {
            "text": "    all_preds = []",
            "lineNumber": 647
          },
          {
            "text": "    all_true = []",
            "lineNumber": 648
          },
          {
            "text": "    total_loss = 0.0",
            "lineNumber": 649
          },
          {
            "text": "    num_batches = 0",
            "lineNumber": 650
          },
          {
            "lineNumber": 651
          },
          {
            "text": "    with torch.no_grad():",
            "lineNumber": 652
          },
          {
            "text": "        for batch in tqdm(dataloader, desc=\"Evaluating\"):",
            "lineNumber": 653
          },
          {
            "text": "            input_ids = batch[\"input_ids\"].to(device)",
            "lineNumber": 654
          },
          {
            "text": "            ph_true = batch[\"ph\"].to(device)",
            "lineNumber": 655
          },
          {
            "lineNumber": 656
          },
          {
            "text": "            # Forward pass",
            "lineNumber": 657
          },
          {
            "text": "            ph_pred = model(input_ids)",
            "lineNumber": 658
          }
        ]
      },
      "score": 0.40360546112060547
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 686
          },
          "endPosition": {
            "line": 996
          }
        },
        "contents": "def save_checkpoint(\n    model: nn.Module,\n    epoch: int,\n    metrics: Dict[str, float],\n    output_dir: Path,\n    is_best: bool = False,\n):\n\n        print(f\"Saved best model to {best_path}\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Train HyenaDNA for pH prediction\")\n\n    # Model arguments\n    parser.add_argument(\n        \"--model-name\",\n        type=str,\n        default=\"hyenadna-tiny-1k-seqlen\",\n        choices=list(MODEL_CONFIGS.keys()),\n        help=\"HyenaDNA model name\",\n    )\n    parser.add_argument(\n        \"--head-architecture\",\n        type=str,\n        default=\"mlp2\",\n        choices=[\"linear\", \"mlp2\", \"mlp3\", \"mlp4_high\", \"mlp4_low\", \"mlp5\", \"mlp2_ln\"],\n        help=\"Regression head architecture\",\n    )\n    parser.add_argument(\n        \"--pooling-mode\",\n        type=str,\n        default=\"pool\",\n        choices=[\"pool\", \"last\", \"first\", \"sum\"],\n        help=\"Pooling mode for regression head\",\n    )\n    parser.add_argument(\n        \"--freeze-backbone\",\n        action=\"store_true\",\n        help=\"Freeze backbone weights (only train head)\",\n    )\n\n    # Training arguments\n    parser.add_argument(\"--batch-size\", type=int, default=4, help=\"Batch size\")\n    parser.add_argument(\n        \"--learning-rate\", type=float, default=1e-4, help=\"Learning rate\"\n    )\n    parser.add_argument(\"--num-epochs\", type=int, default=10, help=\"Number of epochs\")\n    parser.add_argument(\n        \"--loss-function\",\n        type=str,\n        default=\"smooth_l1\",\n        choices=[\"mse\", \"mae\", \"smooth_l1\", \"huber\"],\n        help=\"Loss function\",\n    )\n    parser.add_argument(\n        \"--weight-decay\", type=float, default=0.01, help=\"Weight decay for AdamW\"\n    )\n\n    # Data arguments\n    parser.add_argument(\n        \"--dataset-repo\",\n        type=str,\n        default=\"jedick/microbial-DNA-pH\",\n        help=\"HuggingFace dataset repository\",\n    )\n    parser.add_argument(\n        \"--random-seed\",\n        type=int,\n        default=42,\n        help=\"Random seed for reproducibility (default: 42)\",\n    )",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 530,
                "column": 1
              },
              "endPosition": {
                "line": 531,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def save_checkpoint(",
            "lineNumber": 687,
            "isSignature": true
          },
          {
            "text": "    model: nn.Module,",
            "lineNumber": 688,
            "isSignature": true
          },
          {
            "text": "    epoch: int,",
            "lineNumber": 689,
            "isSignature": true
          },
          {
            "text": "    metrics: Dict[str, float],",
            "lineNumber": 690,
            "isSignature": true
          },
          {
            "text": "    output_dir: Path,",
            "lineNumber": 691,
            "isSignature": true
          },
          {
            "text": "    is_best: bool = False,",
            "lineNumber": 692,
            "isSignature": true
          },
          {
            "text": "):",
            "lineNumber": 693,
            "isSignature": true
          },
          {
            "lineNumber": 708
          },
          {
            "text": "        print(f\"Saved best model to {best_path}\")",
            "lineNumber": 709,
            "isSignature": true
          },
          {
            "lineNumber": 710
          },
          {
            "lineNumber": 711
          },
          {
            "text": "def main():",
            "lineNumber": 712,
            "isSignature": true
          },
          {
            "text": "    parser = argparse.ArgumentParser(description=\"Train HyenaDNA for pH prediction\")",
            "lineNumber": 713
          },
          {
            "lineNumber": 714
          },
          {
            "text": "    # Model arguments",
            "lineNumber": 715
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 716
          },
          {
            "text": "        \"--model-name\",",
            "lineNumber": 717
          },
          {
            "text": "        type=str,",
            "lineNumber": 718
          },
          {
            "text": "        default=\"hyenadna-tiny-1k-seqlen\",",
            "lineNumber": 719
          },
          {
            "text": "        choices=list(MODEL_CONFIGS.keys()),",
            "lineNumber": 720
          },
          {
            "text": "        help=\"HyenaDNA model name\",",
            "lineNumber": 721
          },
          {
            "text": "    )",
            "lineNumber": 722
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 723
          },
          {
            "text": "        \"--head-architecture\",",
            "lineNumber": 724
          },
          {
            "text": "        type=str,",
            "lineNumber": 725
          },
          {
            "text": "        default=\"mlp2\",",
            "lineNumber": 726
          },
          {
            "text": "        choices=[\"linear\", \"mlp2\", \"mlp3\", \"mlp4_high\", \"mlp4_low\", \"mlp5\", \"mlp2_ln\"],",
            "lineNumber": 727
          },
          {
            "text": "        help=\"Regression head architecture\",",
            "lineNumber": 728
          },
          {
            "text": "    )",
            "lineNumber": 729
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 730
          },
          {
            "text": "        \"--pooling-mode\",",
            "lineNumber": 731
          },
          {
            "text": "        type=str,",
            "lineNumber": 732
          },
          {
            "text": "        default=\"pool\",",
            "lineNumber": 733
          },
          {
            "text": "        choices=[\"pool\", \"last\", \"first\", \"sum\"],",
            "lineNumber": 734
          },
          {
            "text": "        help=\"Pooling mode for regression head\",",
            "lineNumber": 735
          },
          {
            "text": "    )",
            "lineNumber": 736
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 737
          },
          {
            "text": "        \"--freeze-backbone\",",
            "lineNumber": 738
          },
          {
            "text": "        action=\"store_true\",",
            "lineNumber": 739
          },
          {
            "text": "        help=\"Freeze backbone weights (only train head)\",",
            "lineNumber": 740
          },
          {
            "text": "    )",
            "lineNumber": 741
          },
          {
            "lineNumber": 742
          },
          {
            "text": "    # Training arguments",
            "lineNumber": 743
          },
          {
            "text": "    parser.add_argument(\"--batch-size\", type=int, default=4, help=\"Batch size\")",
            "lineNumber": 744
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 745
          },
          {
            "text": "        \"--learning-rate\", type=float, default=1e-4, help=\"Learning rate\"",
            "lineNumber": 746
          },
          {
            "text": "    )",
            "lineNumber": 747
          },
          {
            "text": "    parser.add_argument(\"--num-epochs\", type=int, default=10, help=\"Number of epochs\")",
            "lineNumber": 748
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 749
          },
          {
            "text": "        \"--loss-function\",",
            "lineNumber": 750
          },
          {
            "text": "        type=str,",
            "lineNumber": 751
          },
          {
            "text": "        default=\"smooth_l1\",",
            "lineNumber": 752
          },
          {
            "text": "        choices=[\"mse\", \"mae\", \"smooth_l1\", \"huber\"],",
            "lineNumber": 753
          },
          {
            "text": "        help=\"Loss function\",",
            "lineNumber": 754
          },
          {
            "text": "    )",
            "lineNumber": 755
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 756
          },
          {
            "text": "        \"--weight-decay\", type=float, default=0.01, help=\"Weight decay for AdamW\"",
            "lineNumber": 757
          },
          {
            "text": "    )",
            "lineNumber": 758
          },
          {
            "lineNumber": 759
          },
          {
            "text": "    # Data arguments",
            "lineNumber": 760
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 761
          },
          {
            "text": "        \"--dataset-repo\",",
            "lineNumber": 762
          },
          {
            "text": "        type=str,",
            "lineNumber": 763
          },
          {
            "text": "        default=\"jedick/microbial-DNA-pH\",",
            "lineNumber": 764
          },
          {
            "text": "        help=\"HuggingFace dataset repository\",",
            "lineNumber": 765
          },
          {
            "text": "    )",
            "lineNumber": 766
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 767
          },
          {
            "text": "        \"--random-seed\",",
            "lineNumber": 768
          },
          {
            "text": "        type=int,",
            "lineNumber": 769
          },
          {
            "text": "        default=42,",
            "lineNumber": 770
          },
          {
            "text": "        help=\"Random seed for reproducibility (default: 42)\",",
            "lineNumber": 771
          },
          {
            "text": "    )",
            "lineNumber": 772
          }
        ]
      },
      "score": 0.3514522910118103
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./predict_ph.py",
        "range": {
          "startPosition": {
            "line": 259,
            "column": 61
          },
          "endPosition": {
            "line": 510
          }
        },
        "contents": "def save_predictions(y_true, y_pred, metadata, output_path):\n    \"\"\"\n    Save predictions to CSV file with study_name and sample_id.\n\n    Parameters\n    ----------\n    y_true : array-like\n        True pH values\n    y_pred : array-like\n        Predicted pH values\n    metadata : pd.DataFrame\n        DataFrame with study_name and sample_id columns\n    output_path : str\n        Path to output CSV file\n    \"\"\"\n    results_df = pd.DataFrame(\n        {\n            \"study_name\": metadata[\"study_name\"].values,\n            \"sample_id\": metadata[\"sample_id\"].values,\n            \"true_pH\": y_true,\n            \"predicted_pH\": np.round(y_pred, 3),\n            \"residual\": np.round(y_true - y_pred, 3),\n        }\n    )\n\n    results_df.to_csv(output_path, index=False)\n    print(f\"Saved test predictions to {output_path}\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Predict pH from bacterial abundances using regression models\"\n    )\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        required=True,\n        choices=[\"linear\", \"knn\", \"rf\", \"hgb\"],\n        help=\"Model type: linear (Linear Regression), knn (KNN Regression), \"\n        \"rf (Random Forest), or hgb (HistGradientBoosting)\",\n    )\n    parser.add_argument(\n        \"--dataset-repo\",\n        type=str,\n        default=\"jedick/microbial-DNA-pH\",\n        help=\"HuggingFace dataset repository (default: jedick/microbial-DNA-pH)\",\n    )\n    parser.add_argument(\n        \"--phylum-counts\",\n        type=str,\n        default=\"data/bacteria_phylum_counts.csv.xz\",\n        help=\"Path to bacteria_phylum_counts.csv.xz (default: data/bacteria_phylum_counts.csv.xz)\",\n    )\n    parser.add_argument(\n        \"--random-seed\",\n        type=int,\n        default=42,\n        help=\"Random seed for reproducibility (default: 42)\",\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        type=str,\n        default=\"results\",\n        help=",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def save_predictions(y_true, y_pred, metadata, output_path):",
            "lineNumber": 263,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 264
          },
          {
            "text": "    Save predictions to CSV file with study_name and sample_id.",
            "lineNumber": 265
          },
          {
            "lineNumber": 266
          },
          {
            "text": "    Parameters",
            "lineNumber": 267
          },
          {
            "text": "    ----------",
            "lineNumber": 268
          },
          {
            "text": "    y_true : array-like",
            "lineNumber": 269
          },
          {
            "text": "        True pH values",
            "lineNumber": 270
          },
          {
            "text": "    y_pred : array-like",
            "lineNumber": 271
          },
          {
            "text": "        Predicted pH values",
            "lineNumber": 272
          },
          {
            "text": "    metadata : pd.DataFrame",
            "lineNumber": 273
          },
          {
            "text": "        DataFrame with study_name and sample_id columns",
            "lineNumber": 274
          },
          {
            "text": "    output_path : str",
            "lineNumber": 275
          },
          {
            "text": "        Path to output CSV file",
            "lineNumber": 276
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 277
          },
          {
            "text": "    results_df = pd.DataFrame(",
            "lineNumber": 278
          },
          {
            "text": "        {",
            "lineNumber": 279
          },
          {
            "text": "            \"study_name\": metadata[\"study_name\"].values,",
            "lineNumber": 280
          },
          {
            "text": "            \"sample_id\": metadata[\"sample_id\"].values,",
            "lineNumber": 281
          },
          {
            "text": "            \"true_pH\": y_true,",
            "lineNumber": 282
          },
          {
            "text": "            \"predicted_pH\": np.round(y_pred, 3),",
            "lineNumber": 283
          },
          {
            "text": "            \"residual\": np.round(y_true - y_pred, 3),",
            "lineNumber": 284
          },
          {
            "text": "        }",
            "lineNumber": 285
          },
          {
            "text": "    )",
            "lineNumber": 286
          },
          {
            "lineNumber": 287
          },
          {
            "text": "    results_df.to_csv(output_path, index=False)",
            "lineNumber": 288
          },
          {
            "text": "    print(f\"Saved test predictions to {output_path}\")",
            "lineNumber": 289,
            "isSignature": true
          },
          {
            "lineNumber": 290
          },
          {
            "lineNumber": 291
          },
          {
            "text": "def main():",
            "lineNumber": 292,
            "isSignature": true
          },
          {
            "text": "    parser = argparse.ArgumentParser(",
            "lineNumber": 293
          },
          {
            "text": "        description=\"Predict pH from bacterial abundances using regression models\"",
            "lineNumber": 294
          },
          {
            "text": "    )",
            "lineNumber": 295
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 296
          },
          {
            "text": "        \"--model\",",
            "lineNumber": 297
          },
          {
            "text": "        type=str,",
            "lineNumber": 298
          },
          {
            "text": "        required=True,",
            "lineNumber": 299
          },
          {
            "text": "        choices=[\"linear\", \"knn\", \"rf\", \"hgb\"],",
            "lineNumber": 300
          },
          {
            "text": "        help=\"Model type: linear (Linear Regression), knn (KNN Regression), \"",
            "lineNumber": 301
          },
          {
            "text": "        \"rf (Random Forest), or hgb (HistGradientBoosting)\",",
            "lineNumber": 302
          },
          {
            "text": "    )",
            "lineNumber": 303
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 304
          },
          {
            "text": "        \"--dataset-repo\",",
            "lineNumber": 305
          },
          {
            "text": "        type=str,",
            "lineNumber": 306
          },
          {
            "text": "        default=\"jedick/microbial-DNA-pH\",",
            "lineNumber": 307
          },
          {
            "text": "        help=\"HuggingFace dataset repository (default: jedick/microbial-DNA-pH)\",",
            "lineNumber": 308
          },
          {
            "text": "    )",
            "lineNumber": 309
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 310
          },
          {
            "text": "        \"--phylum-counts\",",
            "lineNumber": 311
          },
          {
            "text": "        type=str,",
            "lineNumber": 312
          },
          {
            "text": "        default=\"data/bacteria_phylum_counts.csv.xz\",",
            "lineNumber": 313
          },
          {
            "text": "        help=\"Path to bacteria_phylum_counts.csv.xz (default: data/bacteria_phylum_counts.csv.xz)\",",
            "lineNumber": 314
          },
          {
            "text": "    )",
            "lineNumber": 315
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 316
          },
          {
            "text": "        \"--random-seed\",",
            "lineNumber": 317
          },
          {
            "text": "        type=int,",
            "lineNumber": 318
          },
          {
            "text": "        default=42,",
            "lineNumber": 319
          },
          {
            "text": "        help=\"Random seed for reproducibility (default: 42)\",",
            "lineNumber": 320
          },
          {
            "text": "    )",
            "lineNumber": 321
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 322
          },
          {
            "text": "        \"--output-dir\",",
            "lineNumber": 323
          },
          {
            "text": "        type=str,",
            "lineNumber": 324
          },
          {
            "text": "        default=\"results\",",
            "lineNumber": 325
          },
          {
            "text": "        help=",
            "lineNumber": 326
          }
        ]
      },
      "score": 0.3467501401901245
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./results/hyenadna_ph/config.json",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 15,
            "column": 1
          }
        },
        "contents": "{\n  \"model_name\": \"hyenadna-small-32k-seqlen\",\n  \"head_architecture\": \"mlp4_low\",\n  \"pooling_mode\": \"pool\",\n  \"freeze_backbone\": false,\n  \"batch_size\": 2,\n  \"learning_rate\": 0.0001,\n  \"num_epochs\": 20,\n  \"loss_function\": \"smooth_l1\",\n  \"weight_decay\": 0.01,\n  \"dataset_repo\": \"jedick/microbial-DNA-pH\",\n  \"random_seed\": 42,\n  \"output_dir\": \"results/hyenadna_ph\",\n  \"checkpoint_dir\": \"./checkpoints\",\n  \"download_model\": false\n}",
        "signatures": {},
        "detailedLines": [
          {
            "text": "{",
            "lineNumber": 1
          },
          {
            "text": "  \"model_name\": \"hyenadna-small-32k-seqlen\",",
            "lineNumber": 2
          },
          {
            "text": "  \"head_architecture\": \"mlp4_low\",",
            "lineNumber": 3
          },
          {
            "text": "  \"pooling_mode\": \"pool\",",
            "lineNumber": 4
          },
          {
            "text": "  \"freeze_backbone\": false,",
            "lineNumber": 5
          },
          {
            "text": "  \"batch_size\": 2,",
            "lineNumber": 6
          },
          {
            "text": "  \"learning_rate\": 0.0001,",
            "lineNumber": 7
          },
          {
            "text": "  \"num_epochs\": 20,",
            "lineNumber": 8
          },
          {
            "text": "  \"loss_function\": \"smooth_l1\",",
            "lineNumber": 9
          },
          {
            "text": "  \"weight_decay\": 0.01,",
            "lineNumber": 10
          },
          {
            "text": "  \"dataset_repo\": \"jedick/microbial-DNA-pH\",",
            "lineNumber": 11
          },
          {
            "text": "  \"random_seed\": 42,",
            "lineNumber": 12
          },
          {
            "text": "  \"output_dir\": \"results/hyenadna_ph\",",
            "lineNumber": 13
          },
          {
            "text": "  \"checkpoint_dir\": \"./checkpoints\",",
            "lineNumber": 14
          },
          {
            "text": "  \"download_model\": false",
            "lineNumber": 15
          },
          {
            "text": "}",
            "lineNumber": 16
          }
        ]
      },
      "score": 0.3436565101146698
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./README.md",
        "range": {
          "startPosition": {
            "line": 109
          },
          "endPosition": {
            "line": 161
          }
        },
        "contents": "3. **Model Training**:\n   - Optional grid search with progress bar for hyperparameter tuning\n   - Uses scikit-learn Pipeline for preprocessing and model fitting\n\n4. **Output**:\n   - Saves test set predictions to CSV files with study names and sample IDs\n   - Generates summary files with model metrics (RMSE, R-squared, MAE) and grid search results\n\n**Available Models:**\n- &#96;linear&#96;: Linear Regression\n- &#96;knn&#96;: K-Nearest Neighbors Regression\n- &#96;rf&#96;: Random Forest Regression\n- &#96;hgb&#96;: HistGradientBoosting Regression\n\n**Grid Search Modes:**\n- &#96;test&#96;: Small grids (2-4 combinations) for quick testing\n- &#96;full&#96;: Comprehensive grids (hundreds of combinations) for thorough tuning\n- &#96;none&#96;: Skip grid search and use default parameters\n\n**Output Files:**\n- &#96;{model}_test_predictions.csv&#96;: Test set predictions with study_name, sample_id, true_pH, predicted_pH, and residual\n- &#96;{model}_summary.txt&#96;: Model summary with metrics and grid search results (if performed)\n\n&lt;/details&gt;\n\n## pH prediction from HyenaDNA sequence model\n\nThe &#96;train_hyenadna_ph.py&#96; script trains a HyenaDNA model with a regression head to predict continuous pH values from DNA sequences. The script loads data from the HuggingFace dataset, uses pretrained HyenaDNA weights, concatenates multiple sequences per sample with [SEP] tokens, and performs a 75:5:20 train-val-test split (same test set as the traditional ML workflow).\n\n&#96;&#96;&#96;bash\n# Basic training with defaults (tiny model, downloads from HuggingFace)\npython train_hyenadna_ph.py --download-model\n\n# Custom configuration with larger model\npython train_hyenadna_ph.py \\\n    --model-name hyenadna-small-32k-seqlen \\\n    --head-architecture mlp3 \\\n    --pooling-mode last \\\n    --batch-size 2 \\\n    --learning-rate 5e-5 \\\n    --num-epochs 20 \\\n    --freeze-backbone \\\n    --download-model\n\n# Train with frozen backbone (only regression head)\npython train_hyenadna_ph.py --freeze-backbone --download-model\n&#96;&#96;&#96;\n\n&lt;details&gt;\n&lt;summary&gt;Details&lt;/summary&gt;\n\nThe script implements a separate &#96;RegressionHead&#96; class that maintains separation from the HyenaDNA codebase. It loads DNA sequences from the HuggingFace dataset (&#96;jedick/microbial-DNA-pH&#96;), concatenates multiple sequences per sample with [SEP] tokens up to the model's maximum length, and fine-tunes pretrained HyenaDNA weights for pH prediction.",
        "signatures": {},
        "detailedLines": [
          {
            "text": "3. **Model Training**:",
            "lineNumber": 110
          },
          {
            "text": "   - Optional grid search with progress bar for hyperparameter tuning",
            "lineNumber": 111
          },
          {
            "text": "   - Uses scikit-learn Pipeline for preprocessing and model fitting",
            "lineNumber": 112
          },
          {
            "lineNumber": 113
          },
          {
            "text": "4. **Output**:",
            "lineNumber": 114
          },
          {
            "text": "   - Saves test set predictions to CSV files with study names and sample IDs",
            "lineNumber": 115
          },
          {
            "text": "   - Generates summary files with model metrics (RMSE, R-squared, MAE) and grid search results",
            "lineNumber": 116
          },
          {
            "lineNumber": 117
          },
          {
            "text": "**Available Models:**",
            "lineNumber": 118
          },
          {
            "text": "- &#96;linear&#96;: Linear Regression",
            "lineNumber": 119
          },
          {
            "text": "- &#96;knn&#96;: K-Nearest Neighbors Regression",
            "lineNumber": 120
          },
          {
            "text": "- &#96;rf&#96;: Random Forest Regression",
            "lineNumber": 121
          },
          {
            "text": "- &#96;hgb&#96;: HistGradientBoosting Regression",
            "lineNumber": 122
          },
          {
            "lineNumber": 123
          },
          {
            "text": "**Grid Search Modes:**",
            "lineNumber": 124
          },
          {
            "text": "- &#96;test&#96;: Small grids (2-4 combinations) for quick testing",
            "lineNumber": 125
          },
          {
            "text": "- &#96;full&#96;: Comprehensive grids (hundreds of combinations) for thorough tuning",
            "lineNumber": 126
          },
          {
            "text": "- &#96;none&#96;: Skip grid search and use default parameters",
            "lineNumber": 127
          },
          {
            "lineNumber": 128
          },
          {
            "text": "**Output Files:**",
            "lineNumber": 129
          },
          {
            "text": "- &#96;{model}_test_predictions.csv&#96;: Test set predictions with study_name, sample_id, true_pH, predicted_pH, and residual",
            "lineNumber": 130
          },
          {
            "text": "- &#96;{model}_summary.txt&#96;: Model summary with metrics and grid search results (if performed)",
            "lineNumber": 131
          },
          {
            "lineNumber": 132
          },
          {
            "text": "&lt;/details&gt;",
            "lineNumber": 133
          },
          {
            "lineNumber": 134
          },
          {
            "text": "## pH prediction from HyenaDNA sequence model",
            "lineNumber": 135
          },
          {
            "lineNumber": 136
          },
          {
            "text": "The &#96;train_hyenadna_ph.py&#96; script trains a HyenaDNA model with a regression head to predict continuous pH values from DNA sequences. The script loads data from the HuggingFace dataset, uses pretrained HyenaDNA weights, concatenates multiple sequences per sample with [SEP] tokens, and performs a 75:5:20 train-val-test split (same test set as the traditional ML workflow).",
            "lineNumber": 137
          },
          {
            "lineNumber": 138
          },
          {
            "text": "&#96;&#96;&#96;bash",
            "lineNumber": 139
          },
          {
            "text": "# Basic training with defaults (tiny model, downloads from HuggingFace)",
            "lineNumber": 140
          },
          {
            "text": "python train_hyenadna_ph.py --download-model",
            "lineNumber": 141
          },
          {
            "lineNumber": 142
          },
          {
            "text": "# Custom configuration with larger model",
            "lineNumber": 143
          },
          {
            "text": "python train_hyenadna_ph.py \\",
            "lineNumber": 144
          },
          {
            "text": "    --model-name hyenadna-small-32k-seqlen \\",
            "lineNumber": 145
          },
          {
            "text": "    --head-architecture mlp3 \\",
            "lineNumber": 146
          },
          {
            "text": "    --pooling-mode last \\",
            "lineNumber": 147
          },
          {
            "text": "    --batch-size 2 \\",
            "lineNumber": 148
          },
          {
            "text": "    --learning-rate 5e-5 \\",
            "lineNumber": 149
          },
          {
            "text": "    --num-epochs 20 \\",
            "lineNumber": 150
          },
          {
            "text": "    --freeze-backbone \\",
            "lineNumber": 151
          },
          {
            "text": "    --download-model",
            "lineNumber": 152
          },
          {
            "lineNumber": 153
          },
          {
            "text": "# Train with frozen backbone (only regression head)",
            "lineNumber": 154
          },
          {
            "text": "python train_hyenadna_ph.py --freeze-backbone --download-model",
            "lineNumber": 155
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 156
          },
          {
            "lineNumber": 157
          },
          {
            "text": "&lt;details&gt;",
            "lineNumber": 158
          },
          {
            "text": "&lt;summary&gt;Details&lt;/summary&gt;",
            "lineNumber": 159
          },
          {
            "lineNumber": 160
          },
          {
            "text": "The script implements a separate &#96;RegressionHead&#96; class that maintains separation from the HyenaDNA codebase. It loads DNA sequences from the HuggingFace dataset (&#96;jedick/microbial-DNA-pH&#96;), concatenates multiple sequences per sample with [SEP] tokens up to the model's maximum length, and fine-tunes pretrained HyenaDNA weights for pH prediction.",
            "lineNumber": 161
          }
        ]
      },
      "score": 0.31558704376220703
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./README.md",
        "range": {
          "startPosition": {
            "line": 154
          },
          "endPosition": {
            "line": 182
          }
        },
        "contents": "python train_hyenadna_ph.py --freeze-backbone --download-model\n&#96;&#96;&#96;\n\n&lt;details&gt;\n&lt;summary&gt;Details&lt;/summary&gt;\n\nThe script implements a separate &#96;RegressionHead&#96; class that maintains separation from the HyenaDNA codebase. It loads DNA sequences from the HuggingFace dataset (&#96;jedick/microbial-DNA-pH&#96;), concatenates multiple sequences per sample with [SEP] tokens up to the model's maximum length, and fine-tunes pretrained HyenaDNA weights for pH prediction.\n\n- **Model options**: Supports all HyenaDNA model sizes (tiny-1k to large-1m-seqlen), default is tiny-1k-seqlen for testing\n- **Regression head architectures**: Linear, 2-layer MLP (default), 3-layer MLP, or 2-layer MLP with LayerNorm\n- **Pooling modes**: Mean pooling (default), last token, first token, or sum pooling\n- **Sequence handling**: Concatenates multiple DNA sequences per sample with [SEP] tokens, truncates to max_length if needed\n- **Training options**: Can freeze backbone (only train head) or fine-tune entire model, configurable loss functions (MSE, MAE, SmoothL1, Huber)\n- **Evaluation**: Computes MSE, RMSE, MAE, and RÂ² metrics on train/val/test splits\n- **Data split**: Creates a 75:5:20 train-val-test split using the same random seed (42) and test set as the traditional ML workflow\n\n**Output Files:**\n- &#96;best_model.pt&#96;: Best model checkpoint based on validation loss\n- &#96;training_log.json&#96;: Training history with metrics per epoch\n- &#96;test_predictions.csv&#96;: Test set predictions with true and predicted pH values\n- &#96;config.json&#96;: Training configuration used\n\n**Key Hyperparameters:**\n- Default batch size: 4 (adjust based on GPU memory and sequence length)\n- Default learning rate: 1e-4 (typical for fine-tuning)\n- Default loss: SmoothL1Loss (good balance between MSE and MAE)\n- Train/val/test split: 75:5:20 (same test set as traditional ML workflow)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "python train_hyenadna_ph.py --freeze-backbone --download-model",
            "lineNumber": 155
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 156
          },
          {
            "lineNumber": 157
          },
          {
            "text": "&lt;details&gt;",
            "lineNumber": 158
          },
          {
            "text": "&lt;summary&gt;Details&lt;/summary&gt;",
            "lineNumber": 159
          },
          {
            "lineNumber": 160
          },
          {
            "text": "The script implements a separate &#96;RegressionHead&#96; class that maintains separation from the HyenaDNA codebase. It loads DNA sequences from the HuggingFace dataset (&#96;jedick/microbial-DNA-pH&#96;), concatenates multiple sequences per sample with [SEP] tokens up to the model's maximum length, and fine-tunes pretrained HyenaDNA weights for pH prediction.",
            "lineNumber": 161
          },
          {
            "lineNumber": 162
          },
          {
            "text": "- **Model options**: Supports all HyenaDNA model sizes (tiny-1k to large-1m-seqlen), default is tiny-1k-seqlen for testing",
            "lineNumber": 163
          },
          {
            "text": "- **Regression head architectures**: Linear, 2-layer MLP (default), 3-layer MLP, or 2-layer MLP with LayerNorm",
            "lineNumber": 164
          },
          {
            "text": "- **Pooling modes**: Mean pooling (default), last token, first token, or sum pooling",
            "lineNumber": 165
          },
          {
            "text": "- **Sequence handling**: Concatenates multiple DNA sequences per sample with [SEP] tokens, truncates to max_length if needed",
            "lineNumber": 166
          },
          {
            "text": "- **Training options**: Can freeze backbone (only train head) or fine-tune entire model, configurable loss functions (MSE, MAE, SmoothL1, Huber)",
            "lineNumber": 167
          },
          {
            "text": "- **Evaluation**: Computes MSE, RMSE, MAE, and RÂ² metrics on train/val/test splits",
            "lineNumber": 168
          },
          {
            "text": "- **Data split**: Creates a 75:5:20 train-val-test split using the same random seed (42) and test set as the traditional ML workflow",
            "lineNumber": 169
          },
          {
            "lineNumber": 170
          },
          {
            "text": "**Output Files:**",
            "lineNumber": 171
          },
          {
            "text": "- &#96;best_model.pt&#96;: Best model checkpoint based on validation loss",
            "lineNumber": 172
          },
          {
            "text": "- &#96;training_log.json&#96;: Training history with metrics per epoch",
            "lineNumber": 173
          },
          {
            "text": "- &#96;test_predictions.csv&#96;: Test set predictions with true and predicted pH values",
            "lineNumber": 174
          },
          {
            "text": "- &#96;config.json&#96;: Training configuration used",
            "lineNumber": 175
          },
          {
            "lineNumber": 176
          },
          {
            "text": "**Key Hyperparameters:**",
            "lineNumber": 177
          },
          {
            "text": "- Default batch size: 4 (adjust based on GPU memory and sequence length)",
            "lineNumber": 178
          },
          {
            "text": "- Default learning rate: 1e-4 (typical for fine-tuning)",
            "lineNumber": 179
          },
          {
            "text": "- Default loss: SmoothL1Loss (good balance between MSE and MAE)",
            "lineNumber": 180
          },
          {
            "text": "- Train/val/test split: 75:5:20 (same test set as traditional ML workflow)",
            "lineNumber": 181
          }
        ]
      },
      "score": 0.31535160541534424
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./README.md",
        "range": {
          "startPosition": {
            "line": 63
          },
          "endPosition": {
            "line": 113
          }
        },
        "contents": "&lt;/details&gt;\n\n## RDP phylum counts (features)\n\nThe &#96;process_rdp_data.py&#96; script is used to aggregate taxonomic counts from RDP classifier output files. The script processes &#96;.tab&#96; files in the &#96;data/RDP_classifier&#96; directory and generates aggregated count tables saved as CSV files in the &#96;data&#96; directory. The output files contain taxonomic counts at various levels (phylum, class, order, family, genus) with samples as rows and taxonomic groups as columns.\n\n**Note**: The source RDP classifier files are not stored in this repository but can be downloaded from the [JMDplots repository](https://github.com/jedick/JMDplots/tree/main/inst/extdata/orp16S/RDP-GTDB).\n\n## pH prediction from phylum counts\n\nThe &#96;predict_ph.py&#96; script implements a machine learning pipeline for predicting pH values from bacterial phylum abundances. The workflow loads sample metadata from the HuggingFace dataset, merges it with phylum count features, and performs an 80:20 train-test split (same test set as the deep learning workflow).\n\n&#96;&#96;&#96;bash\n# Test grid search with KNN (default, 4 combinations)\npython predict_ph.py --model knn --grid-search test\n\n# Full grid search with Random Forest (comprehensive)\npython predict_ph.py --model rf --grid-search full\n\n# Skip grid search and use default parameters\npython predict_ph.py --model hgb --grid-search none\n\n# Custom CV folds and random seed\npython predict_ph.py --model knn --grid-search test --cv-folds 10 --random-seed 123\n\n# Specify custom dataset and output directory\npython predict_ph.py --model rf \\\n    --dataset-repo jedick/microbial-DNA-pH \\\n    --phylum-counts data/bacteria_phylum_counts.csv.xz \\\n    --output-dir results/rf_experiment\n&#96;&#96;&#96;\n\n&lt;details&gt;\n&lt;summary&gt;Details&lt;/summary&gt;\n\n1. **Data Loading and Preprocessing**:\n   - Loads sample metadata from HuggingFace dataset (filters null pH values)\n   - Merges with phylum count data from CSV file\n   - Performs L1 normalization to convert counts to relative abundances\n\n2. **Train-Test Split**:\n   - Creates an 80:20 stratified split based on environment\n   - Uses the same random seed (42) and test set as the deep learning workflow\n   - Ensures balanced representation of environment types in both sets\n\n3. **Model Training**:\n   - Optional grid search with progress bar for hyperparameter tuning\n   - Uses scikit-learn Pipeline for preprocessing and model fitting",
        "signatures": {},
        "detailedLines": [
          {
            "text": "&lt;/details&gt;",
            "lineNumber": 65
          },
          {
            "lineNumber": 66
          },
          {
            "text": "## RDP phylum counts (features)",
            "lineNumber": 67
          },
          {
            "lineNumber": 68
          },
          {
            "text": "The &#96;process_rdp_data.py&#96; script is used to aggregate taxonomic counts from RDP classifier output files. The script processes &#96;.tab&#96; files in the &#96;data/RDP_classifier&#96; directory and generates aggregated count tables saved as CSV files in the &#96;data&#96; directory. The output files contain taxonomic counts at various levels (phylum, class, order, family, genus) with samples as rows and taxonomic groups as columns.",
            "lineNumber": 69
          },
          {
            "lineNumber": 70
          },
          {
            "text": "**Note**: The source RDP classifier files are not stored in this repository but can be downloaded from the [JMDplots repository](https://github.com/jedick/JMDplots/tree/main/inst/extdata/orp16S/RDP-GTDB).",
            "lineNumber": 71
          },
          {
            "lineNumber": 72
          },
          {
            "text": "## pH prediction from phylum counts",
            "lineNumber": 73
          },
          {
            "lineNumber": 74
          },
          {
            "text": "The &#96;predict_ph.py&#96; script implements a machine learning pipeline for predicting pH values from bacterial phylum abundances. The workflow loads sample metadata from the HuggingFace dataset, merges it with phylum count features, and performs an 80:20 train-test split (same test set as the deep learning workflow).",
            "lineNumber": 75
          },
          {
            "lineNumber": 76
          },
          {
            "text": "&#96;&#96;&#96;bash",
            "lineNumber": 77
          },
          {
            "text": "# Test grid search with KNN (default, 4 combinations)",
            "lineNumber": 78
          },
          {
            "text": "python predict_ph.py --model knn --grid-search test",
            "lineNumber": 79
          },
          {
            "lineNumber": 80
          },
          {
            "text": "# Full grid search with Random Forest (comprehensive)",
            "lineNumber": 81
          },
          {
            "text": "python predict_ph.py --model rf --grid-search full",
            "lineNumber": 82
          },
          {
            "lineNumber": 83
          },
          {
            "text": "# Skip grid search and use default parameters",
            "lineNumber": 84
          },
          {
            "text": "python predict_ph.py --model hgb --grid-search none",
            "lineNumber": 85
          },
          {
            "lineNumber": 86
          },
          {
            "text": "# Custom CV folds and random seed",
            "lineNumber": 87
          },
          {
            "text": "python predict_ph.py --model knn --grid-search test --cv-folds 10 --random-seed 123",
            "lineNumber": 88
          },
          {
            "lineNumber": 89
          },
          {
            "text": "# Specify custom dataset and output directory",
            "lineNumber": 90
          },
          {
            "text": "python predict_ph.py --model rf \\",
            "lineNumber": 91
          },
          {
            "text": "    --dataset-repo jedick/microbial-DNA-pH \\",
            "lineNumber": 92
          },
          {
            "text": "    --phylum-counts data/bacteria_phylum_counts.csv.xz \\",
            "lineNumber": 93
          },
          {
            "text": "    --output-dir results/rf_experiment",
            "lineNumber": 94
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 95
          },
          {
            "lineNumber": 96
          },
          {
            "text": "&lt;details&gt;",
            "lineNumber": 97
          },
          {
            "text": "&lt;summary&gt;Details&lt;/summary&gt;",
            "lineNumber": 98
          },
          {
            "lineNumber": 99
          },
          {
            "text": "1. **Data Loading and Preprocessing**:",
            "lineNumber": 100
          },
          {
            "text": "   - Loads sample metadata from HuggingFace dataset (filters null pH values)",
            "lineNumber": 101
          },
          {
            "text": "   - Merges with phylum count data from CSV file",
            "lineNumber": 102
          },
          {
            "text": "   - Performs L1 normalization to convert counts to relative abundances",
            "lineNumber": 103
          },
          {
            "lineNumber": 104
          },
          {
            "text": "2. **Train-Test Split**:",
            "lineNumber": 105
          },
          {
            "text": "   - Creates an 80:20 stratified split based on environment",
            "lineNumber": 106
          },
          {
            "text": "   - Uses the same random seed (42) and test set as the deep learning workflow",
            "lineNumber": 107
          },
          {
            "text": "   - Ensures balanced representation of environment types in both sets",
            "lineNumber": 108
          },
          {
            "lineNumber": 109
          },
          {
            "text": "3. **Model Training**:",
            "lineNumber": 110
          },
          {
            "text": "   - Optional grid search with progress bar for hyperparameter tuning",
            "lineNumber": 111
          },
          {
            "text": "   - Uses scikit-learn Pipeline for preprocessing and model fitting",
            "lineNumber": 112
          }
        ]
      },
      "score": 0.2880178689956665
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 45,
            "column": 16
          }
        },
        "contents": "#!/usr/bin/env python3\n\"\"\"\nTrain HyenaDNA model for pH prediction from DNA sequences.\n\nThis script implements a regression head for HyenaDNA to predict continuous pH values.\nIt loads data from HuggingFace, concatenates multiple sequences with [SEP] tokens,\nand trains using pretrained HyenaDNA weights.\n\"\"\"\n\nimport argparse\nimport json\nimport os\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom datasets import load_dataset\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\n\n# Add hyena-dna to path\nsys.path.insert(0, str(Path(__file__).parent / \"hyena-dna\"))\n\nfrom huggingface_wrapper import HyenaDNAPreTrainedModel\nfrom standalone_hyenadna import CharacterTokenizer, HyenaDNAModel\n\n# Model size configurations\nMODEL_CONFIGS = {\n    \"hyenadna-tiny-1k-seqlen\": {\"max_length\": 1024},\n    \"hyenadna-small-32k-seqlen\": {\"max_length\": 32768},\n    \"hyenadna-medium-160k-seqlen\": {\"max_length\": 160000},\n    \"hyenadna-medium-450k-seqlen\": {\"max_length\": 450000},\n    \"hyenadna-large-1m-seqlen\": {\"max_length\": 1_000_000},\n}\n\n# Special token IDs\nSEP_TOKEN_ID = 1\nPAD_TOKEN_ID = 4\nCLS_TOKEN_ID = 0",
        "signatures": {},
        "detailedLines": [
          {
            "text": "#!/usr/bin/env python3",
            "lineNumber": 1
          },
          {
            "text": "\"\"\"",
            "lineNumber": 2
          },
          {
            "text": "Train HyenaDNA model for pH prediction from DNA sequences.",
            "lineNumber": 3
          },
          {
            "lineNumber": 4
          },
          {
            "text": "This script implements a regression head for HyenaDNA to predict continuous pH values.",
            "lineNumber": 5
          },
          {
            "text": "It loads data from HuggingFace, concatenates multiple sequences with [SEP] tokens,",
            "lineNumber": 6
          },
          {
            "text": "and trains using pretrained HyenaDNA weights.",
            "lineNumber": 7
          },
          {
            "text": "\"\"\"",
            "lineNumber": 8
          },
          {
            "lineNumber": 9
          },
          {
            "text": "import argparse",
            "lineNumber": 10
          },
          {
            "text": "import json",
            "lineNumber": 11
          },
          {
            "text": "import os",
            "lineNumber": 12
          },
          {
            "text": "import sys",
            "lineNumber": 13
          },
          {
            "text": "from pathlib import Path",
            "lineNumber": 14
          },
          {
            "text": "from typing import Dict, List, Optional, Tuple",
            "lineNumber": 15
          },
          {
            "lineNumber": 16
          },
          {
            "text": "import numpy as np",
            "lineNumber": 17
          },
          {
            "text": "import torch",
            "lineNumber": 18
          },
          {
            "text": "import torch.nn as nn",
            "lineNumber": 19
          },
          {
            "text": "import torch.nn.functional as F",
            "lineNumber": 20
          },
          {
            "text": "from datasets import load_dataset",
            "lineNumber": 21
          },
          {
            "text": "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score",
            "lineNumber": 22
          },
          {
            "text": "from sklearn.model_selection import train_test_split",
            "lineNumber": 23
          },
          {
            "lineNumber": 24
          },
          {
            "text": "from torch.utils.data import DataLoader, Dataset",
            "lineNumber": 25
          },
          {
            "text": "from tqdm import tqdm",
            "lineNumber": 26
          },
          {
            "lineNumber": 27
          },
          {
            "text": "# Add hyena-dna to path",
            "lineNumber": 28
          },
          {
            "text": "sys.path.insert(0, str(Path(__file__).parent / \"hyena-dna\"))",
            "lineNumber": 29
          },
          {
            "lineNumber": 30
          },
          {
            "text": "from huggingface_wrapper import HyenaDNAPreTrainedModel",
            "lineNumber": 31
          },
          {
            "text": "from standalone_hyenadna import CharacterTokenizer, HyenaDNAModel",
            "lineNumber": 32
          },
          {
            "lineNumber": 33
          },
          {
            "text": "# Model size configurations",
            "lineNumber": 34
          },
          {
            "text": "MODEL_CONFIGS = {",
            "lineNumber": 35
          },
          {
            "text": "    \"hyenadna-tiny-1k-seqlen\": {\"max_length\": 1024},",
            "lineNumber": 36
          },
          {
            "text": "    \"hyenadna-small-32k-seqlen\": {\"max_length\": 32768},",
            "lineNumber": 37
          },
          {
            "text": "    \"hyenadna-medium-160k-seqlen\": {\"max_length\": 160000},",
            "lineNumber": 38
          },
          {
            "text": "    \"hyenadna-medium-450k-seqlen\": {\"max_length\": 450000},",
            "lineNumber": 39
          },
          {
            "text": "    \"hyenadna-large-1m-seqlen\": {\"max_length\": 1_000_000},",
            "lineNumber": 40
          },
          {
            "text": "}",
            "lineNumber": 41
          },
          {
            "lineNumber": 42
          },
          {
            "text": "# Special token IDs",
            "lineNumber": 43
          },
          {
            "text": "SEP_TOKEN_ID = 1",
            "lineNumber": 44
          },
          {
            "text": "PAD_TOKEN_ID = 4",
            "lineNumber": 45
          },
          {
            "text": "CLS_TOKEN_ID = 0",
            "lineNumber": 46
          }
        ]
      },
      "score": 0.2866925001144409
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./predict_ph.py",
        "range": {
          "startPosition": {
            "line": 291
          },
          "endPosition": {
            "line": 510
          }
        },
        "contents": "def main():\n\n        pipeline.fit(X_train, y_train)\n\n    # Make predictions\n    print(\"Making predictions...\")\n    y_train_pred = pipeline.predict(X_train)\n    y_test_pred = pipeline.predict(X_test)\n\n    # Calculate and print metrics\n    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n    train_r2 = r2_score(y_train, y_train_pred)\n    test_r2 = r2_score(y_test, y_test_pred)\n    train_mae = mean_absolute_error(y_train, y_train_pred)\n    test_mae = mean_absolute_error(y_test, y_test_pred)\n\n    print(f\"\\n{'='*60}\")\n    print(f\"Model: {args.model.upper()}\")\n    print(f\"{'='*60}\")\n    print(f\"Train RMSE: {train_rmse:.4f}\")\n    print(f\"Test RMSE:  {test_rmse:.4f}\")\n    print(f\"Train R-squared:   {train_r2:.4f}\")\n    print(f\"Test R-squared:    {test_r2:.4f}\")\n    print(f\"Train MAE:  {train_mae:.4f}\")\n    print(f\"Test MAE:   {test_mae:.4f}\")\n    print(f\"{'='*60}\\n\")\n\n    # Save test set predictions only\n    test_output_path = output_dir / f\"{args.model}_test_predictions.csv\"\n    save_predictions(y_test, y_test_pred, test_metadata, test_output_path)\n\n    # Save model summary\n    summary_path = output_dir / f\"{args.model}_summary.txt\"\n    with open(summary_path, \"w\") as f:\n        f.write(f\"Model: {args.model.upper()}\\n\")\n        f.write(f\"Random Seed: {args.random_seed}\\n\")\n        f.write(f\"Number of features: {len(feature_names)}\\n\")\n        f.write(f\"Train set size: {X_train.shape[0]}\\n\")\n        f.write(f\"Test set size: {X_test.shape[0]}\\n\")\n        # Grid search information\n        if (\n            args.grid_search != \"none\"\n            and args.model != \"linear\"\n            and best_params is not None\n        ):\n            f.write(f\"\\nGrid Search:\\n\")\n            f.write(f\"  Mode: {args.grid_search}\\n\")\n            f.write(f\"  CV folds: {args.cv_folds}\\n\")\n            f.write(f\"  Number of combinations: {n_combinations}\\n\")",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 263,
                "column": 1
              },
              "endPosition": {
                "line": 264,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def main():",
            "lineNumber": 292,
            "isSignature": true
          },
          {
            "lineNumber": 432
          },
          {
            "text": "        pipeline.fit(X_train, y_train)",
            "lineNumber": 433
          },
          {
            "lineNumber": 434
          },
          {
            "text": "    # Make predictions",
            "lineNumber": 435
          },
          {
            "text": "    print(\"Making predictions...\")",
            "lineNumber": 436
          },
          {
            "text": "    y_train_pred = pipeline.predict(X_train)",
            "lineNumber": 437
          },
          {
            "text": "    y_test_pred = pipeline.predict(X_test)",
            "lineNumber": 438
          },
          {
            "lineNumber": 439
          },
          {
            "text": "    # Calculate and print metrics",
            "lineNumber": 440
          },
          {
            "text": "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error",
            "lineNumber": 441
          },
          {
            "lineNumber": 442
          },
          {
            "text": "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))",
            "lineNumber": 443
          },
          {
            "text": "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))",
            "lineNumber": 444
          },
          {
            "text": "    train_r2 = r2_score(y_train, y_train_pred)",
            "lineNumber": 445
          },
          {
            "text": "    test_r2 = r2_score(y_test, y_test_pred)",
            "lineNumber": 446
          },
          {
            "text": "    train_mae = mean_absolute_error(y_train, y_train_pred)",
            "lineNumber": 447
          },
          {
            "text": "    test_mae = mean_absolute_error(y_test, y_test_pred)",
            "lineNumber": 448
          },
          {
            "lineNumber": 449
          },
          {
            "text": "    print(f\"\\n{'='*60}\")",
            "lineNumber": 450
          },
          {
            "text": "    print(f\"Model: {args.model.upper()}\")",
            "lineNumber": 451
          },
          {
            "text": "    print(f\"{'='*60}\")",
            "lineNumber": 452
          },
          {
            "text": "    print(f\"Train RMSE: {train_rmse:.4f}\")",
            "lineNumber": 453
          },
          {
            "text": "    print(f\"Test RMSE:  {test_rmse:.4f}\")",
            "lineNumber": 454
          },
          {
            "text": "    print(f\"Train R-squared:   {train_r2:.4f}\")",
            "lineNumber": 455
          },
          {
            "text": "    print(f\"Test R-squared:    {test_r2:.4f}\")",
            "lineNumber": 456
          },
          {
            "text": "    print(f\"Train MAE:  {train_mae:.4f}\")",
            "lineNumber": 457
          },
          {
            "text": "    print(f\"Test MAE:   {test_mae:.4f}\")",
            "lineNumber": 458
          },
          {
            "text": "    print(f\"{'='*60}\\n\")",
            "lineNumber": 459
          },
          {
            "lineNumber": 460
          },
          {
            "text": "    # Save test set predictions only",
            "lineNumber": 461
          },
          {
            "text": "    test_output_path = output_dir / f\"{args.model}_test_predictions.csv\"",
            "lineNumber": 462
          },
          {
            "text": "    save_predictions(y_test, y_test_pred, test_metadata, test_output_path)",
            "lineNumber": 463
          },
          {
            "lineNumber": 464
          },
          {
            "text": "    # Save model summary",
            "lineNumber": 465
          },
          {
            "text": "    summary_path = output_dir / f\"{args.model}_summary.txt\"",
            "lineNumber": 466
          },
          {
            "text": "    with open(summary_path, \"w\") as f:",
            "lineNumber": 467
          },
          {
            "text": "        f.write(f\"Model: {args.model.upper()}\\n\")",
            "lineNumber": 468
          },
          {
            "text": "        f.write(f\"Random Seed: {args.random_seed}\\n\")",
            "lineNumber": 469
          },
          {
            "text": "        f.write(f\"Number of features: {len(feature_names)}\\n\")",
            "lineNumber": 470
          },
          {
            "text": "        f.write(f\"Train set size: {X_train.shape[0]}\\n\")",
            "lineNumber": 471
          },
          {
            "text": "        f.write(f\"Test set size: {X_test.shape[0]}\\n\")",
            "lineNumber": 472
          },
          {
            "text": "        # Grid search information",
            "lineNumber": 473
          },
          {
            "text": "        if (",
            "lineNumber": 474
          },
          {
            "text": "            args.grid_search != \"none\"",
            "lineNumber": 475
          },
          {
            "text": "            and args.model != \"linear\"",
            "lineNumber": 476
          },
          {
            "text": "            and best_params is not None",
            "lineNumber": 477
          },
          {
            "text": "        ):",
            "lineNumber": 478
          },
          {
            "text": "            f.write(f\"\\nGrid Search:\\n\")",
            "lineNumber": 479
          },
          {
            "text": "            f.write(f\"  Mode: {args.grid_search}\\n\")",
            "lineNumber": 480
          },
          {
            "text": "            f.write(f\"  CV folds: {args.cv_folds}\\n\")",
            "lineNumber": 481
          },
          {
            "text": "            f.write(f\"  Number of combinations: {n_combinations}\\n\")",
            "lineNumber": 482
          }
        ]
      },
      "score": 0.27649986743927
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 638
          },
          "endPosition": {
            "line": 996
          }
        },
        "contents": "def evaluate(\n    model: nn.Module,\n    dataloader: DataLoader,\n    criterion: nn.Module,\n    device: str,\n) -&gt; Dict[str, float]:\n\n            ph_true = batch[\"ph\"].to(device)\n\n            # Forward pass\n            ph_pred = model(input_ids)\n\n            # Compute loss\n            loss = criterion(ph_pred, ph_true)\n\n            # Store predictions\n            all_preds.extend(ph_pred.cpu().numpy())\n            all_true.extend(ph_true.cpu().numpy())\n\n            total_loss += loss.item()\n            num_batches += 1\n\n    # Compute metrics\n    all_preds = np.array(all_preds)\n    all_true = np.array(all_true)\n\n    mse = mean_squared_error(all_true, all_preds)\n    mae = mean_absolute_error(all_true, all_preds)\n    r2 = r2_score(all_true, all_preds)\n\n    return {\n        \"loss\": total_loss / num_batches if num_batches &gt; 0 else 0.0,\n        \"mse\": mse,\n        \"rmse\": np.sqrt(mse),\n        \"mae\": mae,\n        \"r2\": r2,\n    }\n\n\ndef save_checkpoint(\n    model: nn.Module,\n    epoch: int,\n    metrics: Dict[str, float],\n    output_dir: Path,\n    is_best: bool = False,\n):\n    \"\"\"Save model checkpoint.\"\"\"\n    checkpoint = {\n        \"epoch\": epoch,\n        \"model_state_dict\": model.state_dict(),\n        \"metrics\": metrics,\n    }\n\n    # Save regular checkpoint\n    checkpoint_path = output_dir / f\"checkpoint_epoch_{epoch}.pt\"\n    torch.save(checkpoint, checkpoint_path)\n\n    # Save best model\n    if is_best:\n        best_path = output_dir / \"best_model.pt\"\n        torch.save(checkpoint, best_path)\n        print(f\"Saved best model to {best_path}\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Train HyenaDNA for pH prediction\")",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 530,
                "column": 1
              },
              "endPosition": {
                "line": 531,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def evaluate(",
            "lineNumber": 639,
            "isSignature": true
          },
          {
            "text": "    model: nn.Module,",
            "lineNumber": 640,
            "isSignature": true
          },
          {
            "text": "    dataloader: DataLoader,",
            "lineNumber": 641,
            "isSignature": true
          },
          {
            "text": "    criterion: nn.Module,",
            "lineNumber": 642,
            "isSignature": true
          },
          {
            "text": "    device: str,",
            "lineNumber": 643,
            "isSignature": true
          },
          {
            "text": ") -&gt; Dict[str, float]:",
            "lineNumber": 644,
            "isSignature": true
          },
          {
            "lineNumber": 654
          },
          {
            "text": "            ph_true = batch[\"ph\"].to(device)",
            "lineNumber": 655
          },
          {
            "lineNumber": 656
          },
          {
            "text": "            # Forward pass",
            "lineNumber": 657
          },
          {
            "text": "            ph_pred = model(input_ids)",
            "lineNumber": 658
          },
          {
            "lineNumber": 659
          },
          {
            "text": "            # Compute loss",
            "lineNumber": 660
          },
          {
            "text": "            loss = criterion(ph_pred, ph_true)",
            "lineNumber": 661
          },
          {
            "lineNumber": 662
          },
          {
            "text": "            # Store predictions",
            "lineNumber": 663
          },
          {
            "text": "            all_preds.extend(ph_pred.cpu().numpy())",
            "lineNumber": 664
          },
          {
            "text": "            all_true.extend(ph_true.cpu().numpy())",
            "lineNumber": 665
          },
          {
            "lineNumber": 666
          },
          {
            "text": "            total_loss += loss.item()",
            "lineNumber": 667
          },
          {
            "text": "            num_batches += 1",
            "lineNumber": 668
          },
          {
            "lineNumber": 669
          },
          {
            "text": "    # Compute metrics",
            "lineNumber": 670
          },
          {
            "text": "    all_preds = np.array(all_preds)",
            "lineNumber": 671
          },
          {
            "text": "    all_true = np.array(all_true)",
            "lineNumber": 672
          },
          {
            "lineNumber": 673
          },
          {
            "text": "    mse = mean_squared_error(all_true, all_preds)",
            "lineNumber": 674
          },
          {
            "text": "    mae = mean_absolute_error(all_true, all_preds)",
            "lineNumber": 675
          },
          {
            "text": "    r2 = r2_score(all_true, all_preds)",
            "lineNumber": 676
          },
          {
            "lineNumber": 677
          },
          {
            "text": "    return {",
            "lineNumber": 678
          },
          {
            "text": "        \"loss\": total_loss / num_batches if num_batches &gt; 0 else 0.0,",
            "lineNumber": 679
          },
          {
            "text": "        \"mse\": mse,",
            "lineNumber": 680
          },
          {
            "text": "        \"rmse\": np.sqrt(mse),",
            "lineNumber": 681
          },
          {
            "text": "        \"mae\": mae,",
            "lineNumber": 682
          },
          {
            "text": "        \"r2\": r2,",
            "lineNumber": 683
          },
          {
            "text": "    }",
            "lineNumber": 684,
            "isSignature": true
          },
          {
            "lineNumber": 685
          },
          {
            "lineNumber": 686
          },
          {
            "text": "def save_checkpoint(",
            "lineNumber": 687,
            "isSignature": true
          },
          {
            "text": "    model: nn.Module,",
            "lineNumber": 688,
            "isSignature": true
          },
          {
            "text": "    epoch: int,",
            "lineNumber": 689,
            "isSignature": true
          },
          {
            "text": "    metrics: Dict[str, float],",
            "lineNumber": 690,
            "isSignature": true
          },
          {
            "text": "    output_dir: Path,",
            "lineNumber": 691,
            "isSignature": true
          },
          {
            "text": "    is_best: bool = False,",
            "lineNumber": 692,
            "isSignature": true
          },
          {
            "text": "):",
            "lineNumber": 693,
            "isSignature": true
          },
          {
            "text": "    \"\"\"Save model checkpoint.\"\"\"",
            "lineNumber": 694
          },
          {
            "text": "    checkpoint = {",
            "lineNumber": 695
          },
          {
            "text": "        \"epoch\": epoch,",
            "lineNumber": 696
          },
          {
            "text": "        \"model_state_dict\": model.state_dict(),",
            "lineNumber": 697
          },
          {
            "text": "        \"metrics\": metrics,",
            "lineNumber": 698
          },
          {
            "text": "    }",
            "lineNumber": 699
          },
          {
            "lineNumber": 700
          },
          {
            "text": "    # Save regular checkpoint",
            "lineNumber": 701
          },
          {
            "text": "    checkpoint_path = output_dir / f\"checkpoint_epoch_{epoch}.pt\"",
            "lineNumber": 702
          },
          {
            "text": "    torch.save(checkpoint, checkpoint_path)",
            "lineNumber": 703
          },
          {
            "lineNumber": 704
          },
          {
            "text": "    # Save best model",
            "lineNumber": 705
          },
          {
            "text": "    if is_best:",
            "lineNumber": 706
          },
          {
            "text": "        best_path = output_dir / \"best_model.pt\"",
            "lineNumber": 707
          },
          {
            "text": "        torch.save(checkpoint, best_path)",
            "lineNumber": 708
          },
          {
            "text": "        print(f\"Saved best model to {best_path}\")",
            "lineNumber": 709,
            "isSignature": true
          },
          {
            "lineNumber": 710
          },
          {
            "lineNumber": 711
          },
          {
            "text": "def main():",
            "lineNumber": 712,
            "isSignature": true
          },
          {
            "text": "    parser = argparse.ArgumentParser(description=\"Train HyenaDNA for pH prediction\")",
            "lineNumber": 713
          }
        ]
      },
      "score": 0.2647508382797241
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./predict_ph.py",
        "range": {
          "startPosition": {
            "line": 291
          },
          "endPosition": {
            "line": 510
          }
        },
        "contents": "def main():\n\n    pipeline = create_pipeline(args.model)\n\n    # Perform grid search if requested\n    grid_search_time = None\n    best_params = None\n    best_cv_score = None\n    n_combinations = None\n\n    if args.grid_search != \"none\" and args.model != \"linear\":\n        param_grid = get_param_grid(args.model, args.grid_search)\n        if param_grid:\n            # Create ParameterGrid and calculate number of combinations\n            param_grid_list = list(ParameterGrid(param_grid))\n            n_combinations = len(param_grid_list)\n\n            print(\n                f\"\\nPerforming grid search ({args.grid_search} grid, {n_combinations} combinations)...\"\n            )\n            print(f\"Parameter grid: {param_grid}\")\n            print(f\"CV folds: {args.cv_folds}\\n\")\n\n            # Prepare progress bar iterator\n            if HAS_TQDM:\n                param_iter = tqdm(\n                    param_grid_list, desc=\"Grid Search Progress\", unit=\"combination\"\n                )\n            else:\n                param_iter = param_grid_list\n                print(f\"Evaluating {n_combinations} parameter combinations...\")\n\n            results = []\n            start_time = time.time()\n\n            # Manual grid search with progress bar\n            for i, params in enumerate(param_iter):\n                # Set parameters on the pipeline\n                pipeline.set_params(**params)\n\n                # Perform cross-validation\n                scores = cross_val_score(\n                    pipeline,\n                    X_train,\n                    y_train,\n                    cv=args.cv_folds,\n                    scoring=\"neg_mean_squared_error\",\n                    n_jobs=-1,\n                )\n\n                # Store mean score (negative MSE, so higher is better)\n                mean_score = scores.mean()\n                results.append((params, mean_score))\n\n                # Print progress if not using tqdm",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 263,
                "column": 1
              },
              "endPosition": {
                "line": 264,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def main():",
            "lineNumber": 292,
            "isSignature": true
          },
          {
            "lineNumber": 357
          },
          {
            "text": "    pipeline = create_pipeline(args.model)",
            "lineNumber": 358
          },
          {
            "lineNumber": 359
          },
          {
            "text": "    # Perform grid search if requested",
            "lineNumber": 360
          },
          {
            "text": "    grid_search_time = None",
            "lineNumber": 361
          },
          {
            "text": "    best_params = None",
            "lineNumber": 362
          },
          {
            "text": "    best_cv_score = None",
            "lineNumber": 363
          },
          {
            "text": "    n_combinations = None",
            "lineNumber": 364
          },
          {
            "lineNumber": 365
          },
          {
            "text": "    if args.grid_search != \"none\" and args.model != \"linear\":",
            "lineNumber": 366
          },
          {
            "text": "        param_grid = get_param_grid(args.model, args.grid_search)",
            "lineNumber": 367
          },
          {
            "text": "        if param_grid:",
            "lineNumber": 368
          },
          {
            "text": "            # Create ParameterGrid and calculate number of combinations",
            "lineNumber": 369
          },
          {
            "text": "            param_grid_list = list(ParameterGrid(param_grid))",
            "lineNumber": 370
          },
          {
            "text": "            n_combinations = len(param_grid_list)",
            "lineNumber": 371
          },
          {
            "lineNumber": 372
          },
          {
            "text": "            print(",
            "lineNumber": 373
          },
          {
            "text": "                f\"\\nPerforming grid search ({args.grid_search} grid, {n_combinations} combinations)...\"",
            "lineNumber": 374
          },
          {
            "text": "            )",
            "lineNumber": 375
          },
          {
            "text": "            print(f\"Parameter grid: {param_grid}\")",
            "lineNumber": 376
          },
          {
            "text": "            print(f\"CV folds: {args.cv_folds}\\n\")",
            "lineNumber": 377
          },
          {
            "lineNumber": 378
          },
          {
            "text": "            # Prepare progress bar iterator",
            "lineNumber": 379
          },
          {
            "text": "            if HAS_TQDM:",
            "lineNumber": 380
          },
          {
            "text": "                param_iter = tqdm(",
            "lineNumber": 381
          },
          {
            "text": "                    param_grid_list, desc=\"Grid Search Progress\", unit=\"combination\"",
            "lineNumber": 382
          },
          {
            "text": "                )",
            "lineNumber": 383
          },
          {
            "text": "            else:",
            "lineNumber": 384
          },
          {
            "text": "                param_iter = param_grid_list",
            "lineNumber": 385
          },
          {
            "text": "                print(f\"Evaluating {n_combinations} parameter combinations...\")",
            "lineNumber": 386
          },
          {
            "lineNumber": 387
          },
          {
            "text": "            results = []",
            "lineNumber": 388
          },
          {
            "text": "            start_time = time.time()",
            "lineNumber": 389
          },
          {
            "lineNumber": 390
          },
          {
            "text": "            # Manual grid search with progress bar",
            "lineNumber": 391
          },
          {
            "text": "            for i, params in enumerate(param_iter):",
            "lineNumber": 392
          },
          {
            "text": "                # Set parameters on the pipeline",
            "lineNumber": 393
          },
          {
            "text": "                pipeline.set_params(**params)",
            "lineNumber": 394
          },
          {
            "lineNumber": 395
          },
          {
            "text": "                # Perform cross-validation",
            "lineNumber": 396
          },
          {
            "text": "                scores = cross_val_score(",
            "lineNumber": 397
          },
          {
            "text": "                    pipeline,",
            "lineNumber": 398
          },
          {
            "text": "                    X_train,",
            "lineNumber": 399
          },
          {
            "text": "                    y_train,",
            "lineNumber": 400
          },
          {
            "text": "                    cv=args.cv_folds,",
            "lineNumber": 401
          },
          {
            "text": "                    scoring=\"neg_mean_squared_error\",",
            "lineNumber": 402
          },
          {
            "text": "                    n_jobs=-1,",
            "lineNumber": 403
          },
          {
            "text": "                )",
            "lineNumber": 404
          },
          {
            "lineNumber": 405
          },
          {
            "text": "                # Store mean score (negative MSE, so higher is better)",
            "lineNumber": 406
          },
          {
            "text": "                mean_score = scores.mean()",
            "lineNumber": 407
          },
          {
            "text": "                results.append((params, mean_score))",
            "lineNumber": 408
          },
          {
            "lineNumber": 409
          },
          {
            "text": "                # Print progress if not using tqdm",
            "lineNumber": 410
          }
        ]
      },
      "score": 0.26391899585723877
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./.cursor/plans/hyenadna_ph_regression_head_0c97fe04.plan.md",
        "range": {
          "startPosition": {
            "line": 164
          },
          "endPosition": {
            "line": 232
          }
        },
        "contents": "2. **HyenaDNAModel modifications**:\n   &#96;&#96;&#96;python\n   def __init__(\n       ...,\n       use_head=False,\n       task_type=\"classification\",      # NEW\n       n_classes: int = 2,              # For classification\n       head_architecture=\"linear\",     # NEW\n       ...\n   ):\n   &#96;&#96;&#96;\n\n\n   - Determine &#96;d_output&#96; based on &#96;task_type&#96;\n   - Pass &#96;task_type&#96; and &#96;head_architecture&#96; to &#96;SequenceDecoder&#96;\n\n3. **Output shape**:\n\n   - Classification: &#96;(batch, n_classes)&#96; \n   - Regression: &#96;(batch, 1)&#96; or &#96;(batch,)&#96; after squeeze\n\n### HyenaDNA Model Configuration\n\nBased on the [HyenaDNA repository](https://github.com/HazyResearch/hyena-dna), needed configurations:\n\n**Model Size Options** (with max_lengths):\n\n- &#96;hyenadna-tiny-1k-seqlen&#96;: max_length=1024 (default for testing)\n- &#96;hyenadna-small-32k-seqlen&#96;: max_length=32768\n- &#96;hyenadna-medium-160k-seqlen&#96;: max_length=160000\n- &#96;hyenadna-medium-450k-seqlen&#96;: max_length=450000\n- &#96;hyenadna-large-1m-seqlen&#96;: max_length=1_000_000\n\n**Tokenizer Configuration**:\n\n- Characters: &#96;[\"A\", \"C\", \"G\", \"T\", \"N\"]&#96; (N for uncertain nucleotides)\n- &#96;model_max_length = max_length + 2&#96; (account for special tokens)\n- &#96;padding_side = \"left\"&#96; (causal model)\n\n**Model Loading**:\n\n- Use &#96;HyenaDNAPreTrainedModel.from_pretrained()&#96; from &#96;hyena-dna/huggingface_wrapper.py&#96;\n- Set &#96;use_head=False&#96; to get embeddings only\n- Download weights if not present locally (configurable)\n\n### Default Hyperparameters\n\n**Model**:\n\n- Model size: &#96;hyenadna-tiny-1k-seqlen&#96; (default, configurable)\n- Head architecture: &#96;mlp2&#96; (2-layer MLP)\n- Pooling mode: &#96;pool&#96; (mean pooling)\n- Freeze backbone: &#96;False&#96; (fine-tune entire model)\n\n**Training**:\n\n- Batch size: &#96;4&#96; (small for long sequences, adjust based on GPU memory)\n- Learning rate: &#96;1e-4&#96; (typical for fine-tuning)\n- Num epochs: &#96;10&#96; (configurable)\n- Loss function: &#96;SmoothL1Loss&#96; (beta=1.0)\n- Optimizer: &#96;AdamW&#96; with weight decay &#96;0.01&#96;\n- Learning rate scheduler: Optional &#96;ReduceLROnPlateau&#96; (patience=3)\n\n**Data**:\n\n- Train/Val/Test split: &#96;0.8/0.1/0.1&#96;\n- Filter samples with missing pH values\n- Random seed: &#96;42&#96; (for reproducibility)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "2. **HyenaDNAModel modifications**:",
            "lineNumber": 165
          },
          {
            "text": "   &#96;&#96;&#96;python",
            "lineNumber": 166
          },
          {
            "text": "   def __init__(",
            "lineNumber": 167
          },
          {
            "text": "       ...,",
            "lineNumber": 168
          },
          {
            "text": "       use_head=False,",
            "lineNumber": 169
          },
          {
            "text": "       task_type=\"classification\",      # NEW",
            "lineNumber": 170
          },
          {
            "text": "       n_classes: int = 2,              # For classification",
            "lineNumber": 171
          },
          {
            "text": "       head_architecture=\"linear\",     # NEW",
            "lineNumber": 172
          },
          {
            "text": "       ...",
            "lineNumber": 173
          },
          {
            "text": "   ):",
            "lineNumber": 174
          },
          {
            "text": "   &#96;&#96;&#96;",
            "lineNumber": 175
          },
          {
            "lineNumber": 176
          },
          {
            "lineNumber": 177
          },
          {
            "text": "   - Determine &#96;d_output&#96; based on &#96;task_type&#96;",
            "lineNumber": 178
          },
          {
            "text": "   - Pass &#96;task_type&#96; and &#96;head_architecture&#96; to &#96;SequenceDecoder&#96;",
            "lineNumber": 179
          },
          {
            "lineNumber": 180
          },
          {
            "text": "3. **Output shape**:",
            "lineNumber": 181
          },
          {
            "lineNumber": 182
          },
          {
            "text": "   - Classification: &#96;(batch, n_classes)&#96; ",
            "lineNumber": 183
          },
          {
            "text": "   - Regression: &#96;(batch, 1)&#96; or &#96;(batch,)&#96; after squeeze",
            "lineNumber": 184
          },
          {
            "lineNumber": 185
          },
          {
            "text": "### HyenaDNA Model Configuration",
            "lineNumber": 186
          },
          {
            "lineNumber": 187
          },
          {
            "text": "Based on the [HyenaDNA repository](https://github.com/HazyResearch/hyena-dna), needed configurations:",
            "lineNumber": 188
          },
          {
            "lineNumber": 189
          },
          {
            "text": "**Model Size Options** (with max_lengths):",
            "lineNumber": 190
          },
          {
            "lineNumber": 191
          },
          {
            "text": "- &#96;hyenadna-tiny-1k-seqlen&#96;: max_length=1024 (default for testing)",
            "lineNumber": 192
          },
          {
            "text": "- &#96;hyenadna-small-32k-seqlen&#96;: max_length=32768",
            "lineNumber": 193
          },
          {
            "text": "- &#96;hyenadna-medium-160k-seqlen&#96;: max_length=160000",
            "lineNumber": 194
          },
          {
            "text": "- &#96;hyenadna-medium-450k-seqlen&#96;: max_length=450000",
            "lineNumber": 195
          },
          {
            "text": "- &#96;hyenadna-large-1m-seqlen&#96;: max_length=1_000_000",
            "lineNumber": 196
          },
          {
            "lineNumber": 197
          },
          {
            "text": "**Tokenizer Configuration**:",
            "lineNumber": 198
          },
          {
            "lineNumber": 199
          },
          {
            "text": "- Characters: &#96;[\"A\", \"C\", \"G\", \"T\", \"N\"]&#96; (N for uncertain nucleotides)",
            "lineNumber": 200
          },
          {
            "text": "- &#96;model_max_length = max_length + 2&#96; (account for special tokens)",
            "lineNumber": 201
          },
          {
            "text": "- &#96;padding_side = \"left\"&#96; (causal model)",
            "lineNumber": 202
          },
          {
            "lineNumber": 203
          },
          {
            "text": "**Model Loading**:",
            "lineNumber": 204
          },
          {
            "lineNumber": 205
          },
          {
            "text": "- Use &#96;HyenaDNAPreTrainedModel.from_pretrained()&#96; from &#96;hyena-dna/huggingface_wrapper.py&#96;",
            "lineNumber": 206
          },
          {
            "text": "- Set &#96;use_head=False&#96; to get embeddings only",
            "lineNumber": 207
          },
          {
            "text": "- Download weights if not present locally (configurable)",
            "lineNumber": 208
          },
          {
            "lineNumber": 209
          },
          {
            "text": "### Default Hyperparameters",
            "lineNumber": 210
          },
          {
            "lineNumber": 211
          },
          {
            "text": "**Model**:",
            "lineNumber": 212
          },
          {
            "lineNumber": 213
          },
          {
            "text": "- Model size: &#96;hyenadna-tiny-1k-seqlen&#96; (default, configurable)",
            "lineNumber": 214
          },
          {
            "text": "- Head architecture: &#96;mlp2&#96; (2-layer MLP)",
            "lineNumber": 215
          },
          {
            "text": "- Pooling mode: &#96;pool&#96; (mean pooling)",
            "lineNumber": 216
          },
          {
            "text": "- Freeze backbone: &#96;False&#96; (fine-tune entire model)",
            "lineNumber": 217
          },
          {
            "lineNumber": 218
          },
          {
            "text": "**Training**:",
            "lineNumber": 219
          },
          {
            "lineNumber": 220
          },
          {
            "text": "- Batch size: &#96;4&#96; (small for long sequences, adjust based on GPU memory)",
            "lineNumber": 221
          },
          {
            "text": "- Learning rate: &#96;1e-4&#96; (typical for fine-tuning)",
            "lineNumber": 222
          },
          {
            "text": "- Num epochs: &#96;10&#96; (configurable)",
            "lineNumber": 223
          },
          {
            "text": "- Loss function: &#96;SmoothL1Loss&#96; (beta=1.0)",
            "lineNumber": 224
          },
          {
            "text": "- Optimizer: &#96;AdamW&#96; with weight decay &#96;0.01&#96;",
            "lineNumber": 225
          },
          {
            "text": "- Learning rate scheduler: Optional &#96;ReduceLROnPlateau&#96; (patience=3)",
            "lineNumber": 226
          },
          {
            "lineNumber": 227
          },
          {
            "text": "**Data**:",
            "lineNumber": 228
          },
          {
            "lineNumber": 229
          },
          {
            "text": "- Train/Val/Test split: &#96;0.8/0.1/0.1&#96;",
            "lineNumber": 230
          },
          {
            "text": "- Filter samples with missing pH values",
            "lineNumber": 231
          },
          {
            "text": "- Random seed: &#96;42&#96; (for reproducibility)",
            "lineNumber": 232
          }
        ]
      },
      "score": 0.24315255880355835
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./predict_ph.py",
        "range": {
          "startPosition": {
            "line": 291
          },
          "endPosition": {
            "line": 510
          }
        },
        "contents": "def main():\n,\n    )\n    parser.add_argument(\n        \"--output-dir\",\n        type=str,\n        default=\"results\",\n        help=\"Output directory for results (default: results)\",\n    )\n    parser.add_argument(\n        \"--grid-search\",\n        type=str,\n        choices=[\"test\", \"full\", \"none\"],\n        default=\"test\",\n        help='Grid search mode: \"test\" for small grids (2-4 combinations), '\n        '\"full\" for comprehensive grids, or \"none\" to skip (default: test)',\n    )\n    parser.add_argument(\n        \"--cv-folds\",\n        type=int,\n        default=5,\n        help=\"Number of CV folds for grid search (default: 5)\",\n    )\n\n    args = parser.parse_args()\n\n    # Create output directory if it doesn't exist\n    output_dir = Path(args.output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Load and preprocess data\n    X_train, X_test, y_train, y_test, feature_names, test_metadata = (\n        load_and_preprocess_data(\n            args.dataset_repo, args.phylum_counts, args.random_seed\n        )\n    )\n\n    # Create pipeline\n    print(f\"\\nCreating {args.model} pipeline...\")\n    pipeline = create_pipeline(args.model)\n\n    # Perform grid search if requested\n    grid_search_time = None",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 263,
                "column": 1
              },
              "endPosition": {
                "line": 264,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def main():",
            "lineNumber": 292,
            "isSignature": true
          },
          {
            "text": ",",
            "lineNumber": 320
          },
          {
            "text": "    )",
            "lineNumber": 321
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 322
          },
          {
            "text": "        \"--output-dir\",",
            "lineNumber": 323
          },
          {
            "text": "        type=str,",
            "lineNumber": 324
          },
          {
            "text": "        default=\"results\",",
            "lineNumber": 325
          },
          {
            "text": "        help=\"Output directory for results (default: results)\",",
            "lineNumber": 326
          },
          {
            "text": "    )",
            "lineNumber": 327
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 328
          },
          {
            "text": "        \"--grid-search\",",
            "lineNumber": 329
          },
          {
            "text": "        type=str,",
            "lineNumber": 330
          },
          {
            "text": "        choices=[\"test\", \"full\", \"none\"],",
            "lineNumber": 331
          },
          {
            "text": "        default=\"test\",",
            "lineNumber": 332
          },
          {
            "text": "        help='Grid search mode: \"test\" for small grids (2-4 combinations), '",
            "lineNumber": 333
          },
          {
            "text": "        '\"full\" for comprehensive grids, or \"none\" to skip (default: test)',",
            "lineNumber": 334
          },
          {
            "text": "    )",
            "lineNumber": 335
          },
          {
            "text": "    parser.add_argument(",
            "lineNumber": 336
          },
          {
            "text": "        \"--cv-folds\",",
            "lineNumber": 337
          },
          {
            "text": "        type=int,",
            "lineNumber": 338
          },
          {
            "text": "        default=5,",
            "lineNumber": 339
          },
          {
            "text": "        help=\"Number of CV folds for grid search (default: 5)\",",
            "lineNumber": 340
          },
          {
            "text": "    )",
            "lineNumber": 341
          },
          {
            "lineNumber": 342
          },
          {
            "text": "    args = parser.parse_args()",
            "lineNumber": 343
          },
          {
            "lineNumber": 344
          },
          {
            "text": "    # Create output directory if it doesn't exist",
            "lineNumber": 345
          },
          {
            "text": "    output_dir = Path(args.output_dir)",
            "lineNumber": 346
          },
          {
            "text": "    output_dir.mkdir(parents=True, exist_ok=True)",
            "lineNumber": 347
          },
          {
            "lineNumber": 348
          },
          {
            "text": "    # Load and preprocess data",
            "lineNumber": 349
          },
          {
            "text": "    X_train, X_test, y_train, y_test, feature_names, test_metadata = (",
            "lineNumber": 350
          },
          {
            "text": "        load_and_preprocess_data(",
            "lineNumber": 351
          },
          {
            "text": "            args.dataset_repo, args.phylum_counts, args.random_seed",
            "lineNumber": 352
          },
          {
            "text": "        )",
            "lineNumber": 353
          },
          {
            "text": "    )",
            "lineNumber": 354
          },
          {
            "lineNumber": 355
          },
          {
            "text": "    # Create pipeline",
            "lineNumber": 356
          },
          {
            "text": "    print(f\"\\nCreating {args.model} pipeline...\")",
            "lineNumber": 357
          },
          {
            "text": "    pipeline = create_pipeline(args.model)",
            "lineNumber": 358
          },
          {
            "lineNumber": 359
          },
          {
            "text": "    # Perform grid search if requested",
            "lineNumber": 360
          },
          {
            "text": "    grid_search_time = None",
            "lineNumber": 361
          }
        ]
      },
      "score": 0.23736579716205597
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 272
          },
          "endPosition": {
            "line": 409
          }
        },
        "contents": "def split_sequences_into_sets(\n    sequences: List[str],\n    max_length: int,\n    num_sets: int = 5,\n) -&gt; List[List[str]]:\n\n\n            if cumulative_length + tokens_needed &lt;= max_length:\n                current_set.append(seq)\n                cumulative_length += tokens_needed\n                sequence_idx += 1\n            else:\n                # This sequence would exceed max_length for current set\n                # Move to next set\n                break\n\n        # Move to next set (either we filled this one or can't add more)\n        current_set_idx += 1\n\n        # If all sequences processed, we're done\n        if sequence_idx &gt;= len(valid_sequences):\n            break\n\n    return sets\n\n\ndef concatenate_sequences(\n    sequences: List[str],\n    tokenizer: CharacterTokenizer,\n    max_length: int,\n    sep_token_id: int = SEP_TOKEN_ID,\n) -&gt; List[int]:\n    \"\"\"\n    Concatenate multiple DNA sequences with [SEP] tokens.\n\n    Args:\n        sequences: List of DNA sequence strings\n        tokenizer: CharacterTokenizer instance\n        max_length: Maximum total length (including special tokens)\n        sep_token_id: [SEP] token ID\n\n    Returns:\n        List of token IDs of length &lt;= max_length\n    \"\"\"\n    if not sequences:\n        # Return single [SEP] token if no sequences\n        return [sep_token_id]\n\n    # Filter out empty sequences first\n    valid_sequences = [seq for seq in sequences if seq and len(seq) &gt; 0]\n\n    if not valid_sequences:\n        # No valid sequences, return single [SEP]\n        return [sep_token_id]\n\n    # Precompute how many sequences we need based on character lengths\n    # For character tokenizer, each DNA character maps to one token (after filtering special tokens)\n    # We need to account for: sequence lengths + (n-1) SEP tokens for n sequences\n    cumulative_length = 0\n    sequences_to_use = []\n\n    for seq in valid_sequences:\n        seq_len = len(seq)\n        # Estimate tokens needed: seq_len characters + 1 SEP token (if not first sequence)\n        tokens_needed = seq_len + (1 if sequences_to_use else 0)\n\n        if cumulative_length + tokens_needed &lt;= max_length:",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def split_sequences_into_sets(",
            "lineNumber": 273,
            "isSignature": true
          },
          {
            "text": "    sequences: List[str],",
            "lineNumber": 274,
            "isSignature": true
          },
          {
            "text": "    max_length: int,",
            "lineNumber": 275,
            "isSignature": true
          },
          {
            "text": "    num_sets: int = 5,",
            "lineNumber": 276,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[List[str]]:",
            "lineNumber": 277,
            "isSignature": true
          },
          {
            "lineNumber": 313
          },
          {
            "lineNumber": 314
          },
          {
            "text": "            if cumulative_length + tokens_needed &lt;= max_length:",
            "lineNumber": 315
          },
          {
            "text": "                current_set.append(seq)",
            "lineNumber": 316
          },
          {
            "text": "                cumulative_length += tokens_needed",
            "lineNumber": 317
          },
          {
            "text": "                sequence_idx += 1",
            "lineNumber": 318
          },
          {
            "text": "            else:",
            "lineNumber": 319
          },
          {
            "text": "                # This sequence would exceed max_length for current set",
            "lineNumber": 320
          },
          {
            "text": "                # Move to next set",
            "lineNumber": 321
          },
          {
            "text": "                break",
            "lineNumber": 322
          },
          {
            "lineNumber": 323
          },
          {
            "text": "        # Move to next set (either we filled this one or can't add more)",
            "lineNumber": 324
          },
          {
            "text": "        current_set_idx += 1",
            "lineNumber": 325
          },
          {
            "lineNumber": 326
          },
          {
            "text": "        # If all sequences processed, we're done",
            "lineNumber": 327
          },
          {
            "text": "        if sequence_idx &gt;= len(valid_sequences):",
            "lineNumber": 328
          },
          {
            "text": "            break",
            "lineNumber": 329
          },
          {
            "lineNumber": 330
          },
          {
            "text": "    return sets",
            "lineNumber": 331,
            "isSignature": true
          },
          {
            "lineNumber": 332
          },
          {
            "lineNumber": 333
          },
          {
            "text": "def concatenate_sequences(",
            "lineNumber": 334,
            "isSignature": true
          },
          {
            "text": "    sequences: List[str],",
            "lineNumber": 335,
            "isSignature": true
          },
          {
            "text": "    tokenizer: CharacterTokenizer,",
            "lineNumber": 336,
            "isSignature": true
          },
          {
            "text": "    max_length: int,",
            "lineNumber": 337,
            "isSignature": true
          },
          {
            "text": "    sep_token_id: int = SEP_TOKEN_ID,",
            "lineNumber": 338,
            "isSignature": true
          },
          {
            "text": ") -&gt; List[int]:",
            "lineNumber": 339,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 340
          },
          {
            "text": "    Concatenate multiple DNA sequences with [SEP] tokens.",
            "lineNumber": 341
          },
          {
            "lineNumber": 342
          },
          {
            "text": "    Args:",
            "lineNumber": 343
          },
          {
            "text": "        sequences: List of DNA sequence strings",
            "lineNumber": 344
          },
          {
            "text": "        tokenizer: CharacterTokenizer instance",
            "lineNumber": 345
          },
          {
            "text": "        max_length: Maximum total length (including special tokens)",
            "lineNumber": 346
          },
          {
            "text": "        sep_token_id: [SEP] token ID",
            "lineNumber": 347
          },
          {
            "lineNumber": 348
          },
          {
            "text": "    Returns:",
            "lineNumber": 349
          },
          {
            "text": "        List of token IDs of length &lt;= max_length",
            "lineNumber": 350
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 351
          },
          {
            "text": "    if not sequences:",
            "lineNumber": 352
          },
          {
            "text": "        # Return single [SEP] token if no sequences",
            "lineNumber": 353
          },
          {
            "text": "        return [sep_token_id]",
            "lineNumber": 354
          },
          {
            "lineNumber": 355
          },
          {
            "text": "    # Filter out empty sequences first",
            "lineNumber": 356
          },
          {
            "text": "    valid_sequences = [seq for seq in sequences if seq and len(seq) &gt; 0]",
            "lineNumber": 357
          },
          {
            "lineNumber": 358
          },
          {
            "text": "    if not valid_sequences:",
            "lineNumber": 359
          },
          {
            "text": "        # No valid sequences, return single [SEP]",
            "lineNumber": 360
          },
          {
            "text": "        return [sep_token_id]",
            "lineNumber": 361
          },
          {
            "lineNumber": 362
          },
          {
            "text": "    # Precompute how many sequences we need based on character lengths",
            "lineNumber": 363
          },
          {
            "text": "    # For character tokenizer, each DNA character maps to one token (after filtering special tokens)",
            "lineNumber": 364
          },
          {
            "text": "    # We need to account for: sequence lengths + (n-1) SEP tokens for n sequences",
            "lineNumber": 365
          },
          {
            "text": "    cumulative_length = 0",
            "lineNumber": 366
          },
          {
            "text": "    sequences_to_use = []",
            "lineNumber": 367
          },
          {
            "lineNumber": 368
          },
          {
            "text": "    for seq in valid_sequences:",
            "lineNumber": 369
          },
          {
            "text": "        seq_len = len(seq)",
            "lineNumber": 370
          },
          {
            "text": "        # Estimate tokens needed: seq_len characters + 1 SEP token (if not first sequence)",
            "lineNumber": 371
          },
          {
            "text": "        tokens_needed = seq_len + (1 if sequences_to_use else 0)",
            "lineNumber": 372
          },
          {
            "lineNumber": 373
          },
          {
            "text": "        if cumulative_length + tokens_needed &lt;= max_length:",
            "lineNumber": 374
          }
        ]
      },
      "score": 0.23152196407318115
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 711
          },
          "endPosition": {
            "line": 996
          }
        },
        "contents": "def main():\n,\n        help=\"Download model from HuggingFace if not present\",\n    )\n\n    args = parser.parse_args()\n\n    # Set random seeds\n    torch.manual_seed(args.random_seed)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def main():",
            "lineNumber": 712,
            "isSignature": true
          },
          {
            "text": ",",
            "lineNumber": 789
          },
          {
            "text": "        help=\"Download model from HuggingFace if not present\",",
            "lineNumber": 790
          },
          {
            "text": "    )",
            "lineNumber": 791
          },
          {
            "lineNumber": 792
          },
          {
            "text": "    args = parser.parse_args()",
            "lineNumber": 793
          },
          {
            "lineNumber": 794
          },
          {
            "text": "    # Set random seeds",
            "lineNumber": 795
          },
          {
            "text": "    torch.manual_seed(args.random_seed)",
            "lineNumber": 796
          }
        ]
      },
      "score": 0.22099286317825317
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./predict_ph.py",
        "range": {
          "startPosition": {
            "line": 291
          },
          "endPosition": {
            "line": 510
          }
        },
        "contents": "def main():\n\n                results.append((params, mean_score))\n\n                # Print progress if not using tqdm\n                if not HAS_TQDM and (i + 1) % max(1, n_combinations // 10) == 0:\n                    print(f\"  Completed {i + 1}/{n_combinations} combinations...\")\n\n            grid_search_time = time.time() - start_time\n\n            # Find best parameters (highest score = lowest MSE)\n            best_params, best_cv_score = max(results, key=lambda x: x[1])\n            best_cv_score = -best_cv_score  # Convert back to positive MSE\n\n            # Set pipeline to best parameters and fit on full training set\n            pipeline.set_params(**best_params)\n            pipeline.fit(X_train, y_train)\n\n            print(f\"\\nGrid search completed in {grid_search_time:.2f} seconds\")\n            print(f\"Best parameters: {best_params}\")\n            print(f\"Best CV score (RMSE): {np.sqrt(best_cv_score):.4f}\")\n        else:\n            print(\"No parameter grid available for this model.\")\n    else:\n        if args.model == \"linear\":\n            print(\"Linear regression doesn't require hyperparameter tuning.\")\n        print(f\"Fitting {args.model} model with default parameters...\")\n        pipeline.fit(X_train, y_train)\n\n    # Make predictions\n    print(\"Making predictions...\")\n    y_train_pred = pipeline.predict(X_train)",
        "signatures": {
          "ranges": [
            {
              "startPosition": {
                "line": 263,
                "column": 1
              },
              "endPosition": {
                "line": 264,
                "column": 5
              }
            }
          ]
        },
        "detailedLines": [
          {
            "text": "def main():",
            "lineNumber": 292,
            "isSignature": true
          },
          {
            "lineNumber": 407
          },
          {
            "text": "                results.append((params, mean_score))",
            "lineNumber": 408
          },
          {
            "lineNumber": 409
          },
          {
            "text": "                # Print progress if not using tqdm",
            "lineNumber": 410
          },
          {
            "text": "                if not HAS_TQDM and (i + 1) % max(1, n_combinations // 10) == 0:",
            "lineNumber": 411
          },
          {
            "text": "                    print(f\"  Completed {i + 1}/{n_combinations} combinations...\")",
            "lineNumber": 412
          },
          {
            "lineNumber": 413
          },
          {
            "text": "            grid_search_time = time.time() - start_time",
            "lineNumber": 414
          },
          {
            "lineNumber": 415
          },
          {
            "text": "            # Find best parameters (highest score = lowest MSE)",
            "lineNumber": 416
          },
          {
            "text": "            best_params, best_cv_score = max(results, key=lambda x: x[1])",
            "lineNumber": 417
          },
          {
            "text": "            best_cv_score = -best_cv_score  # Convert back to positive MSE",
            "lineNumber": 418
          },
          {
            "lineNumber": 419
          },
          {
            "text": "            # Set pipeline to best parameters and fit on full training set",
            "lineNumber": 420
          },
          {
            "text": "            pipeline.set_params(**best_params)",
            "lineNumber": 421
          },
          {
            "text": "            pipeline.fit(X_train, y_train)",
            "lineNumber": 422
          },
          {
            "lineNumber": 423
          },
          {
            "text": "            print(f\"\\nGrid search completed in {grid_search_time:.2f} seconds\")",
            "lineNumber": 424
          },
          {
            "text": "            print(f\"Best parameters: {best_params}\")",
            "lineNumber": 425
          },
          {
            "text": "            print(f\"Best CV score (RMSE): {np.sqrt(best_cv_score):.4f}\")",
            "lineNumber": 426
          },
          {
            "text": "        else:",
            "lineNumber": 427
          },
          {
            "text": "            print(\"No parameter grid available for this model.\")",
            "lineNumber": 428
          },
          {
            "text": "    else:",
            "lineNumber": 429
          },
          {
            "text": "        if args.model == \"linear\":",
            "lineNumber": 430
          },
          {
            "text": "            print(\"Linear regression doesn't require hyperparameter tuning.\")",
            "lineNumber": 431
          },
          {
            "text": "        print(f\"Fitting {args.model} model with default parameters...\")",
            "lineNumber": 432
          },
          {
            "text": "        pipeline.fit(X_train, y_train)",
            "lineNumber": 433
          },
          {
            "lineNumber": 434
          },
          {
            "text": "    # Make predictions",
            "lineNumber": 435
          },
          {
            "text": "    print(\"Making predictions...\")",
            "lineNumber": 436
          },
          {
            "text": "    y_train_pred = pipeline.predict(X_train)",
            "lineNumber": 437
          }
        ]
      },
      "score": 0.21700559556484222
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./train_hyenadna_ph.py",
        "range": {
          "startPosition": {
            "line": 495,
            "column": 15
          },
          "endPosition": {
            "line": 574
          }
        },
        "contents": "def setup_model(\n    model_name: str = \"hyenadna-tiny-1k-seqlen\",\n    checkpoint_dir: str = \"./checkpoints\",\n    head_architecture: str = \"mlp2\",\n    pooling_mode: str = \"pool\",\n    freeze_backbone: bool = False,\n    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    download: bool = False,\n) -&gt; Tuple[nn.Module, CharacterTokenizer, int]:\n    \"\"\"\n    Load pretrained HyenaDNA model and attach regression head.\n\n    Args:\n        model_name: Model name from MODEL_CONFIGS\n        checkpoint_dir: Directory for model checkpoints\n        head_architecture: Regression head architecture\n        pooling_mode: Pooling mode for regression head\n        freeze_backbone: Whether to freeze backbone weights\n        device: Device to use\n        download: Whether to download model if not present\n\n    Returns:\n        (model, tokenizer, max_length)\n    \"\"\"\n    if model_name not in MODEL_CONFIGS:\n        raise ValueError(\n            f\"Unknown model: {model_name}. \"\n            f\"Choose from: {list(MODEL_CONFIGS.keys())}\"\n        )\n\n    max_length = MODEL_CONFIGS[model_name][\"max_length\"]",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def setup_model(",
            "lineNumber": 499,
            "isSignature": true
          },
          {
            "text": "    model_name: str = \"hyenadna-tiny-1k-seqlen\",",
            "lineNumber": 500,
            "isSignature": true
          },
          {
            "text": "    checkpoint_dir: str = \"./checkpoints\",",
            "lineNumber": 501,
            "isSignature": true
          },
          {
            "text": "    head_architecture: str = \"mlp2\",",
            "lineNumber": 502,
            "isSignature": true
          },
          {
            "text": "    pooling_mode: str = \"pool\",",
            "lineNumber": 503,
            "isSignature": true
          },
          {
            "text": "    freeze_backbone: bool = False,",
            "lineNumber": 504,
            "isSignature": true
          },
          {
            "text": "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",",
            "lineNumber": 505,
            "isSignature": true
          },
          {
            "text": "    download: bool = False,",
            "lineNumber": 506,
            "isSignature": true
          },
          {
            "text": ") -&gt; Tuple[nn.Module, CharacterTokenizer, int]:",
            "lineNumber": 507,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 508
          },
          {
            "text": "    Load pretrained HyenaDNA model and attach regression head.",
            "lineNumber": 509
          },
          {
            "lineNumber": 510
          },
          {
            "text": "    Args:",
            "lineNumber": 511
          },
          {
            "text": "        model_name: Model name from MODEL_CONFIGS",
            "lineNumber": 512
          },
          {
            "text": "        checkpoint_dir: Directory for model checkpoints",
            "lineNumber": 513
          },
          {
            "text": "        head_architecture: Regression head architecture",
            "lineNumber": 514
          },
          {
            "text": "        pooling_mode: Pooling mode for regression head",
            "lineNumber": 515
          },
          {
            "text": "        freeze_backbone: Whether to freeze backbone weights",
            "lineNumber": 516
          },
          {
            "text": "        device: Device to use",
            "lineNumber": 517
          },
          {
            "text": "        download: Whether to download model if not present",
            "lineNumber": 518
          },
          {
            "lineNumber": 519
          },
          {
            "text": "    Returns:",
            "lineNumber": 520
          },
          {
            "text": "        (model, tokenizer, max_length)",
            "lineNumber": 521
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 522
          },
          {
            "text": "    if model_name not in MODEL_CONFIGS:",
            "lineNumber": 523
          },
          {
            "text": "        raise ValueError(",
            "lineNumber": 524
          },
          {
            "text": "            f\"Unknown model: {model_name}. \"",
            "lineNumber": 525
          },
          {
            "text": "            f\"Choose from: {list(MODEL_CONFIGS.keys())}\"",
            "lineNumber": 526
          },
          {
            "text": "        )",
            "lineNumber": 527
          },
          {
            "lineNumber": 528
          },
          {
            "text": "    max_length = MODEL_CONFIGS[model_name][\"max_length\"]",
            "lineNumber": 529
          }
        ]
      },
      "score": 0.21494759619235992
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./predict_ph.py",
        "range": {
          "startPosition": {
            "line": 152
          },
          "endPosition": {
            "line": 259
          }
        },
        "contents": "def create_pipeline(model_type):\n\n    Create a scikit-learn pipeline with L1 normalization and regression model.\n\n    Parameters\n    ----------\n    model_type : str\n        Type of model: 'linear', 'knn', 'rf', or 'hgb'\n\n    Returns\n    -------\n    Pipeline\n        scikit-learn Pipeline object ready for fitting and GridSearchCV\n    \"\"\"\n    # L1 normalization (normalize to sum to 1 for each sample)\n    normalizer = Normalizer(norm=\"l1\")\n\n    # Select model based on model_type\n    if model_type == \"linear\":\n        model = LinearRegression()\n    elif model_type == \"knn\":\n        # Default parameters, ready for GridSearchCV tuning\n        model = KNeighborsRegressor()\n    elif model_type == \"rf\":\n        # Default parameters, ready for GridSearchCV tuning\n        model = RandomForestRegressor(random_state=42, n_jobs=-1)\n    elif model_type == \"hgb\":\n        # Default parameters, ready for GridSearchCV tuning\n        model = HistGradientBoostingRegressor(random_state=42)\n    else:\n        raise ValueError(\n            f\"Unknown model type: {model_type}. Choose from: linear, knn, rf, hgb\"\n        )\n\n    # Create pipeline: normalization -&gt; model\n    pipeline = Pipeline([(\"normalizer\", normalizer), (\"regressor\", model)])\n\n    return pipeline\n\n\ndef get_param_grid(model_type, grid_type=\"test\"):\n    \"\"\"\n    Get parameter grid for GridSearchCV.\n\n    Parameters\n    ----------\n    model_type : str\n        Type of model: 'linear', 'knn', 'rf', or 'hgb'\n    grid_type : str\n        'test' for small grids (2-4 combinations) or 'full' for comprehensive grids\n\n    Returns\n    -------\n    dict or None\n        Parameter grid dictionary, or None if model doesn't need tuning\n    \"\"\"\n    if model_type == \"linear\":\n        # Linear regression doesn't need hyperparameter tuning\n        return None\n\n    elif model_type == \"knn\":\n        if grid_type == \"test\":\n            # Small test grid: 2x2 = 4 combinations\n            return",
        "signatures": {},
        "detailedLines": [
          {
            "text": "def create_pipeline(model_type):",
            "lineNumber": 153,
            "isSignature": true
          },
          {
            "lineNumber": 154
          },
          {
            "text": "    Create a scikit-learn pipeline with L1 normalization and regression model.",
            "lineNumber": 155
          },
          {
            "lineNumber": 156
          },
          {
            "text": "    Parameters",
            "lineNumber": 157
          },
          {
            "text": "    ----------",
            "lineNumber": 158
          },
          {
            "text": "    model_type : str",
            "lineNumber": 159
          },
          {
            "text": "        Type of model: 'linear', 'knn', 'rf', or 'hgb'",
            "lineNumber": 160
          },
          {
            "lineNumber": 161
          },
          {
            "text": "    Returns",
            "lineNumber": 162
          },
          {
            "text": "    -------",
            "lineNumber": 163
          },
          {
            "text": "    Pipeline",
            "lineNumber": 164
          },
          {
            "text": "        scikit-learn Pipeline object ready for fitting and GridSearchCV",
            "lineNumber": 165
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 166
          },
          {
            "text": "    # L1 normalization (normalize to sum to 1 for each sample)",
            "lineNumber": 167
          },
          {
            "text": "    normalizer = Normalizer(norm=\"l1\")",
            "lineNumber": 168
          },
          {
            "lineNumber": 169
          },
          {
            "text": "    # Select model based on model_type",
            "lineNumber": 170
          },
          {
            "text": "    if model_type == \"linear\":",
            "lineNumber": 171
          },
          {
            "text": "        model = LinearRegression()",
            "lineNumber": 172
          },
          {
            "text": "    elif model_type == \"knn\":",
            "lineNumber": 173
          },
          {
            "text": "        # Default parameters, ready for GridSearchCV tuning",
            "lineNumber": 174
          },
          {
            "text": "        model = KNeighborsRegressor()",
            "lineNumber": 175
          },
          {
            "text": "    elif model_type == \"rf\":",
            "lineNumber": 176
          },
          {
            "text": "        # Default parameters, ready for GridSearchCV tuning",
            "lineNumber": 177
          },
          {
            "text": "        model = RandomForestRegressor(random_state=42, n_jobs=-1)",
            "lineNumber": 178
          },
          {
            "text": "    elif model_type == \"hgb\":",
            "lineNumber": 179
          },
          {
            "text": "        # Default parameters, ready for GridSearchCV tuning",
            "lineNumber": 180
          },
          {
            "text": "        model = HistGradientBoostingRegressor(random_state=42)",
            "lineNumber": 181
          },
          {
            "text": "    else:",
            "lineNumber": 182
          },
          {
            "text": "        raise ValueError(",
            "lineNumber": 183
          },
          {
            "text": "            f\"Unknown model type: {model_type}. Choose from: linear, knn, rf, hgb\"",
            "lineNumber": 184
          },
          {
            "text": "        )",
            "lineNumber": 185
          },
          {
            "lineNumber": 186
          },
          {
            "text": "    # Create pipeline: normalization -&gt; model",
            "lineNumber": 187
          },
          {
            "text": "    pipeline = Pipeline([(\"normalizer\", normalizer), (\"regressor\", model)])",
            "lineNumber": 188
          },
          {
            "lineNumber": 189
          },
          {
            "text": "    return pipeline",
            "lineNumber": 190,
            "isSignature": true
          },
          {
            "lineNumber": 191
          },
          {
            "lineNumber": 192
          },
          {
            "text": "def get_param_grid(model_type, grid_type=\"test\"):",
            "lineNumber": 193,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 194
          },
          {
            "text": "    Get parameter grid for GridSearchCV.",
            "lineNumber": 195
          },
          {
            "lineNumber": 196
          },
          {
            "text": "    Parameters",
            "lineNumber": 197
          },
          {
            "text": "    ----------",
            "lineNumber": 198
          },
          {
            "text": "    model_type : str",
            "lineNumber": 199
          },
          {
            "text": "        Type of model: 'linear', 'knn', 'rf', or 'hgb'",
            "lineNumber": 200
          },
          {
            "text": "    grid_type : str",
            "lineNumber": 201
          },
          {
            "text": "        'test' for small grids (2-4 combinations) or 'full' for comprehensive grids",
            "lineNumber": 202
          },
          {
            "lineNumber": 203
          },
          {
            "text": "    Returns",
            "lineNumber": 204
          },
          {
            "text": "    -------",
            "lineNumber": 205
          },
          {
            "text": "    dict or None",
            "lineNumber": 206
          },
          {
            "text": "        Parameter grid dictionary, or None if model doesn't need tuning",
            "lineNumber": 207
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 208
          },
          {
            "text": "    if model_type == \"linear\":",
            "lineNumber": 209
          },
          {
            "text": "        # Linear regression doesn't need hyperparameter tuning",
            "lineNumber": 210
          },
          {
            "text": "        return None",
            "lineNumber": 211
          },
          {
            "lineNumber": 212
          },
          {
            "text": "    elif model_type == \"knn\":",
            "lineNumber": 213
          },
          {
            "text": "        if grid_type == \"test\":",
            "lineNumber": 214
          },
          {
            "text": "            # Small test grid: 2x2 = 4 combinations",
            "lineNumber": 215
          },
          {
            "text": "            return",
            "lineNumber": 216
          }
        ]
      },
      "score": 0.20917165279388428
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./process_rdp_data.py",
        "range": {
          "startPosition": {
            "line": 331,
            "column": 22
          },
          "endPosition": {
            "line": 373,
            "column": 26
          }
        },
        "contents": "if __name__ == \"__main__\":\n    # Example usage\n    import sys\n\n    if len(sys.argv) &lt; 2:\n        print(\"Usage:\")\n        print(\"  Single file: python process_rdp_data.py &lt;filename&gt; [domain]\")\n        print(\n            \"  Directory:   python process_rdp_data.py &lt;directory&gt; [domain] [taxonomic_level] [output_file]\"\n        )\n        print(\"\\nExamples:\")\n        print(\"  python process_rdp_data.py data/RDP_classifier/APV+20.tab Bacteria\")\n        print(\"  python process_rdp_data.py data/RDP_classifier Bacteria\")\n        print(\"  python process_rdp_data.py data/RDP_classifier Bacteria phylum\")\n        print(\n            \"  python process_rdp_data.py data/RDP_classifier Bacteria phylum data/bacteria_phylum_counts.csv\"\n        )\n        print(\"\\nValid taxonomic levels: phylum, class, order, family, genus\")\n        sys.exit(1)\n\n    path = sys.argv[1]\n    domain = sys.argv[2] if len(sys.argv) &gt; 2 else \"Bacteria\"\n    taxonomic_level = None\n    output_file = None\n\n    # Parse remaining arguments\n    # Check if 3rd arg is a taxonomic level or output file\n    if len(sys.argv) &gt; 3:\n        arg3 = sys.argv[3]\n        valid_levels = [\"phylum\", \"class\", \"order\", \"family\", \"genus\"]\n        if arg3 in valid_levels:\n            taxonomic_level = arg3\n            output_file = sys.argv[4] if len(sys.argv) &gt; 4 else None\n        else:\n            # Assume it's an output file\n            output_file = arg3\n\n    path_obj = Path(path)\n\n    if path_obj.is_file():",
        "signatures": {},
        "detailedLines": [
          {
            "text": "if __name__ == \"__main__\":",
            "lineNumber": 335
          },
          {
            "text": "    # Example usage",
            "lineNumber": 336
          },
          {
            "text": "    import sys",
            "lineNumber": 337
          },
          {
            "lineNumber": 338
          },
          {
            "text": "    if len(sys.argv) &lt; 2:",
            "lineNumber": 339
          },
          {
            "text": "        print(\"Usage:\")",
            "lineNumber": 340
          },
          {
            "text": "        print(\"  Single file: python process_rdp_data.py &lt;filename&gt; [domain]\")",
            "lineNumber": 341
          },
          {
            "text": "        print(",
            "lineNumber": 342
          },
          {
            "text": "            \"  Directory:   python process_rdp_data.py &lt;directory&gt; [domain] [taxonomic_level] [output_file]\"",
            "lineNumber": 343
          },
          {
            "text": "        )",
            "lineNumber": 344
          },
          {
            "text": "        print(\"\\nExamples:\")",
            "lineNumber": 345
          },
          {
            "text": "        print(\"  python process_rdp_data.py data/RDP_classifier/APV+20.tab Bacteria\")",
            "lineNumber": 346
          },
          {
            "text": "        print(\"  python process_rdp_data.py data/RDP_classifier Bacteria\")",
            "lineNumber": 347
          },
          {
            "text": "        print(\"  python process_rdp_data.py data/RDP_classifier Bacteria phylum\")",
            "lineNumber": 348
          },
          {
            "text": "        print(",
            "lineNumber": 349
          },
          {
            "text": "            \"  python process_rdp_data.py data/RDP_classifier Bacteria phylum data/bacteria_phylum_counts.csv\"",
            "lineNumber": 350
          },
          {
            "text": "        )",
            "lineNumber": 351
          },
          {
            "text": "        print(\"\\nValid taxonomic levels: phylum, class, order, family, genus\")",
            "lineNumber": 352
          },
          {
            "text": "        sys.exit(1)",
            "lineNumber": 353
          },
          {
            "lineNumber": 354
          },
          {
            "text": "    path = sys.argv[1]",
            "lineNumber": 355
          },
          {
            "text": "    domain = sys.argv[2] if len(sys.argv) &gt; 2 else \"Bacteria\"",
            "lineNumber": 356
          },
          {
            "text": "    taxonomic_level = None",
            "lineNumber": 357
          },
          {
            "text": "    output_file = None",
            "lineNumber": 358
          },
          {
            "lineNumber": 359
          },
          {
            "text": "    # Parse remaining arguments",
            "lineNumber": 360
          },
          {
            "text": "    # Check if 3rd arg is a taxonomic level or output file",
            "lineNumber": 361
          },
          {
            "text": "    if len(sys.argv) &gt; 3:",
            "lineNumber": 362
          },
          {
            "text": "        arg3 = sys.argv[3]",
            "lineNumber": 363
          },
          {
            "text": "        valid_levels = [\"phylum\", \"class\", \"order\", \"family\", \"genus\"]",
            "lineNumber": 364
          },
          {
            "text": "        if arg3 in valid_levels:",
            "lineNumber": 365
          },
          {
            "text": "            taxonomic_level = arg3",
            "lineNumber": 366
          },
          {
            "text": "            output_file = sys.argv[4] if len(sys.argv) &gt; 4 else None",
            "lineNumber": 367
          },
          {
            "text": "        else:",
            "lineNumber": 368
          },
          {
            "text": "            # Assume it's an output file",
            "lineNumber": 369
          },
          {
            "text": "            output_file = arg3",
            "lineNumber": 370
          },
          {
            "lineNumber": 371
          },
          {
            "text": "    path_obj = Path(path)",
            "lineNumber": 372
          },
          {
            "lineNumber": 373
          },
          {
            "text": "    if path_obj.is_file():",
            "lineNumber": 374
          }
        ]
      },
      "score": 0.20479559898376465
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./hyena-dna/inference.py",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 104
          }
        },
        "contents": "# @title Single example\n# From https://colab.research.google.com/drive/1wyVEQd4R3HYLTUOXEEQmp_I8aNC_aLhL\nimport json\nimport os\nimport subprocess\nimport transformers\nfrom transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n\n# HyenaDNA\nfrom standalone_hyenadna import *\nfrom huggingface_wrapper import *\n\n\ndef inference_single():\n    \"\"\"\n    this selects which backbone to use, and grabs weights/ config from HF\n    4 options:\n      'hyenadna-tiny-1k-seqlen'   # fine-tune on colab ok\n      'hyenadna-small-32k-seqlen'\n      'hyenadna-medium-160k-seqlen'  # inference only on colab\n      'hyenadna-medium-450k-seqlen'  # inference only on colab\n      'hyenadna-large-1m-seqlen'  # inference only on colab\n    \"\"\"\n\n    # you only need to select which model to use here, we'll do the rest!\n    pretrained_model_name = \"hyenadna-small-32k-seqlen\"\n\n    max_lengths = {\n        \"hyenadna-tiny-1k-seqlen\": 1024,\n        \"hyenadna-small-32k-seqlen\": 32768,\n        \"hyenadna-medium-160k-seqlen\": 160000,\n        \"hyenadna-medium-450k-seqlen\": 450000,  # T4 up to here\n        \"hyenadna-large-1m-seqlen\": 1_000_000,  # only A100 (paid tier)\n    }\n\n    max_length = max_lengths[pretrained_model_name]  # auto selects\n\n    # data settings:\n    use_padding = True\n    rc_aug = False  # reverse complement augmentation\n    add_eos = False  # add end of sentence token\n\n    # we need these for the decoder head, if using\n    use_head = False\n    n_classes = 2  # not used for embeddings only\n\n    # you can override with your own backbone config here if you want,\n    # otherwise we'll load the HF one in None\n    backbone_cfg = None\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(\"Using device:\", device)\n\n    # instantiate the model (pretrained here)\n    if",
        "signatures": {},
        "detailedLines": [
          {
            "text": "# @title Single example",
            "lineNumber": 1
          },
          {
            "text": "# From https://colab.research.google.com/drive/1wyVEQd4R3HYLTUOXEEQmp_I8aNC_aLhL",
            "lineNumber": 2
          },
          {
            "text": "import json",
            "lineNumber": 3
          },
          {
            "text": "import os",
            "lineNumber": 4
          },
          {
            "text": "import subprocess",
            "lineNumber": 5
          },
          {
            "text": "import transformers",
            "lineNumber": 6
          },
          {
            "text": "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig",
            "lineNumber": 7
          },
          {
            "lineNumber": 8
          },
          {
            "text": "# HyenaDNA",
            "lineNumber": 9
          },
          {
            "text": "from standalone_hyenadna import *",
            "lineNumber": 10
          },
          {
            "text": "from huggingface_wrapper import *",
            "lineNumber": 11
          },
          {
            "lineNumber": 12
          },
          {
            "lineNumber": 13
          },
          {
            "text": "def inference_single():",
            "lineNumber": 14,
            "isSignature": true
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 15
          },
          {
            "text": "    this selects which backbone to use, and grabs weights/ config from HF",
            "lineNumber": 16
          },
          {
            "text": "    4 options:",
            "lineNumber": 17
          },
          {
            "text": "      'hyenadna-tiny-1k-seqlen'   # fine-tune on colab ok",
            "lineNumber": 18
          },
          {
            "text": "      'hyenadna-small-32k-seqlen'",
            "lineNumber": 19
          },
          {
            "text": "      'hyenadna-medium-160k-seqlen'  # inference only on colab",
            "lineNumber": 20
          },
          {
            "text": "      'hyenadna-medium-450k-seqlen'  # inference only on colab",
            "lineNumber": 21
          },
          {
            "text": "      'hyenadna-large-1m-seqlen'  # inference only on colab",
            "lineNumber": 22
          },
          {
            "text": "    \"\"\"",
            "lineNumber": 23
          },
          {
            "lineNumber": 24
          },
          {
            "text": "    # you only need to select which model to use here, we'll do the rest!",
            "lineNumber": 25
          },
          {
            "text": "    pretrained_model_name = \"hyenadna-small-32k-seqlen\"",
            "lineNumber": 26
          },
          {
            "lineNumber": 27
          },
          {
            "text": "    max_lengths = {",
            "lineNumber": 28
          },
          {
            "text": "        \"hyenadna-tiny-1k-seqlen\": 1024,",
            "lineNumber": 29
          },
          {
            "text": "        \"hyenadna-small-32k-seqlen\": 32768,",
            "lineNumber": 30
          },
          {
            "text": "        \"hyenadna-medium-160k-seqlen\": 160000,",
            "lineNumber": 31
          },
          {
            "text": "        \"hyenadna-medium-450k-seqlen\": 450000,  # T4 up to here",
            "lineNumber": 32
          },
          {
            "text": "        \"hyenadna-large-1m-seqlen\": 1_000_000,  # only A100 (paid tier)",
            "lineNumber": 33
          },
          {
            "text": "    }",
            "lineNumber": 34
          },
          {
            "lineNumber": 35
          },
          {
            "text": "    max_length = max_lengths[pretrained_model_name]  # auto selects",
            "lineNumber": 36
          },
          {
            "lineNumber": 37
          },
          {
            "text": "    # data settings:",
            "lineNumber": 38
          },
          {
            "text": "    use_padding = True",
            "lineNumber": 39
          },
          {
            "text": "    rc_aug = False  # reverse complement augmentation",
            "lineNumber": 40
          },
          {
            "text": "    add_eos = False  # add end of sentence token",
            "lineNumber": 41
          },
          {
            "lineNumber": 42
          },
          {
            "text": "    # we need these for the decoder head, if using",
            "lineNumber": 43
          },
          {
            "text": "    use_head = False",
            "lineNumber": 44
          },
          {
            "text": "    n_classes = 2  # not used for embeddings only",
            "lineNumber": 45
          },
          {
            "lineNumber": 46
          },
          {
            "text": "    # you can override with your own backbone config here if you want,",
            "lineNumber": 47
          },
          {
            "text": "    # otherwise we'll load the HF one in None",
            "lineNumber": 48
          },
          {
            "text": "    backbone_cfg = None",
            "lineNumber": 49
          },
          {
            "lineNumber": 50
          },
          {
            "text": "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
            "lineNumber": 51
          },
          {
            "text": "    print(\"Using device:\", device)",
            "lineNumber": 52
          },
          {
            "lineNumber": 53
          },
          {
            "text": "    # instantiate the model (pretrained here)",
            "lineNumber": 54
          },
          {
            "text": "    if",
            "lineNumber": 55
          }
        ]
      },
      "score": 0.20251303911209106
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./results/hgb_summary.txt",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 21
          }
        },
        "contents": "Model: HGB\nRandom Seed: 42\nNumber of features: 164\nTrain set size: 1868\nTest set size: 468\n\nGrid Search:\n  Mode: full\n  CV folds: 5\n  Number of combinations: 576\n  Time elapsed: 3674.64 seconds\n  Best parameters: {'regressor__l2_regularization': 0.0, 'regressor__learning_rate': 0.1, 'regressor__max_depth': 20, 'regressor__max_iter': 500, 'regressor__min_samples_leaf': 1}\n  Best CV RMSE: 1.0448\n\nFinal Model Metrics:\nTrain RMSE: 0.0087\nTest RMSE:  0.6313\nTrain R-squared:   1.0000\nTest R-squared:    0.7995\nTrain MAE:  0.0067\nTest MAE:   0.3928",
        "signatures": {},
        "detailedLines": [
          {
            "text": "Model: HGB",
            "lineNumber": 1
          },
          {
            "text": "Random Seed: 42",
            "lineNumber": 2
          },
          {
            "text": "Number of features: 164",
            "lineNumber": 3
          },
          {
            "text": "Train set size: 1868",
            "lineNumber": 4
          },
          {
            "text": "Test set size: 468",
            "lineNumber": 5
          },
          {
            "lineNumber": 6
          },
          {
            "text": "Grid Search:",
            "lineNumber": 7
          },
          {
            "text": "  Mode: full",
            "lineNumber": 8
          },
          {
            "text": "  CV folds: 5",
            "lineNumber": 9
          },
          {
            "text": "  Number of combinations: 576",
            "lineNumber": 10
          },
          {
            "text": "  Time elapsed: 3674.64 seconds",
            "lineNumber": 11
          },
          {
            "text": "  Best parameters: {'regressor__l2_regularization': 0.0, 'regressor__learning_rate': 0.1, 'regressor__max_depth': 20, 'regressor__max_iter': 500, 'regressor__min_samples_leaf': 1}",
            "lineNumber": 12
          },
          {
            "text": "  Best CV RMSE: 1.0448",
            "lineNumber": 13
          },
          {
            "lineNumber": 14
          },
          {
            "text": "Final Model Metrics:",
            "lineNumber": 15
          },
          {
            "text": "Train RMSE: 0.0087",
            "lineNumber": 16
          },
          {
            "text": "Test RMSE:  0.6313",
            "lineNumber": 17
          },
          {
            "text": "Train R-squared:   1.0000",
            "lineNumber": 18
          },
          {
            "text": "Test R-squared:    0.7995",
            "lineNumber": 19
          },
          {
            "text": "Train MAE:  0.0067",
            "lineNumber": 20
          },
          {
            "text": "Test MAE:   0.3928",
            "lineNumber": 21
          }
        ]
      },
      "score": 0.19979643821716309
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./.cursor/plans/hyenadna_ph_regression_head_0c97fe04.plan.md",
        "range": {
          "startPosition": {
            "line": 227
          },
          "endPosition": {
            "line": 296
          }
        },
        "contents": "**Data**:\n\n- Train/Val/Test split: &#96;0.8/0.1/0.1&#96;\n- Filter samples with missing pH values\n- Random seed: &#96;42&#96; (for reproducibility)\n\n### Sequence Concatenation Details\n\n**Confirmed Feasibility**: [SEP] token (ID=1) exists in &#96;CharacterTokenizer&#96; and can be used for concatenation.\n\n**Implementation Strategy**:\n\n1. For each sample with multiple sequences:\n\n   - Tokenize each sequence: &#96;tokenizer(seq)[\"input_ids\"]&#96;\n   - Remove special tokens from individual sequences (keep only DNA tokens)\n   - Join sequences with [SEP] token: &#96;seq1_tokens + [1] + seq2_tokens + [1] + ...&#96;\n   - Truncate if total length &gt; &#96;max_length&#96;: keep last &#96;max_length&#96; tokens\n   - Add [CLS] at start if desired (though causal models may not need it)\n\n2. Edge cases:\n\n   - Empty sequence list: return empty or single [SEP] token\n   - Single sequence: no [SEP] needed, just tokenize and truncate\n   - All sequences too long: take last sequence and truncate to max_length\n\n3. Padding:\n\n   - Pad to &#96;max_length&#96; with [PAD] token (ID=4) on the left\n   - Create attention mask to ignore padding tokens in loss\n\n### File Structure\n\n&#96;&#96;&#96;\ntrain_hyenadna_ph.py\nâ”œâ”€â”€ Imports (torch, datasets, transformers, etc.)\nâ”œâ”€â”€ RegressionHead class\nâ”œâ”€â”€ Data loading functions\nâ”‚   â”œâ”€â”€ load_dataset_from_hf()\nâ”‚   â”œâ”€â”€ concatenate_sequences()\nâ”‚   â”œâ”€â”€ create_dataloader()\nâ”‚   â””â”€â”€ collate_fn() for batching\nâ”œâ”€â”€ Model setup functions\nâ”‚   â”œâ”€â”€ setup_model()\nâ”‚   â””â”€â”€ get_model_config()\nâ”œâ”€â”€ Training functions\nâ”‚   â”œâ”€â”€ train_epoch()\nâ”‚   â”œâ”€â”€ evaluate()\nâ”‚   â””â”€â”€ save_checkpoint()\nâ””â”€â”€ main() with argument parsing\n&#96;&#96;&#96;\n\n### Dependencies\n\nAdd to project requirements (if not already present):\n\n- &#96;torch&#96; (PyTorch)\n- &#96;datasets&#96; (HuggingFace datasets)\n- &#96;transformers&#96; (for tokenizer compatibility)\n- &#96;numpy&#96;\n- &#96;scikit-learn&#96; (for metrics: RÂ², MSE, MAE)\n- &#96;tqdm&#96; (optional, for progress bars)\n\n### Output Files\n\nThe script should save:\n\n- &#96;best_model.pt&#96;: Best model checkpoint (based on validation loss)\n- &#96;training_log.txt&#96;: Training history (loss, metrics per epoch)",
        "signatures": {},
        "detailedLines": [
          {
            "text": "**Data**:",
            "lineNumber": 228
          },
          {
            "lineNumber": 229
          },
          {
            "text": "- Train/Val/Test split: &#96;0.8/0.1/0.1&#96;",
            "lineNumber": 230
          },
          {
            "text": "- Filter samples with missing pH values",
            "lineNumber": 231
          },
          {
            "text": "- Random seed: &#96;42&#96; (for reproducibility)",
            "lineNumber": 232
          },
          {
            "lineNumber": 233
          },
          {
            "text": "### Sequence Concatenation Details",
            "lineNumber": 234
          },
          {
            "lineNumber": 235
          },
          {
            "text": "**Confirmed Feasibility**: [SEP] token (ID=1) exists in &#96;CharacterTokenizer&#96; and can be used for concatenation.",
            "lineNumber": 236
          },
          {
            "lineNumber": 237
          },
          {
            "text": "**Implementation Strategy**:",
            "lineNumber": 238
          },
          {
            "lineNumber": 239
          },
          {
            "text": "1. For each sample with multiple sequences:",
            "lineNumber": 240
          },
          {
            "lineNumber": 241
          },
          {
            "text": "   - Tokenize each sequence: &#96;tokenizer(seq)[\"input_ids\"]&#96;",
            "lineNumber": 242
          },
          {
            "text": "   - Remove special tokens from individual sequences (keep only DNA tokens)",
            "lineNumber": 243
          },
          {
            "text": "   - Join sequences with [SEP] token: &#96;seq1_tokens + [1] + seq2_tokens + [1] + ...&#96;",
            "lineNumber": 244
          },
          {
            "text": "   - Truncate if total length &gt; &#96;max_length&#96;: keep last &#96;max_length&#96; tokens",
            "lineNumber": 245
          },
          {
            "text": "   - Add [CLS] at start if desired (though causal models may not need it)",
            "lineNumber": 246
          },
          {
            "lineNumber": 247
          },
          {
            "text": "2. Edge cases:",
            "lineNumber": 248
          },
          {
            "lineNumber": 249
          },
          {
            "text": "   - Empty sequence list: return empty or single [SEP] token",
            "lineNumber": 250
          },
          {
            "text": "   - Single sequence: no [SEP] needed, just tokenize and truncate",
            "lineNumber": 251
          },
          {
            "text": "   - All sequences too long: take last sequence and truncate to max_length",
            "lineNumber": 252
          },
          {
            "lineNumber": 253
          },
          {
            "text": "3. Padding:",
            "lineNumber": 254
          },
          {
            "lineNumber": 255
          },
          {
            "text": "   - Pad to &#96;max_length&#96; with [PAD] token (ID=4) on the left",
            "lineNumber": 256
          },
          {
            "text": "   - Create attention mask to ignore padding tokens in loss",
            "lineNumber": 257
          },
          {
            "lineNumber": 258
          },
          {
            "text": "### File Structure",
            "lineNumber": 259
          },
          {
            "lineNumber": 260
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 261
          },
          {
            "text": "train_hyenadna_ph.py",
            "lineNumber": 262
          },
          {
            "text": "â”œâ”€â”€ Imports (torch, datasets, transformers, etc.)",
            "lineNumber": 263
          },
          {
            "text": "â”œâ”€â”€ RegressionHead class",
            "lineNumber": 264
          },
          {
            "text": "â”œâ”€â”€ Data loading functions",
            "lineNumber": 265
          },
          {
            "text": "â”‚   â”œâ”€â”€ load_dataset_from_hf()",
            "lineNumber": 266
          },
          {
            "text": "â”‚   â”œâ”€â”€ concatenate_sequences()",
            "lineNumber": 267
          },
          {
            "text": "â”‚   â”œâ”€â”€ create_dataloader()",
            "lineNumber": 268
          },
          {
            "text": "â”‚   â””â”€â”€ collate_fn() for batching",
            "lineNumber": 269
          },
          {
            "text": "â”œâ”€â”€ Model setup functions",
            "lineNumber": 270
          },
          {
            "text": "â”‚   â”œâ”€â”€ setup_model()",
            "lineNumber": 271
          },
          {
            "text": "â”‚   â””â”€â”€ get_model_config()",
            "lineNumber": 272
          },
          {
            "text": "â”œâ”€â”€ Training functions",
            "lineNumber": 273
          },
          {
            "text": "â”‚   â”œâ”€â”€ train_epoch()",
            "lineNumber": 274
          },
          {
            "text": "â”‚   â”œâ”€â”€ evaluate()",
            "lineNumber": 275
          },
          {
            "text": "â”‚   â””â”€â”€ save_checkpoint()",
            "lineNumber": 276
          },
          {
            "text": "â””â”€â”€ main() with argument parsing",
            "lineNumber": 277
          },
          {
            "text": "&#96;&#96;&#96;",
            "lineNumber": 278
          },
          {
            "lineNumber": 279
          },
          {
            "text": "### Dependencies",
            "lineNumber": 280
          },
          {
            "lineNumber": 281
          },
          {
            "text": "Add to project requirements (if not already present):",
            "lineNumber": 282
          },
          {
            "lineNumber": 283
          },
          {
            "text": "- &#96;torch&#96; (PyTorch)",
            "lineNumber": 284
          },
          {
            "text": "- &#96;datasets&#96; (HuggingFace datasets)",
            "lineNumber": 285
          },
          {
            "text": "- &#96;transformers&#96; (for tokenizer compatibility)",
            "lineNumber": 286
          },
          {
            "text": "- &#96;numpy&#96;",
            "lineNumber": 287
          },
          {
            "text": "- &#96;scikit-learn&#96; (for metrics: RÂ², MSE, MAE)",
            "lineNumber": 288
          },
          {
            "text": "- &#96;tqdm&#96; (optional, for progress bars)",
            "lineNumber": 289
          },
          {
            "lineNumber": 290
          },
          {
            "text": "### Output Files",
            "lineNumber": 291
          },
          {
            "lineNumber": 292
          },
          {
            "text": "The script should save:",
            "lineNumber": 293
          },
          {
            "lineNumber": 294
          },
          {
            "text": "- &#96;best_model.pt&#96;: Best model checkpoint (based on validation loss)",
            "lineNumber": 295
          },
          {
            "text": "- &#96;training_log.txt&#96;: Training history (loss, metrics per epoch)",
            "lineNumber": 296
          }
        ]
      },
      "score": 0.1980023980140686
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./requirements.txt",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 29
          }
        },
        "contents": "# Requirements for train_hyenadna_ph.py\n# Core dependencies for HyenaDNA pH regression training\n\n# Deep learning framework\ntorch&gt;=1.13.0\ntorchvision&gt;=0.14.0  # Required for StochasticDepth in HyenaDNA\n\n# Data handling\nnumpy&gt;=1.21.0\npandas&gt;=1.3.0\ndatasets&gt;=2.0.0\n\n# Machine learning utilities\nscikit-learn&gt;=1.0.0\n\n# Progress bars\ntqdm&gt;=4.62.0\n\n# HuggingFace integration (for tokenizer compatibility)\ntransformers&gt;=4.20.0\n\n# Tensor operations (required by HyenaDNA)\neinops&gt;=0.6.0\n\n# Configuration management (required by HyenaDNA)\nomegaconf&gt;=2.1.0\n\n# Note: Additional HyenaDNA dependencies may be needed for full functionality.\n# See hyena-dna/requirements.txt for complete list.",
        "signatures": {},
        "detailedLines": [
          {
            "text": "# Requirements for train_hyenadna_ph.py",
            "lineNumber": 1
          },
          {
            "text": "# Core dependencies for HyenaDNA pH regression training",
            "lineNumber": 2
          },
          {
            "lineNumber": 3
          },
          {
            "text": "# Deep learning framework",
            "lineNumber": 4
          },
          {
            "text": "torch&gt;=1.13.0",
            "lineNumber": 5
          },
          {
            "text": "torchvision&gt;=0.14.0  # Required for StochasticDepth in HyenaDNA",
            "lineNumber": 6
          },
          {
            "lineNumber": 7
          },
          {
            "text": "# Data handling",
            "lineNumber": 8
          },
          {
            "text": "numpy&gt;=1.21.0",
            "lineNumber": 9
          },
          {
            "text": "pandas&gt;=1.3.0",
            "lineNumber": 10
          },
          {
            "text": "datasets&gt;=2.0.0",
            "lineNumber": 11
          },
          {
            "lineNumber": 12
          },
          {
            "text": "# Machine learning utilities",
            "lineNumber": 13
          },
          {
            "text": "scikit-learn&gt;=1.0.0",
            "lineNumber": 14
          },
          {
            "lineNumber": 15
          },
          {
            "text": "# Progress bars",
            "lineNumber": 16
          },
          {
            "text": "tqdm&gt;=4.62.0",
            "lineNumber": 17
          },
          {
            "lineNumber": 18
          },
          {
            "text": "# HuggingFace integration (for tokenizer compatibility)",
            "lineNumber": 19
          },
          {
            "text": "transformers&gt;=4.20.0",
            "lineNumber": 20
          },
          {
            "lineNumber": 21
          },
          {
            "text": "# Tensor operations (required by HyenaDNA)",
            "lineNumber": 22
          },
          {
            "text": "einops&gt;=0.6.0",
            "lineNumber": 23
          },
          {
            "lineNumber": 24
          },
          {
            "text": "# Configuration management (required by HyenaDNA)",
            "lineNumber": 25
          },
          {
            "text": "omegaconf&gt;=2.1.0",
            "lineNumber": 26
          },
          {
            "lineNumber": 27
          },
          {
            "text": "# Note: Additional HyenaDNA dependencies may be needed for full functionality.",
            "lineNumber": 28
          },
          {
            "text": "# See hyena-dna/requirements.txt for complete list.",
            "lineNumber": 29
          }
        ]
      },
      "score": 0.1974899023771286
    },
    {
      "codeBlock": {
        "relativeWorkspacePath": "./results/knn_summary.txt",
        "range": {
          "startPosition": {},
          "endPosition": {
            "line": 21
          }
        },
        "contents": "Model: KNN\nRandom Seed: 42\nNumber of features: 164\nTrain set size: 1868\nTest set size: 468\n\nGrid Search:\n  Mode: full\n  CV folds: 5\n  Number of combinations: 24\n  Time elapsed: 9.21 seconds\n  Best parameters: {'regressor__n_neighbors': 20, 'regressor__p': 1, 'regressor__weights': 'distance'}\n  Best CV RMSE: 1.3020\n\nFinal Model Metrics:\nTrain RMSE: 0.0000\nTest RMSE:  1.0024\nTrain R-squared:   1.0000\nTest R-squared:    0.4945\nTrain MAE:  0.0000\nTest MAE:   0.6109",
        "signatures": {},
        "detailedLines": [
          {
            "text": "Model: KNN",
            "lineNumber": 1
          },
          {
            "text": "Random Seed: 42",
            "lineNumber": 2
          },
          {
            "text": "Number of features: 164",
            "lineNumber": 3
          },
          {
            "text": "Train set size: 1868",
            "lineNumber": 4
          },
          {
            "text": "Test set size: 468",
            "lineNumber": 5
          },
          {
            "lineNumber": 6
          },
          {
            "text": "Grid Search:",
            "lineNumber": 7
          },
          {
            "text": "  Mode: full",
            "lineNumber": 8
          },
          {
            "text": "  CV folds: 5",
            "lineNumber": 9
          },
          {
            "text": "  Number of combinations: 24",
            "lineNumber": 10
          },
          {
            "text": "  Time elapsed: 9.21 seconds",
            "lineNumber": 11
          },
          {
            "text": "  Best parameters: {'regressor__n_neighbors': 20, 'regressor__p': 1, 'regressor__weights': 'distance'}",
            "lineNumber": 12
          },
          {
            "text": "  Best CV RMSE: 1.3020",
            "lineNumber": 13
          },
          {
            "lineNumber": 14
          },
          {
            "text": "Final Model Metrics:",
            "lineNumber": 15
          },
          {
            "text": "Train RMSE: 0.0000",
            "lineNumber": 16
          },
          {
            "text": "Test RMSE:  1.0024",
            "lineNumber": 17
          },
          {
            "text": "Train R-squared:   1.0000",
            "lineNumber": 18
          },
          {
            "text": "Test R-squared:    0.4945",
            "lineNumber": 19
          },
          {
            "text": "Train MAE:  0.0000",
            "lineNumber": 20
          },
          {
            "text": "Test MAE:   0.6109",
            "lineNumber": 21
          }
        ]
      },
      "score": 0.1950533390045166
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---

<tool-use data-tool-type="generic" data-tool-name="glob_file_search">
<details>
<summary>Tool use: **glob_file_search** â€¢ Searched codebase "undefined" â€¢ **0** result in **1** directory</summary>

Directory: **undefined** (0 file)

</details>
</tool-use>

---


Updating the test script to read all hyperparameters from `config.json` and creating the project rule:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/results/hyenadna_ph/config.json</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="unknown" data-tool-name="list_dir_v2">
<details>
<summary>Tool use: **list_dir_v2**</summary>


Parameters:

```json
{
  "targetDirectory": "/home/jedick/github/microbial-pH-predictions/.cursor/rules"
}
```
Result:

```json
{
  "directoryTreeRoot": {
    "absPath": "/home/jedick/github/microbial-pH-predictions/.cursor/rules",
    "childrenFiles": [
      {
        "name": "commit-rules.mdc"
      },
      {
        "name": "python-code-style.mdc"
      },
      {
        "name": "readme-updates.mdc"
      }
    ],
    "childrenWereProcessed": true
  }
}
```
Status: **completed**


</details>
</tool-use>

---


Updating the test script and creating the project rule:


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/.cursor/rules/python-code-style.mdc</summary>

</details>
</tool-use>

---


Creating the project rule about not duplicating config.json arguments:


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/.cursor/rules/config-file-usage.mdc</summary>

</details>

</tool-use>

---


Removing the unused import:


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="unknown" data-tool-name="read_lints">
<details>
<summary>Tool use: **read_lints**</summary>


Parameters:

```json
{
  "paths": [
    "test_hyenadna_ph.py"
  ]
}
```
Result:

```json
{
  "linterErrorsByFile": [
    {
      "relativeWorkspacePath": "test_hyenadna_ph.py"
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---


Verifying the changes:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Verifying the training script changes:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---


## Summary

1. **Updated `test_hyenadna_ph.py`**:
   - Removed command-line arguments that duplicate `config.json` values:
     - `--model-name`, `--head-architecture`, `--pooling-mode`
     - `--dataset-repo`, `--random-seed`, `--batch-size`
     - `--checkpoint-dir`, `--download-model`
   - Made `config.json` required (must exist in the checkpoint directory)
   - All hyperparameters are now loaded from `config.json`
   - Only accepts `--checkpoint` and `--output` as arguments (runtime settings)
   - Updated function signature to remove optional parameters

2. **Updated `train_hyenadna_ph.py`**:
   - Simplified the call to `run_test_predictions()` to only pass `checkpoint_path`, `output_file`, and `device`
   - Removed passing of hyperparameters that are already in `config.json`

3. **Created project rule** (`.cursor/rules/config-file-usage.mdc`):
   - Rule: Don't duplicate configuration values as command-line arguments when they're stored in config files
   - Exception: Arguments are appropriate when the script's purpose is to modify configuration values
   - Distinguishes between configuration/hyperparameters (from config files) and runtime settings (as arguments)

The script now requires `config.json` to exist and loads all hyperparameters from it, ensuring consistency with the training configuration. Usage:

```bash
python test_hyenadna_ph.py --checkpoint results/hyenadna_ph/best_model.pt --output test_predictions.csv
```

All model settings are automatically loaded from the `config.json` file in the checkpoint directory.

---

_**User**_

It works, but it runs the model on the test set twice. First is "Evaluating on test set" and second there is "Generating predictions". Can't the script running on the test set only once then use the output to report the evaluation results and save the predictions? Perhaps this should be another project rule - don't run a model twice for reporting metrics and generating predictions when it can be run once to do both things. 

---

_**Agent (model default, mode Agent)**_


Fixing the duplicate inference. Updating the script to run once and use the results for both metrics and saving predictions. Then adding a project rule.


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>
</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/train_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Updating the script to run inference once and use the results for both metrics and saving predictions:


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Adding the sklearn imports at the top:


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>
</tool-use>

---


Removing the unused `evaluate` import:


---

<tool-use data-tool-type="unknown" data-tool-name="ripgrep_raw_search">
<details>
<summary>Tool use: **ripgrep_raw_search**</summary>


Parameters:

```json
{
  "pattern": "\\bevaluate\\b",
  "path": "/home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py",
  "caseInsensitive": false
}
```
Result:

```json
{
  "success": {
    "pattern": "\\bevaluate\\b",
    "path": "/home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py",
    "outputMode": "content",
    "workspaceResults": {
      "/home/jedick/github/microbial-pH-predictions": {
        "content": {
          "matches": [
            {
              "file": "test_hyenadna_ph.py",
              "matches": [
                {
                  "lineNumber": 37,
                  "content": "    evaluate,"
                }
              ]
            }
          ],
          "totalLines": 1,
          "totalMatchedLines": 1
        }
      }
    }
  }
}
```
Status: **completed**


</details>
</tool-use>

---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>

</tool-use>

---


Creating the project rule:


---

<tool-use data-tool-type="write" data-tool-name="edit_file_v2">

<details><summary>Tool use: **edit_file_v2** â€¢ Edit file: /home/jedick/github/microbial-pH-predictions/.cursor/rules/efficient-inference.mdc</summary>

</details>

</tool-use>

---

<tool-use data-tool-type="unknown" data-tool-name="read_lints">
<details>
<summary>Tool use: **read_lints**</summary>


Parameters:

```json
{
  "paths": [
    "test_hyenadna_ph.py"
  ]
}
```
Result:

```json
{
  "linterErrorsByFile": [
    {
      "relativeWorkspacePath": "test_hyenadna_ph.py"
    }
  ]
}
```
Status: **completed**


</details>
</tool-use>

---


Verifying the changes:


---

<tool-use data-tool-type="read" data-tool-name="read_file_v2">
<details><summary>Tool use: **read_file_v2** â€¢ Read file: /home/jedick/github/microbial-pH-predictions/test_hyenadna_ph.py</summary>

</details>
</tool-use>

---


## Summary

1. **Updated `test_hyenadna_ph.py`**:
   - Removed duplicate inference: the script now runs the model once on the test set
   - Single pass collects:
     - Model predictions
     - True pH values
     - Metadata (study_name, sample_id, set_idx)
     - Loss values for metrics
   - Metrics are computed from the collected arrays
   - The same collected data is used to save predictions to CSV
   - Removed unused `evaluate` import

2. **Created project rule** (`.cursor/rules/efficient-inference.mdc`):
   - Rule: Don't run models twice when once will do
   - When both metrics and predictions are needed, run inference once and use the results for both
   - Includes examples of bad vs. good patterns
   - Notes exceptions (different preprocessing, memory constraints, different models)

The script is now more efficient: it runs inference once instead of twice, reducing runtime by ~50% for this operation. The metrics and saved predictions come from the same forward passes, ensuring consistency.

---

_**User**_

Please add a short section to the README for this script, following the project rules for README updates.

---

